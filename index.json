[{"authors":["gsarti"],"categories":null,"content":"Welcome to my website! ðŸ‘‹ I am a PhD student at the Computational Linguistics Group of the University of Groningen and member of the InDeep consortium, working on user-centric interpretability for neural machine translation. I am also the main developer of the Inseq library. My supervisors are Arianna Bisazza, Malvina Nissim and Grzegorz ChrupaÅ‚a.\nPreviously, I was a research intern at Amazon Translate NYC, a research scientist at Aindo, a Data Science MSc student at the University of Trieste and a co-founder of the AI Student Society.\nMy research focuses on interpretability for generative language models, with a particular interest to end-users\u0026rsquo; benefits and the usage of human behavioral signals. I am also into causality topics and open source collaboration.\nYour (anonymous) feedback is always welcome! ðŸ™‚\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"9a59029f2594915e13ca7ed4b3a51fc3","permalink":"https://gsarti.com/authors/gsarti/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/gsarti/","section":"authors","summary":"Welcome to my website! ðŸ‘‹ I am a PhD student at the Computational Linguistics Group of the University of Groningen and member of the InDeep consortium, working on user-centric interpretability for neural machine translation. I am also the main developer of the Inseq library. My supervisors are Arianna Bisazza, Malvina Nissim and Grzegorz ChrupaÅ‚a.\nPreviously, I was a research intern at Amazon Translate NYC, a research scientist at Aindo, a Data Science MSc student at the University of Trieste and a co-founder of the AI Student Society.","tags":null,"title":"Gabriele Sarti","type":"authors"},{"authors":["Gabriele Sarti"],"categories":["Natural Language Processing","Academic"],"content":"","date":1716192000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1716192000,"objectID":"e2a5407bd27ea1fc7c94bcc5f4ec06a2","permalink":"https://gsarti.com/talk/polito-inseq-pecore-2024/","publishdate":"2024-02-26T00:00:00Z","relpermalink":"/talk/polito-inseq-pecore-2024/","section":"talk","summary":"This talk discusses the challenges and opportunities in conducting interpretability analyses of generative language models. We begin by presenting Inseq, an open-source toolkit for advanced feature attribution analyses of language models. The usage of Inseq is illustrated through examples of state-of-the-art approaches contrastive attribution, input dependence and locating factual knowledge in intermediate model representations. Then, we introduce Plausibility Evaluation of Context Reliance (PECoRe), an end-to-end interpretability framework using model internals  to detect context-dependent spans in model generations and trace their prediction back to salient tokens in the available context. The usage of PECoRe is showcased on various generative tasks, including machine translation, story generation and retrieval-augmented question answering.","tags":["Natural Language Processing","Interpretability","Sequence-to-sequence","Language Modeling","Feature Attribution"],"title":"Interpreting Context Usage in Generative Language Models with Inseq and PECoRe","type":"talk"},{"authors":["Gabriele Sarti","Malvina Nissim"],"categories":["Natural Language Processing"],"content":"","date":1716159600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1716159600,"objectID":"2eaf51058a67b91ea3a1147275051ab7","permalink":"https://gsarti.com/publication/it5/","publishdate":"2024-05-20T01:00:00+02:00","relpermalink":"/publication/it5/","section":"publication","summary":"IT5s are the first encoder-decoder transformers pretrained on more than 40 billion Italian words.","tags":["Natural Language Processing","Pre-training","Italian","HuggingFace","Deep Learning","T5","Conditional Language Generation","Multilingual"],"title":"IT5: Text-to-text Pretraining for Italian Language Understanding and Generation","type":"publication"},{"authors":["Gabriele Sarti"],"categories":["Natural Language Processing","Academic"],"content":"","date":1715950800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1715950800,"objectID":"9176e0bac04b017e925991ea2140f6bd","permalink":"https://gsarti.com/talk/area-pecore-2024/","publishdate":"2022-05-17T00:00:00Z","relpermalink":"/talk/area-pecore-2024/","section":"talk","summary":"This talk presents the PECoRe framework for quantifying the plausibility of context reliance in neural machine translation. The framework is applied to a case study on the impact of context on the translation of gendered pronouns and other contextual phenomena in English-to-French translation. Finally, the online demo allowing users to try PECoRe with any generative language model is presented.","tags":["Natural Language Processing","Neural Machine Translation","Interpretability","Sequence-to-sequence"],"title":"Quantifying the Plausibility of Context Reliance in Neural Machine Translation","type":"talk"},{"authors":["Javier Ferrando","Gabriele Sarti","Arianna Bisazza","Marta Costa-jussÃ "],"categories":[],"content":"","date":1714514400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1714514400,"objectID":"1df6ac103ff228ddd22066dfe121db42","permalink":"https://gsarti.com/publication/transformer-lm-inner-workings/","publishdate":"2024-05-01T00:00:00+02:00","relpermalink":"/publication/transformer-lm-inner-workings/","section":"publication","summary":"This primer provides a concise technical introduction to the current techniques used to interpret the inner workings of Transformer-based language models, focusing on the generative decoder-only architecture.","tags":["Natural Language Processing","Deep Learning","Interpretability","Mechanistic Interpretability","Language Modeling","Transformers"],"title":"A Primer on the Inner Workings of Transformer-based Language Models","type":"publication"},{"authors":["Gabriele Sarti"],"categories":["Natural Language Processing","Academic"],"content":"","date":1714132800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1714132800,"objectID":"5e5f7521c30385e3efa4cf4d77250d45","permalink":"https://gsarti.com/talk/gronlp-rg-pecore/","publishdate":"2022-05-19T00:00:00Z","relpermalink":"/talk/gronlp-rg-pecore/","section":"talk","summary":"This talk presents the PECoRe framework for quantifying the plausibility of context reliance in neural machine translation. The framework is applied to a case study on the impact of context on the translation of gendered pronouns and other contextual phenomena in English-to-French translation. Finally, the online demo allowing users to try PECoRe with any generative language model is presented.","tags":["Natural Language Processing","Neural Machine Translation","Interpretability","Sequence-to-sequence"],"title":"Quantifying the Plausibility of Context Reliance in Neural Machine Translation","type":"talk"},{"authors":["Gabriele Sarti"],"categories":["Natural Language Processing","Academic"],"content":"","date":1709308800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1709308800,"objectID":"cfdf8f940a9360384d55e261581567f0","permalink":"https://gsarti.com/talk/sheffield-seminar-2024/","publishdate":"2024-02-26T00:00:00Z","relpermalink":"/talk/sheffield-seminar-2024/","section":"talk","summary":"This talk discusses the challenges of interpreting generative language models and presents Inseq, a toolkit for interpreting sequence generation models. The usage of Inseq is illustrated with examples introducing state-of-the-art approaches for interpreting language models such as contrastive attribution. Finally, the PECoRe framework is presented as a mean to evaluate the plausibility of context usage in language models.","tags":["Natural Language Processing","Interpretability","Sequence-to-sequence","Language Modeling","Feature Attribution"],"title":"Post-hoc Interpretability for Generative Language Models: Explaining Context Usage in Transformers","type":"talk"},{"authors":["Gabriele Sarti","Grzegorz ChrupaÅ‚a","Arianna Bisazza"],"categories":["Natural Language Processing","Industry-oriented"],"content":"","date":1698913800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1698913800,"objectID":"7a1654ca871a84e1742a8545760dc153","permalink":"https://gsarti.com/talk/indeep-masterclass-nov23/","publishdate":"2022-05-19T00:00:00Z","relpermalink":"/talk/indeep-masterclass-nov23/","section":"talk","summary":"In recent years, Transformer-based language models have achieved remarkable progress in most language generation and understanding tasks. However, the internal computations of these models are hardly interpretable due to their highly nonlinear structure, hindering their usage for mission-critical applications requiring trustworthiness and transparency guarantees. This presentation will introduce interpretability methods used for tracing the predictions of language models back to their inputs and discuss how these can be used to gain insights into model biases and behaviors. Several concrete examples of language model attributions will be presented throughout the presentation using the Inseq interpretability library.","tags":["Natural Language Processing","Interpretability","Sequence-to-sequence","Language Modeling","Feature Attribution"],"title":"Explaining Language Models with Inseq","type":"talk"},{"authors":["Gabriele Sarti"],"categories":["Natural Language Processing","Academic"],"content":"","date":1698319800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1698319800,"objectID":"7732d34e03caf633dad7e3b06b10844d","permalink":"https://gsarti.com/talk/escience-signlp-seminar-oct23/","publishdate":"2023-10-11T00:00:00+02:00","relpermalink":"/talk/escience-signlp-seminar-oct23/","section":"talk","summary":"This talk discusses the challenges of interpreting generative language models and presents Inseq, a toolkit for interpreting sequence generation models. The usage of Inseq is illustrated with examples introducing state-of-the-art approaches for interpreting language models such as contrastive attribution. Finally, the PECoRe framework is presented as a mean to evaluate the plausibility of context usage in language models.","tags":["Natural Language Processing","Interpretability","Sequence-to-sequence","Language Modeling","Feature Attribution"],"title":"Post-hoc Interpretability for Language Models","type":"talk"},{"authors":["Anna Langedijk","Hosein Mohebbi","Gabriele Sarti","Willem Zuidema","Jaap Jumelet"],"categories":["Natural Language Processing"],"content":"","date":1696456800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1696456800,"objectID":"e95b465a0d79d75292719202c6ce66d5","permalink":"https://gsarti.com/publication/decoderlens/","publishdate":"2023-10-05T00:00:00+02:00","relpermalink":"/publication/decoderlens/","section":"publication","summary":"We propose DecoderLens, a method to evaluate the iterative refinement of representations in encoder-decoder Transformer models.","tags":["Natural Language Processing","Deep Learning","Interpretability","Machine Translation","Automatic Speech Recognition","Question Answering","Logical Reasoning"],"title":"DecoderLens: Layerwise Interpretation of Encoder-Decoder Transformers","type":"publication"},{"authors":["Gabriele Sarti","Grzegorz ChrupaÅ‚a","Malvina Nissim","Arianna Bisazza"],"categories":["Natural Language Processing"],"content":"","date":1696197600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1696197600,"objectID":"38a8115956ac23410526fbdae1817cf6","permalink":"https://gsarti.com/publication/pecore/","publishdate":"2023-10-02T00:00:00+02:00","relpermalink":"/publication/pecore/","section":"publication","summary":"We introduce PECoRe, an interpretability framework for identifying context dependence in language model generations.","tags":["Natural Language Processing","Deep Learning","Interpretability","Machine Translation","Feature Attribution","Context Usage"],"title":"Quantifying the Plausibility of Context Reliance in Neural Machine Translation","type":"publication"},{"authors":["Gabriele Sarti"],"categories":["Natural Language Processing","Academic"],"content":"","date":1688306400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1688306400,"objectID":"e784075e5c7c8e2dafb733d37d32c58d","permalink":"https://gsarti.com/talk/restcl-2023/","publishdate":"2023-07-02T00:00:00Z","relpermalink":"/talk/restcl-2023/","section":"talk","summary":"In recent years, Transformer-based language models have achieved remarkable progress in most language generation and understanding tasks. However, the internal computations of these models are hardly interpretable due to their highly nonlinear structure, hindering their usage for mission-critical applications requiring trustworthiness and transparency guarantees. This presentation will introduce interpretability methods used for tracing the predictions of language models back to their inputs and discuss how these can be used to gain insights into model biases and behaviors. Several concrete examples of language model attributions will be presented throughout the presentation using the Inseq interpretability library.","tags":["Natural Language Processing","Interpretability","Sequence-to-sequence","Language Modeling","Feature Attribution"],"title":"Post-hoc Interpretability for NLG \u0026 Inseq: an Interpretability Toolkit for Sequence Generation Models","type":"talk"},{"authors":["Gabriele Sarti"],"categories":["Natural Language Processing","Academic"],"content":"","date":1685626200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1685626200,"objectID":"40f924278ce1527a2c3ff77015dcad9b","permalink":"https://gsarti.com/talk/cosmo-units-2023/","publishdate":"2023-06-01T00:00:00Z","relpermalink":"/talk/cosmo-units-2023/","section":"talk","summary":"In recent years, Transformer-based language models have achieved remarkable progress in most language generation and understanding tasks. However, the internal computations of these models are hardly interpretable due to their highly nonlinear structure, hindering their usage for mission-critical applications requiring trustworthiness and transparency guarantees. This presentation will introduce interpretability methods used for tracing the predictions of language models back to their inputs and discuss how these can be used to gain insights into model biases and behaviors. Several concrete examples of language model attributions will be presented throughout the presentation using the Inseq interpretability library.","tags":["Natural Language Processing","Interpretability","Sequence-to-sequence","Language Modeling","Feature Attribution"],"title":"Post-hoc Interpretability for Neural Language Models","type":"talk"},{"authors":["Gabriele Sarti","Alessio Miaschi"],"categories":["Natural Language Processing","Academic"],"content":"","date":1685538000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1685538000,"objectID":"9827ebe3cb95764f688a8a0d3c27b6b9","permalink":"https://gsarti.com/talk/ailc-lcl-2023/","publishdate":"2023-06-01T00:00:00Z","relpermalink":"/talk/ailc-lcl-2023/","section":"talk","summary":"As language models become increasingly complex and sophisticated, the processes leading to their predictions are growing increasingly difficult to understand. Research in NLP interpretability focuses on explaining the rationales driving model predictions and is crucial for building trust and transparency in the usage of these systems in real-world scenarios. In this laboratory, we will explore various techniques for analyzing Neural Language Models, such as feature attribution methods and diagnostic classifiers. Besides common approaches to inspect modelsâ€™ internal representations, we will also introduce prompting techniques to elicit model responses and motivate their usage as alternative methods for the behavioral study of model generations.","tags":["Natural Language Processing","Interpretability","Sequence-to-sequence","Language Modeling","Feature Attribution","Probing Classifiers"],"title":"Explaining Neural Language Models from Internal Representations to Model Predictions","type":"talk"},{"authors":["Gabriele Sarti","Phu Mon Htut","Xing Niu","Benjamin Hsu","Anna Currey","Georgiana Dinu","Maria Nadejde"],"categories":["Natural Language Processing"],"content":"","date":1685346458,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1685346458,"objectID":"f764626e4a7e1b3763e570ec10931a00","permalink":"https://gsarti.com/publication/ramp/","publishdate":"2023-05-29T09:47:38+02:00","relpermalink":"/publication/ramp/","section":"publication","summary":"We introduce Retrieval and Attribute-Marking enhanced Prompting (RAMP) to perform attribute-controlled MT with multilingual LLMs.","tags":["Natural Language Processing","Deep Learning","Machine Translation","Style Transfer","Multilingual","Prompting","Large Language Models"],"title":"RAMP: Retrieval and Attribute-Marking Enhanced Prompting for Attribute-Controlled Translation","type":"publication"},{"authors":["Gabriele Sarti"],"categories":["Natural Language Processing","Academic"],"content":"","date":1684854000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1684854000,"objectID":"d1304ee92794525e9505ee9e49c12104","permalink":"https://gsarti.com/talk/ailo-xai-2023/","publishdate":"2022-05-23T00:00:00Z","relpermalink":"/talk/ailo-xai-2023/","section":"talk","summary":"In recent years, Transformer-based language models have achieved remarkable progress in most language generation and understanding tasks. However, the internal computations of these models are hardly interpretable due to their highly nonlinear structure, hindering their usage for mission-critical applications requiring trustworthiness and transparency guarantees. This presentation will introduce interpretability methods used for tracing the predictions of language models back to their inputs and discuss how these can be used to gain insights into model biases and behaviors. Throughout the presentation, several concrete examples of language model attributions will be presented using the Inseq interpretability library.","tags":["Natural Language Processing","Interpretability","Sequence-to-sequence","Language Modeling","Feature Attribution"],"title":"Post-hoc Interpretability for Neural Language Models","type":"talk"},{"authors":["Gabriele Sarti"],"categories":["Natural Language Processing","Academic"],"content":"","date":1680789600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1680789600,"objectID":"90ecc524b920b4ff4bfed8380d9f3a4b","permalink":"https://gsarti.com/talk/sapienzanlp-seminar-2023/","publishdate":"2022-05-19T00:00:00Z","relpermalink":"/talk/sapienzanlp-seminar-2023/","section":"talk","summary":"This talk introduces the Inseq toolkit for interpreting sequence generation models. The usage of Inseq is illustrated with examples introducing state-of-the-art approaches for interpreting language models such as contrastive attribution, tuned lenses and causal mediation analysis.","tags":["Natural Language Processing","Interpretability","Sequence-to-sequence","Language Modeling","Feature Attribution"],"title":"Inseq: An Interpretability Toolkit for Sequence Generation Models","type":"talk"},{"authors":["Gabriele Sarti"],"categories":["Natural Language Processing","Academic"],"content":"","date":1679576400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1679576400,"objectID":"9cc42f7e04713e71674a03a25da1c5e3","permalink":"https://gsarti.com/talk/indeep-meeting-mar23/","publishdate":"2022-05-19T00:00:00Z","relpermalink":"/talk/indeep-meeting-mar23/","section":"talk","summary":"This talk introduces the Inseq toolkit for interpreting sequence generation models. The usage of Inseq is illustrated with examples introducing state-of-the-art approaches for interpreting language models such as contrastive attribution, tuned lenses and causal mediation analysis.","tags":["Natural Language Processing","Interpretability","Sequence-to-sequence","Language Modeling","Feature Attribution"],"title":"Advanced XAI Techniques and Inseq: An Interpretability Toolkit for Sequence Generation Models","type":"talk"},{"authors":["Gabriele Sarti"],"categories":["Natural Language Processing","Academic"],"content":"","date":1678449600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1678449600,"objectID":"68ef1717380d867e8f1e067f51a3cf9e","permalink":"https://gsarti.com/talk/gronlp-rg-inseq/","publishdate":"2022-05-19T00:00:00Z","relpermalink":"/talk/gronlp-rg-inseq/","section":"talk","summary":"After motivating the usage of interpretability methods in NLP, this talk introduces the Inseq toolkit for interpreting sequence generation models. The usage of Inseq is illustrated on two case studies related to gender bias in machine translation and locating factual knowledge withing GPT-2 representations.","tags":["Natural Language Processing","Neural Machine Translation","Interpretability","Sequence-to-sequence"],"title":"Introducing Inseq: An Interpretability Toolkit for Sequence Generation Models","type":"talk"},{"authors":["Lukas Edman","Gabriele Sarti","Antonio Toral","Gertjan van Noord","Arianna Bisazza"],"categories":["Natural Language Processing"],"content":"","date":1677570458,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1677570458,"objectID":"f98c2caee75dcef07cdb5a3b43bb224d","permalink":"https://gsarti.com/publication/char-mt-analysis/","publishdate":"2023-02-28T09:47:38+02:00","relpermalink":"/publication/char-mt-analysis/","section":"publication","summary":"We analyze input contributions of char-level MT models and show how they modulate word and character-level information.","tags":["Natural Language Processing","Deep Learning","Interpretability","Machine Translation","Feature Attribution","Character-level"],"title":"Are Character-level Translations Worth the Wait? Comparing ByT5 and mT5 for Machine Translation","type":"publication"},{"authors":["Gabriele Sarti","Nils Feldhus","Ludwig Sickert","Oskar van der Wal","Malvina Nissim","Arianna Bisazza"],"categories":["Natural Language Processing"],"content":"","date":1677484058,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1677484058,"objectID":"f849025865e410b201efae161a65ba5b","permalink":"https://gsarti.com/publication/inseq/","publishdate":"2023-02-27T09:47:38+02:00","relpermalink":"/publication/inseq/","section":"publication","summary":"We present Inseq, a Python library to democratize access to interpretability analyses of sequence generation models.","tags":["Natural Language Processing","HuggingFace","Deep Learning","Interpretability","Machine Translation","Transformers","Feature Attribution","Natural Language Generation","Library"],"title":"Inseq: An Interpretability Toolkit for Sequence Generation Models","type":"publication"},{"authors":["Gabriele Sarti","Nils Feldhus","Oskar van der Waals","Ludwig Sickert"],"categories":["Interpretability"],"content":"Inseq is a Pytorch-based hackable toolkit to democratize the study of interpretability for sequence generation models. Inseq supports a wide set of models from the ðŸ¤— Transformers library and an ever-growing set of feature attribution methods, leveraging in part the widely-used Captum library. For a quick introduction to common use cases, see the Getting started with Inseq page.\nUsing Inseq, feature attribution maps that can be saved, reloaded, aggregated and visualized either as HTMLs (with Jupyter notebook support) or directly in the console using rich. Besides simple attribution, Inseq also supports features like step score extraction, attribution aggregation and attributed functions customization for more advanced use cases.\n","date":1670889600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1670889600,"objectID":"cd10ca553e806492c56344928f6a2ff5","permalink":"https://gsarti.com/project/inseq/","publishdate":"2022-12-13T00:00:00Z","relpermalink":"/project/inseq/","section":"project","summary":"An open-source library to democratize access to model interpretability for sequence generation models","tags":["Natural Language Processing","Interpretability","HuggingFace","Deep Learning","Natural Language Generation"],"title":"Inseq: An Interpretability Toolkit for Sequence Generation Models","type":"project"},{"authors":["Gabriele Sarti","Grzegorz ChrupaÅ‚a","Malvina Nissim","Arianna Bisazza"],"categories":["Interpretability"],"content":"PECoRe is a framework using the internal properties of generative language models to identify and attribute context usage in their generations. In particular, the framework is composed by two steps: Context-sensitive Token Identification (CTI), where generated tokens are classified as context-sensitive by contrastively comparing their probabilities with and without context, and Contextual Cues Imputation (CCI), where the dependence of token selected in the CTI step is highlighted by using contrastive attribution. The framework is integrated in the Inseq interpretability library and can be easily used thanks to the inseq attribute-context command. The framework is described in detail in the paper Quantifying the Plausibility of Context Reliance in Neural Machine Translation, published at ICLR 2024.\n","date":1670889600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1670889600,"objectID":"58a2800d81b3159cdf8677d4d9fdf5ad","permalink":"https://gsarti.com/project/pecore/","publishdate":"2022-12-13T00:00:00Z","relpermalink":"/project/pecore/","section":"project","summary":"An interpretability framework to detect and attribute context usage in language models' generations","tags":["Natural Language Processing","Interpretability","Deep Learning","Natural Language Generation"],"title":"PECoRe: Plausibility Evaluation of Context Usage in Language Models","type":"project"},{"authors":["Gabriele Sarti"],"categories":["Natural Language Processing","Academic"],"content":"","date":1666224000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1666224000,"objectID":"f4cd8c6b7944c34ec70686b56b431fd5","permalink":"https://gsarti.com/talk/linguistics-lunch/","publishdate":"2022-05-19T00:00:00Z","relpermalink":"/talk/linguistics-lunch/","section":"talk","summary":"With the astounding advances of artificial intelligence in recent years, interpretability research has emerged as a fundamental effort to ensure the development of robust and transparent AI systems aligned with human needs. This talk will focus on user-centric interpretability applications aimed at improving our understanding of machine translation systems, with the ultimate goal of improving post-editing productivity and enjoyability.","tags":["Natural Language Processing","Neural Machine Translation","Interpretability","Sequence-to-sequence","Behavioral Data","Linguistic Complexity"],"title":"Towards User-centric Interpretability of Machine Translation Models","type":"talk"},{"authors":["Gabriele Sarti","Arianna Bisazza","Ana Guerberof Arenas","Antonio Toral"],"categories":["Natural Language Processing"],"content":"","date":1653346800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1653346800,"objectID":"e5094ae8a5a8e1cbd4dcf58f57f16540","permalink":"https://gsarti.com/publication/divemt/","publishdate":"2022-05-24T01:00:00+02:00","relpermalink":"/publication/divemt/","section":"publication","summary":"DivEMT is a publicly available post-editing study of Neural Machine Translation over a typologically diverse set of target languages.","tags":["Natural Language Processing","Post-editing","Machine Translation","Multilingual","Dataset","Behavioral Data"],"title":"DivEMT: Neural Machine Translation Post-Editing Effort Across Typologically Diverse Languages","type":"publication"},{"authors":["Gabriele Sarti"],"categories":["Natural Language Processing","Industry-oriented"],"content":"","date":1652832000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1652832000,"objectID":"198c29ce48c766630f2d098b0fabba46","permalink":"https://gsarti.com/talk/tech-talk-translated-2022/","publishdate":"2022-05-18T00:00:00Z","relpermalink":"/talk/tech-talk-translated-2022/","section":"talk","summary":"With the astounding advances of artificial intelligence in recent years, the field of interpretability research has emerged as a fundamental effort to ensure the development of robust AI systems aligned with human values. In this talk, two perspectives on AI interpretability will be presented alongside two case studies in natural language processing. The first study leverages behavioral data and probing tasks to study the perception and encoding of linguistic complexity in humans and language models. The second introduces a user-centric interpretability perspective for neural machine translation to improve post-editing productivity and enjoyability. The need for such application-driven approaches will be emphasized in light of current challenges in faithfully evaluating advances in this field of study.","tags":["Natural Language Processing","Neural Machine Translation","Interpretability","Sequence-to-sequence","Behavioral Data","Linguistic Complexity"],"title":"Towards User-centric Interpretability of NLP Models","type":"talk"},{"authors":["Gabriele Sarti"],"categories":["Natural Language Processing","Academic"],"content":"","date":1639440000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1639440000,"objectID":"41b3e023570d136e585b718b2a39a360","permalink":"https://gsarti.com/talk/xai4debugging21/","publishdate":"2021-11-09T10:30:41+01:00","relpermalink":"/talk/xai4debugging21/","section":"talk","summary":"Discussing the potential applications of interpretability research to the field of neural machine translation.","tags":["Natural Language Processing","Neural Machine Translation","Interpretability","Sequence-to-sequence"],"title":"Empowering Human Translators via Interpretable Interactive Neural Machine Translation","type":"talk"},{"authors":["Gabriele Sarti"],"categories":["Natural Language Processing","Academic"],"content":"","date":1636070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636070400,"objectID":"bbd061d18d46550ef3bdae8746637187","permalink":"https://gsarti.com/talk/aperitivo-bocconi/","publishdate":"2021-11-09T10:30:41+01:00","relpermalink":"/talk/aperitivo-bocconi/","section":"talk","summary":"Presenting my work on studying different metrics of linguistic complexity and how they correlate with linguistic phenomena and learned representations in neural language models","tags":["Natural Language Processing","Eye-tracking","Deep Learning","Interpretability","Probing Tasks"],"title":"Characterizing Linguistic Complexity in Humans and Language Models","type":"talk"},{"authors":["Federico Bianchi","Giuseppe Attanasio","Raphael Pisoni","Silvia Terragni","Gabriele Sarti"],"categories":["Multimodality"],"content":"","date":1629359258,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1629359258,"objectID":"e222ff2d16d7fe8780871ce179aab3b6","permalink":"https://gsarti.com/publication/clip-italian/","publishdate":"2021-08-19T09:47:38+02:00","relpermalink":"/publication/clip-italian/","section":"publication","summary":"We present the first CLIP model for the Italian Language (CLIP-Italian), trained on more than 1.4 million image-text pairs.","tags":["Computer Vision","Natural Language Processing","Multimodality","Italian","HuggingFace","Deep Learning","Contrastive Learning","CLIP"],"title":"Contrastive Language-Image Pre-training for the Italian Language","type":"publication"},{"authors":["Federico Bianchi","Raphael Pisoni","Giuseppe Attanasio","Silvia Terragni","Dario Balestri","Gabriele Sarti","Sri Lakshmi"],"categories":["Multimodality"],"content":"CLIP is a multimodel model that can learn to represent images and text jointly in the same space. In this project, we aim to propose the first CLIP model trained on Italian data, that in this context can be considered a low resource language. Using a few techniques, we have been able to fine-tune a SOTA Italian CLIP model with only 1.4 million training samples.\nFor more information, refer to our demo.\n","date":1626998400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1626998400,"objectID":"ec6905b548c40d40acfcf6c214b3821a","permalink":"https://gsarti.com/project/clip-italian/","publishdate":"2021-07-23T00:00:00Z","relpermalink":"/project/clip-italian/","section":"project","summary":"The first CLIP model pretrained on the Italian language.","tags":["Computer Vision","Natural Language Processing","Multimodality","Italian","HuggingFace","Deep Learning","Contrastive Learning","CLIP"],"title":"Contrastive Image-Text Pretraining for Italian","type":"project"},{"authors":["Ludovica Pannitto","Lucia Busso","Claudia Roberta Combei","Lucio Messina","Alessio Miaschi","Gabriele Sarti","Malvina Nissim"],"categories":[],"content":"","date":1622965658,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1622965658,"objectID":"36e466633662949775e675e2ab632ed0","permalink":"https://gsarti.com/publication/teaching-nlp-bracelets-menus/","publishdate":"2021-06-06T09:47:38+02:00","relpermalink":"/publication/teaching-nlp-bracelets-menus/","section":"publication","summary":"We developed an interactive workshop designed to illustrate the NLP and computational linguistics to Italian high schoolers.","tags":["Natural Language Processing","Teaching NLP","Computational Linguistics","Italian"],"title":"Teaching NLP with Bracelets and Restaurant Menus: An Interactive Workshop for Italian Students","type":"publication"},{"authors":["Gabriele Sarti","Dominique Brunato","Felice Dell'Orletta"],"categories":[],"content":"","date":1622965658,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1622965658,"objectID":"b504c42ac08dbbf6345d0363e2895c7d","permalink":"https://gsarti.com/publication/that-looks-hard/","publishdate":"2021-06-06T09:47:38+02:00","relpermalink":"/publication/that-looks-hard/","section":"publication","summary":"This paper investigates the relationship between two complementary perspectives in the human assessment of sentence complexity and how they are modeled in a neural language model (NLM), highlighting how linguistic information encoded in representations changes when the model learns to predict complexity.","tags":["Natural Language Processing","Eye-tracking","Deep Learning","Interpretability","Probing Tasks","Behavioral Data"],"title":"That Looks Hard: Characterizing Linguistic Complexity in Humans and Language Models","type":"publication"},{"authors":null,"categories":null,"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"a76f61f3cbd395de8d63d4d95dd62b47","permalink":"https://gsarti.com/activities/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/activities/","section":"","summary":"A summary of my professional activities","tags":null,"title":"Activities","type":"widget_page"},{"authors":["Gabriele Sarti"],"categories":[],"content":"","date":1608364058,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608364058,"objectID":"818dc3dd24b63aad9f4d61d2ae11ddd9","permalink":"https://gsarti.com/publication/interpreting-nlms-for-lca/","publishdate":"2020-08-26T09:47:38+02:00","relpermalink":"/publication/interpreting-nlms-for-lca/","section":"publication","summary":"This thesis presents a model-driven study of multiple phenomena associated with linguistic complexity, and how those get encoded by neural language models' learned representations.","tags":["Natural Language Processing","Deep Learning","Interpretability","Language Modeling","Transformers","Canonical Correlation Analysis","Garden-path Sentences","Probing Tasks","Representational Similarity Analysis","SyntaxGym","Surprisal","Eye-tracking"],"title":"Interpreting Neural Language Models for Linguistic Complexity Assessment","type":"publication"},{"authors":["Gabriele Sarti"],"categories":[],"content":"","date":1608364058,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608364058,"objectID":"39f32791b458e436d803a421acc3279f","permalink":"https://gsarti.com/publication/umberto-mtsa/","publishdate":"2020-12-19T09:47:38+02:00","relpermalink":"/publication/umberto-mtsa/","section":"publication","summary":"This work describes a self-supervised data augmentation approach used to improve learning models' performances when only a moderate amount of labeled data is available.","tags":["Natural Language Processing","Deep Learning","Multi-task Learning","Transformers","Neural Language Models","Self-training"],"title":"UmBERTo-MTSA@ AcCompl-It: Improving Complexity and Acceptability Prediction with Multi-task Learning on Self-Supervised Annotations","type":"publication"},{"authors":["Ginevra Carbone","Gabriele Sarti"],"categories":[],"content":"","date":1606808858,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606808858,"objectID":"0659fcb83a8eb334bdd07368b02cfb99","permalink":"https://gsarti.com/publication/etc-nlg/","publishdate":"2020-08-26T09:47:38+02:00","relpermalink":"/publication/etc-nlg/","section":"publication","summary":"We present ETC-NLG, an approach leveraging topic modeling annotations to enable fully-unsupervised End-to-end Topic-Conditioned Natural Language Generation over emergent topics in unlabeled document collections.","tags":["Natural Language Processing","Deep Learning","Natural Language Generation","Topic Modeling","Transformers"],"title":"ETC-NLG: End-to-end Topic-Conditioned Natural Language Generation","type":"publication"},{"authors":["Gabriele Sarti"],"categories":[],"content":"(For an overview of the Transformer, see The Illustrated Transformer by Jay Alammar )\nThe Transformer architecture was first proposed in Attention is All you Need as a valid alternative to sequential language modeling approaches like LSTMs and has since then become ubiquitous in the field of Natural Language Processing, pushing the state-of-the-art in most downstream language-related tasks.\nThis year\u0026rsquo;s edition of the International Conference on Learning Representation (ICLR) brought a lot of promising revisions to the original Transformer and its more recent variants BERT and Transformer-XL. Proposed improvements address the well-known weaknesses of Transformers, namely:\nOptimizing the self-attention computation. Injecting linguistically-motivated inductive biases in the model architecture. Making the model more parameter and data-efficient. This post wants to summarize and provide a high-level overview of those contributions, highlighting current trends in the development of better and faster models for Natural Language Processing. All image credits go to their respective paper authors.\nIndex Self-Attention Variants Long-Short Range Attention Tree-Structured Attention with Subtree Masking Hashed Attention eXtra Hop Attention Training Objectives Discriminative Replacement Task Word and Sentence Structural Tasks Type-Constrained Entity Replacement Embeddings Position-Aware Complex Word Embeddings Hierarchical Embeddings Factorized Embedding Parametrization Model Architecture Compressive Memory Reversible Layers Cross-Layer Parameter Sharing Adaptive Depth Estimation Conclusion Self-Attention Variants Scaled dot-product self-attention is one of the main components in the standard Transformer layer, enabling the modelling of dependencies regardless of their distance in the input. The self-attention operation projects an input activation tensor $\\bf A$ to queries $Q$ of dimension $d_k$, keys $K$ of dimension $d_k$ and values $V$ of dimension $d_v$, returning a weighted version of $V$:\n$$\\tag{1} \\text{Attention}(Q,K,V) = \\text{softmax}\\Big(\\frac{QK^T}{\\sqrt d_k}\\Big)V$$\nIn the multi-head self-attention variant, the attention function is applied in parallel to $h$ version of queries, keys and values projected with learned projections $W$, and outputs are finally concatenated and projected again to obtain final values:\n$$\\text{MultiHead}(Q,K,V) = \\text{Concat}(\\text{head}_1,\\dots, \\text{head}_h)W^O$$\n$$\\tag{2} \\text{where } \\text{head}_i = \\text{Attention}(QW_i^Q,KW_i^K,VW_i^V)$$\nThis section presents some variants of the self-attention component that make it more efficient and effective in the context of language applications.\nLong-Short Range Attention Introduced in: Lite Transformer with Long-Short Range Attention by Wu, Liu et al.\nConventional self-attention is deemed as redundant since it was empirically shown to put excessive emphasis on local relations inside a sentence, which can be modeled more efficiently by a standard convolution, as shown also in On the Relationship between Self-Attention and Convolutional Layers. While the redundancy may help model performances in some cases, it is not suitable for lighter applications.\nLong-Short Range Attention (LSRA) makes the computation more efficient by splitting the input into two parts along channel dimensions and feeding each to two modules: a global extractor using standard self-attention and a local extractor using light depth-wise convolutions. Authors report a $2\\times$ reduced overall computation for the model, making it suitable for mobile settings.\nTree-Structured Attention with Subtree Masking Introduced in: Tree-Structured Attention with Hierarchical Accumulation by Nguyen et al.\nA weakness of the standard Transformer is the absence of inductive biases to account for the hierarchical structure of language. This is due in part to the difficulty in operating with tree-like structures that are usually modeled by recurrent or recursive mechanisms while maintaining the constant parallel time complexity of self-attention.\nThe proposed solution leverages constituency parses of input text to build a tree of hidden states, using hierarchical accumulation to build the value of non-terminals as the aggregation of lower representations in the tree. The final output representation is built by performing a weighted aggregation of branch-level representations.\nAn interesting addition is the use of subtree masking to filter out superfluous noise by constraining the attention of each node query only on its subtree descendants. The cost for this inductive bias is an increased computational and memory cost, which is then mitigated using parameter sharing\nHashed Attention Introduced in: Reformer: The Efficient Transformer by Kitaev et al.\nIn the self-attention equation the factor $QK^T$ represents a bottleneck, taking $\\mathcal{O}(L^2)$ for input sequences of length $L$ both in computational and memory complexity. This effectively hinders the possibility of modeling long sequences.\nReformer proposes to restrict the pool of candidates attended by each query to a small set of neighbors found through locally-sensitive hashing. Since LSH bucketing employs random projections, similar vectors may sometimes fall in different neighborhoods; an approach using multiple parallel rounds of hashing is suggested to mitigate this issue. Using LSH attention reduces the computational cost of the self-attention operation to $\\mathcal{O}(L \\log L)$, allowing the model to operate on longer sequences.\neXtra Hop Attention Introduced in: Transformer-XH: Multi-Evidence Reasoning with eXtra Hop Attention by Zhao et al.\nWhile Transformers were optimized to operate on single sequences or pairs of sequences, they can hardly generalize to settings where evidence is scattered in multiple pieces of text, as in the challenging task of multi-hop question answering.\nTransformer-XH introduces a new variant of attention, eXtra Hop Attention, that can be applied to a graph of text sequences connected by edges (e.g. hyperlinks). This new attention mechanism uses the special [CLS] token at the beginning of each sequence as an attention hub that attends to all other connected sequences in the graph. The resulting representation is then combined to the one obtained by standard self-attention through a linear projection. The resulting model shows significant improvements for tasks requiring reasoning over graphs, at the cost of the extra computations introduced by the new attention mechanism.\nTraining Objectives The pre-training of Transformer models is usually achieved by the mean of multiple unsupervised objectives, leverage huge quantities of non-annotated texts. The most common tasks used for this purpose are autoregressive language modeling, also known as standard language modeling (LM), and autoencoding of masked input, often referred to as masked language modeling (MLM).\nThe standard Transformer implementation and its GPT variants adopt the autoregressive approach, leveraging a unidirectional context (forward or backward) inside a sequence $\\textbf{x} = (x_1, \\dots, x_L)$ to estimate next token probability:\n$$p(\\textbf{x}) = \\prod_{l=1}^L p(x_l|\\textbf{x}_{\u0026lt; or \u0026gt;l})$$\nInstead, BERT-like approaches use a bidirectional context to recover a small fraction of the input that was artificially replaced by special [MASK] tokens. This variant was shown to be especially effective for downstream natural language understanding tasks.\nBesides word-level modeling, a sentence-level classification task like next sentence prediction (NSP) is usually added to the training procedure since many important language applications require an understanding of the relationship between two sequences.\nWhile those tasks seem to induce meaningful token and sentence-level representation, many of the approaches covered in this section suggest better alternatives that make learning more efficient and grounded in the structure and the content of the input.\nDiscriminative Replacement Task Introduced in: ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators by Clark et al.\nThe masking strategy used in BERT-like models is quite data inefficient, using only ~15% of the input text to complete the MLM task. However, the percentage of masked data can hardly be increased since having too many masked tokens may degrade the overall context information.\nELECTRA proposes a simple yet effective approach to cope with this inefficiency. A small masked language model is trained and then used as a generator to fill the masked tokens in the input with its predictions, as in normal MLM. However, the new task for the main model will be a discriminative one: instead of predicting masked tokens, the model has to detect which tokens have been replaced by the generator. This allows leveraging the entire input sequence for training. As mentioned by the authors, this approach consistently outperforms MLM pre-training given the same compute budget.\nWord and Sentence Structural Tasks Introduced in: StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding by Wang et al.\nAs seen previously, Transformers do not explicitly account for structures present in the input. While tree-structured attention injects a heavy hierarchical bias in the model architecture, StructBERT adopts two lighter but effective approaches to make the resulting representations more aware of the underlying sequentiality of language.\nThe first is a word structural objective where trigrams inside the inputs are randomly shuffled, and their original position must be reconstructed. This is done in parallel with normal MLM. The sentence structural objective is a lighter variant of the sentence reordering introduced in ERNIE 2.0 and equal to the sentence ordering prediction introduced in ALBERT: given a pair of sentences $(S_1, S_2)$ as input, we ask the model to discriminate whether $S_2$ precedes, follows or is unrelated to $S_1$. This new task extends the standard NSP, which was deemed as too easy for learning meaningful sentence relations. These additions result in significant improvements over standard benchmarks for natural language understanding.\nType-Constrained Entity Replacement Introduced in: Pretrained Encyclopedia: Weakly Supervised Knowledge-Pretrained Language Model by Xiong et al.\nWhile it was shown that pre-trained Transformer models implicitly capture real-world knowledge, their standard training objectives do not explicitly take into account the entity-centric information needed for robust reasoning over real-world settings.\nType-constrained entity replacement is a weakly supervised approach where random entities in the text are replaced with other entities taken from Wikidata that have the same entity type. The model then uses a discriminative objective similar to the one of ELECTRA to determine which entities were replaced. This is done along with MLM in a multi-task setup, and authors report significant improvements in settings requiring a deeper entity understanding, such as open-domain QA and entity typing.\nEmbeddings The original Transformer relies on two sets of embeddings to represent the input sequence:\nLearned word embeddings for each token present in the vocabulary, used as token vector representations for the model.\nFixed positional embeddings (PE), used to inject information about the position of tokens in the sequence. For position $\\text{pos}$ and dimension $i$, those correspond to sinusoidal periodic functions that were empirically shown to perform on par with learned embeddings, and were chosen to enable extrapolation for longer sequences:\n$$PE_{pos, 2i} = \\sin(\\text{pos}/10000^{2i/d_{model}})$$\n$$PE_{pos, 2i + 1} = \\cos(\\text{pos}/10000^{2i/d_{model}})$$\nFor BERT-like models able to operate on multiple input segments, a third set of learned segment embeddings is used to differentiate tokens belonging to different sentences.\nAll those embeddings have the same dimensions and get summed together to obtain an input representation. Approaches introduced in this section aim to inject more structure in the embeddings, or to optimize their dimension for better efficiency.\nPosition-Aware Complex Word Embeddings Introduced in: Encoding word order in complex embeddings by Wang et al.\nWhile PE capture different positions in the input, they do not explicitly take into account the relation between those positions, i.e. ordered relationships such as adjacency or precedence. This problem was already addressed in Transformer-XL by leveraging relative distances between words instead of raw position indices.\nA proposed improvement is to generalize word embeddings to continuous functions defined over positions, extending the solutions to the complex-valued domain to benefit from richer representations. The resulting complex-valued embeddings introduce new parameters for amplitudes, frequencies and initial phases that determine various properties of the embedding such as position sensitivity. Empirical results show that the complex embeddings with parameter-sharing schemas outperform previous embedding approaches without a significant increase in the number of trainable parameters.\nHierarchical Embeddings Introduced in: Tree-Structured Attention with Hierarchical Accumulation by Nguyen et al.\nIn the overview of tree-structured attention, we saw how hierarchical accumulation is used to form a representation based on descendants for nonterminal nodes. This procedure, however, has the disadvantage of not taking into account the hierarchical structure of descendants.\nHierarchical embeddings are used to inject this structural bias by concatenating vertical and horizontal embeddings matrices representing respectively hierarchical ordering inside branches and relationships between siblings nodes in a subtree. Those embeddings are shared across attention heads, thus accounting only for 0.1% of the total parameters.\nFactorized Embedding Parametrization Introduced in: ALBERT: A Lite BERT for Self-supervised Learning of Language Representations by Lan et al.\nIn recent models based on BERT and Transformer-XL the input embeddings size $E$ is tied with the hidden layer size $H$, i.e. $E \\equiv H$. This is very impractical since, to augment the expressivity of hidden representations used to learn context-dependent representation, one should also increase the size of the embedding matrix $\\textbf{M} = V \\times E$, where $V$ is the vocabulary size. Even for relatively small hidden layer dimensions, this results in billions of parameters that are rarely updated during training.\nALBERT authors propose to insert a projection between $E$ and $V$ to make both dimensions independent, an approach that is especially efficient to reduce the parameter count when $H \\gg E$. As a result, an ALBERT base with $E = 128$ and $H = 768$ obtains performances comparable with a BERT base with the same configuration on many downstream tasks, using 21M fewer parameters (89M in Table 3 vs 110M for BERT).\nModel Architecture The original Transformer architecture is composed of an encoder and a decoder, each composed by a stacked sequence of identical layers that transform input embeddings in outputs having the same dimension (hence the name Transformer).\nEach layer of the Transformer encoder is composed of two sublayers, a multi-head self-attention mechanism and a feed-forward network, surrounded by residual connections and followed by layer normalization. The decoder includes a third layer that performs multi-head self-attention over the encoder output and modifies the original self-attention sublayer to prevent attending to future context, as required by the autoregressive language modeling objective presented above.\nBidirectional variants of the Transformer drop the decoder structure and focus solely on the encoder to generate the contextual embeddings needed for various tasks, including MLM.\nTransformer-XL notably introduces a notion of memory for Transformer networks, where hidden states obtained in previous segments are weighted with attention and reused to better model long-term dependencies, preventing context fragmentation.\nThe following approaches try to build on top of current structures to improve long-range modeling, reduce the parameter count, or optimize the computation performed by the model.\nCompressive Memory Introduced in: Compressive Transformers for Long-Range Sequence Modelling by Rae et al.\nIn Transformer-XL\u0026rsquo;s recurrent memory approach, old memories are discarded to enable the storing of new ones in a first-in-first-out fashion. This method accounts only for recency, not taking into account the relevance of information that might get discarded.\nCompressive Transformers builds upon the memory notion by adding a new compressed memory that stores coarse representations of older memories instead of discarding them. Authors try multiple alternatives for the compression function, finally selecting an attention-reconstruction loss that discards information that is not attended by the network. The use of compressive memory shows large improvements over the modeling of infrequent words, with empirical evidence of the network learning to preserve salient information through the compression mechanism.\nReversible Layers Introduced in: Reformer: The Efficient Transformer by Kitaev et al.\nThe main idea behind reversibility is to enable the recovering of activations in any layer of the network by using only activations of the following layer and model parameters. This feature is especially interesting when applied to Transformer models since they are usually composed of a large pile of stacked layers and their memory complexity grows linearly with the layer count.\nReformer introduces reversibility in the Transformer architecture by combining attention and feed-forward sublayers into a single reversible layer. This allows to store activations only for the topmost layer and recover all the other ones by reversing layers during back-propagation, making the model depth irrelevant memory-wise. Further improvements in memory complexity are achieved by chunking independent computations in feed-forward and reversible layers.\nCross-Layer Parameter Sharing Introduced in: ALBERT: A Lite BERT for Self-supervised Learning of Language Representations by Lan et al.\nA simple yet very effective approach to greatly reduce the parameter count inside deep Transformer models is to share parameters across multiple layers, as it was shown in the Universal Transformer paper at ICLR 2019.\nALBERT authors experiment cross-layer parameter sharing for both self-attention and feed-forward sublayers, finding that sharing both weight matrices contributes to bringing down the total parameter count of the model by a factor of $7\\times$ (for embedding size $E = 128$) while only slightly affecting final performances. The use of parameter sharing leads to smoother transition across layers and effectively stabilizes network parameters.\nAdaptive Depth Estimation Introduced in: Depth-Adaptive Transformer by Elbayad et al.\nCurrent models perform a fixed number of computations for each input, regardless of the underlying complexity specific to each sequence. This problem was already highlighted in the Universal Transformer, which proposes a repeated application of the same layer with adaptive computation time (ACT), but the resulting increase in per-layer weights considerably reduce the overall network speed.\nDepth-adaptive Transformer solves this issue by encoding a sequence with a standard Transformer encoder and decoding it with a variable number of steps. To do so, a classifier is attached to each repeated layer of the decoder and the whole set is then trained with aligned and mixed training (see image) using the anytime prediction approach first introduced in the field of computer vision. Authors explore different mechanisms to adaptively control the amount of computation both on sequence level and on a per-token basis and conclude that an adaptive reduction of more than 75% of decoder layers can be applied without any loss in accuracy on machine translation tasks.\nConclusion Many of the approaches introduced at ICLR 2020 offer widely applicable solutions to specific problems that characterize the original Transformer architecture, ranging from the self-attention computation to the model structure itself.\nMany of these approaches seem promising for future developments of the Transformer and, most importantly, are likely to bring complementary improvements once many of them included in a single architecture.\nMy hope for ICLR 2021 is to see more incremental work that puts together already-existing strategies to highlight the most effective combinations between them.\nSee also: Whatâ€™s new for Transformers at the ICLR 2020 Conference? by Sergi Castella\n","date":1588526856,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588526856,"objectID":"754ac8a82f8a2d875490e527ac58aa9b","permalink":"https://gsarti.com/post/iclr2020-transformers/","publishdate":"2020-05-03T19:27:36+02:00","relpermalink":"/post/iclr2020-transformers/","section":"post","summary":" A summary of promising directions from ICLR 2020 for better and faster pretrained tranformers language models. ","tags":["Natural Language Processing","Language Modeling","Deep Learning","Transformers","ICLR2020","Word Embeddings","Self-Attention"],"title":"ICLR 2020 Trends: Better \u0026 Faster Transformers for Natural Language Processing","type":"post"},{"authors":["Gabriele Sarti","Francesco Zuppichini","Tommaso Rodani","Marco Franzon","Mirko Lai"],"categories":["Natural Language Processing"],"content":"In 2020, more than 3,000 scientific studies have been published on the SARS-CoV-2 virus and on the Covid-19 pathology. The total number of articles on the topic of coronaviruses exceeds 40,000 units. Such a volume of scientific production makes it impossible for doctors and researchers to keep up with the latest discoveries without the support of adequate digital platforms that are currently nowhere in sight.\nTo make up for this shortcoming, we propose an artificial intelligence system associated with a web application to perform natural language semantic querying inside the COVID-19 Open Research Dataset published by the American nonprofit AllenAI. The system leverages state-of-the-art neural language models trained on scientific publications in the biomedical domain for optimal retrieval performances. The adoption of the system aims to facilitate knowledge sharing across the scientific community and to accelerate the development of adequate drugs and vaccines to counter the ongoing pandemic.\nThe project is led by Gabriele Sarti in collaboration with Area Science Park and the Italian Association of Computational Linguistics. The project used to be publicly available at covidbrowser.areasciencepark.it. You can now refer to the code implementation on GitHub.\n","date":1586563200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586563200,"objectID":"af817b3031f8d4070d0147dac0c93688","permalink":"https://gsarti.com/project/covid-browser/","publishdate":"2020-04-11T00:00:00Z","relpermalink":"/project/covid-browser/","section":"project","summary":"A semantic browser for SARS-CoV-2 and COVID-19 powered by neural language models.","tags":["Natural Language Processing","Deep Learning","Information Extraction","Kaggle Competition","AREA Science Park","Italian Association for Computational Linguistics"],"title":"Covid-19 Semantic Browser","type":"project"},{"authors":["Gabriele Sarti"],"categories":["Natural Language Processing","Academic"],"content":"","date":1574380800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1574380800,"objectID":"01e69dceb562087e59061477e0d6505e","permalink":"https://gsarti.com/talk/neural-lm/","publishdate":"2019-10-27T17:59:41+01:00","relpermalink":"/talk/neural-lm/","section":"talk","summary":"An overview of the latest advances in the field of NLP, with a focus on neural models and language understanding.","tags":["Natural Language Processing","Natural Language Understanding","Language Modeling","Deep Learning","StaTalk","University of Trieste"],"title":"Neural Language Models: the New Frontier of Natural Language Understanding","type":"talk"},{"authors":["Gabriele Sarti","Felice Dell'Orletta","Cristina Fenu"],"categories":["Digital Humanities","Divulgative"],"content":"","date":1572519641,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572519641,"objectID":"3519e5984477ceede5ff698b8a0d638c","permalink":"https://gsarti.com/talk/literary-ordnance/","publishdate":"2019-10-27T17:59:41+01:00","relpermalink":"/talk/literary-ordnance/","section":"talk","summary":"Discussing the applications of AI and NLP in the fields of literature and digital humanities.","tags":["Natural Language Processing","Digital Humanities","Natural Language Generation","Science+Fiction Festival"],"title":"The Literary Ordnance: When the Writer is an AI","type":"talk"},{"authors":["Gabriele Sarti","Cristina Fenu","Eric Medvet"],"categories":["Digital Humanities"],"content":"We developed an interactive experience putting together NLP and literature to raise awareness on the latest developments in language modeling and natural language generation. Participants received a printed letter written by a neural language model (GPT-2) fine-tuned on the Italian epistolary corpus of Italo Svevo, an italian writer of the 20th century. Participants could choose among several topics discussed by the author in his letters, and were also given some context on author\u0026rsquo;s life and literary production by Cristina Fenu, a digital humanist working at the Svevian Museum of Trieste. An open-source implementation will soon be available on Github.\n","date":1569542400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569542400,"objectID":"effd515fe0067a47ad486378929beac6","permalink":"https://gsarti.com/project/aitalo-svevo/","publishdate":"2019-09-27T00:00:00Z","relpermalink":"/project/aitalo-svevo/","section":"project","summary":"Generating letters with a neural language model in the style of Italo Svevo, a famous italian writer of the 20th century.","tags":["Natural Language Generation","Language Modeling","Italian NLP","Trieste Next"],"title":"AItalo Svevo: Letters from an Artificial Intelligence","type":"project"},{"authors":["Gabriele Sarti"],"categories":["Deep Learning","Academic"],"content":"","date":1563408000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563408000,"objectID":"0de45716e8e1d68d3d4fce6d94bc1eac","permalink":"https://gsarti.com/talk/lth/","publishdate":"2019-11-22T10:42:03+01:00","relpermalink":"/talk/lth/","section":"talk","summary":"Is it possible to induce sparseness in neural networks while preserving its performances? An overview of latest advances in making neural approaches more parsimonious","tags":["Deep Learning","Course Presentation","University of Trieste"],"title":"Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks","type":"talk"},{"authors":["Gabriele Sarti","Leonardo Stincone","Andrea Lorenzon"],"categories":["Computer Vision"],"content":"As our final project for the course of Statistical Machine Learning held by Prof. Luca Bortolussi we explored different statistical and deep approaches to the problem of detecting tumors in histopathologic scans. We notably tried a random forest on distributional features extracted by pigment segmentation, a state-of-the-art DenseNet and a Capsule Network with Dynamic Routing.\n","date":1561680000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561680000,"objectID":"0964199e2bac61b39ed684cfcebe5fa0","permalink":"https://gsarti.com/project/cancer-detection/","publishdate":"2019-06-28T00:00:00Z","relpermalink":"/project/cancer-detection/","section":"project","summary":"A journey into the state of the art of histopathologic cancer detection approaches.","tags":["Computer Vision","Capsule Networks","Cancer Detection","Kaggle Competition","University of Trieste"],"title":"Histopathologic Cancer Detection with Neural Networks","type":"project"},{"authors":["Alessio Miaschi","Gabriele Sarti","Dominique Brunato","Felice Dellâ€™Orletta","Giulia Venturi"],"categories":[],"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"a1f30cd72145308e12876ba2e93c61b3","permalink":"https://gsarti.com/publication/italian-transformers/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/italian-transformers/","section":"publication","summary":"We investigate whether and how using different architectures of probing models affects the performance of Italian transformers in encoding a wide spectrum of linguistic features.","tags":["Natural Language Processing","Deep Learning","Interpretability","Italian Language","Transformers","Neural Language Models","Probing Task"],"title":"Probing Linguistic Knowledge in Italian Neural Language Models across Language Varieties","type":"publication"}]