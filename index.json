[{"authors":["gsarti"],"categories":null,"content":"Hey there! I am currently working with the ItaliaNLP Lab and the L2R Lab under the joint supervision of Felice Dell\u0026rsquo;Orletta and Davide Crepaldi for my thesis project on NLP and psycholinguistics perspectives for linguistic complexity assessment.\nMy research interests focus on interpreting deep models and understanding their learning dynamics, in particular when dealing with natural language. I am also interested in positive social applications of machine learning and ethical AI.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"9a59029f2594915e13ca7ed4b3a51fc3","permalink":"https://gsarti.com/authors/gsarti/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/gsarti/","section":"authors","summary":"Hey there! I am currently working with the ItaliaNLP Lab and the L2R Lab under the joint supervision of Felice Dell\u0026rsquo;Orletta and Davide Crepaldi for my thesis project on NLP and psycholinguistics perspectives for linguistic complexity assessment.\nMy research interests focus on interpreting deep models and understanding their learning dynamics, in particular when dealing with natural language. I am also interested in positive social applications of machine learning and ethical AI.","tags":null,"title":"Gabriele Sarti","type":"authors"},{"authors":["Ginevra Carbone","Gabriele Sarti"],"categories":[],"content":"","date":1598428058,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598428058,"objectID":"0659fcb83a8eb334bdd07368b02cfb99","permalink":"https://gsarti.com/publication/etc-nlg/","publishdate":"2020-08-26T09:47:38+02:00","relpermalink":"/publication/etc-nlg/","section":"publication","summary":"We present ETC-NLG, an approach leveraging topic modeling annotations to enable fully-unsupervised End-to-end Topic-Conditioned Natural Language Generation over emergent topics in unlabeled document collections.","tags":["Natural Language Processing","Deep Learning","Natural Language Generation","Topic Modeling","Transformers"],"title":"ETC-NLG: End-to-end Topic-Conditioned Natural Language Generation","type":"publication"},{"authors":["Gabriele Sarti"],"categories":[],"content":"(For an overview of the Transformer, see The Illustrated Transformer by Jay Alammar )\nThe Transformer architecture was first proposed in Attention is All you Need as a valid alternative to sequential language modeling approaches like LSTMs and has since then become ubiquitous in the field of Natural Language Processing, pushing the state-of-the-art in most downstream language-related tasks.\nThis year\u0026rsquo;s edition of the International Conference on Learning Representation (ICLR) brought a lot of promising revisions to the original Transformer and its more recent variants BERT and Transformer-XL. Proposed improvements address the well-known weaknesses of Transformers, namely:\n Optimizing the self-attention computation. Injecting linguistically-motivated inductive biases in the model architecture. Making the model more parameter and data-efficient.  This post wants to summarize and provide a high-level overview of those contributions, highlighting current trends in the development of better and faster models for Natural Language Processing. All image credits go to their respective paper authors.\n Index  Self-Attention Variants  Long-Short Range Attention Tree-Structured Attention with Subtree Masking Hashed Attention eXtra Hop Attention   Training Objectives  Discriminative Replacement Task Word and Sentence Structural Tasks Type-Constrained Entity Replacement   Embeddings  Position-Aware Complex Word Embeddings Hierarchical Embeddings Factorized Embedding Parametrization   Model Architecture  Compressive Memory Reversible Layers Cross-Layer Parameter Sharing Adaptive Depth Estimation   Conclusion   Self-Attention Variants Scaled dot-product self-attention is one of the main components in the standard Transformer layer, enabling the modelling of dependencies regardless of their distance in the input. The self-attention operation projects an input activation tensor $\\bf A$ to queries $Q$ of dimension $d_k$, keys $K$ of dimension $d_k$ and values $V$ of dimension $d_v$, returning a weighted version of $V$:\n$$\\tag{1} \\text{Attention}(Q,K,V) = \\text{softmax}\\Big(\\frac{QK^T}{\\sqrt d_k}\\Big)V$$\nIn the multi-head self-attention variant, the attention function is applied in parallel to $h$ version of queries, keys and values projected with learned projections $W$, and outputs are finally concatenated and projected again to obtain final values:\n$$\\text{MultiHead}(Q,K,V) = \\text{Concat}(\\text{head}_1,\\dots, \\text{head}_h)W^O$$\n$$\\tag{2} \\text{where } \\text{head}_i = \\text{Attention}(QW_i^Q,KW_i^K,VW_i^V)$$\nThis section presents some variants of the self-attention component that make it more efficient and effective in the context of language applications.\nLong-Short Range Attention Introduced in: Lite Transformer with Long-Short Range Attention by Wu, Liu et al.\nConventional self-attention is deemed as redundant since it was empirically shown to put excessive emphasis on local relations inside a sentence, which can be modeled more efficiently by a standard convolution, as shown also in On the Relationship between Self-Attention and Convolutional Layers. While the redundancy may help model performances in some cases, it is not suitable for lighter applications.\nLong-Short Range Attention (LSRA) makes the computation more efficient by splitting the input into two parts along channel dimensions and feeding each to two modules: a global extractor using standard self-attention and a local extractor using light depth-wise convolutions. Authors report a $2\\times$ reduced overall computation for the model, making it suitable for mobile settings.\nTree-Structured Attention with Subtree Masking Introduced in: Tree-Structured Attention with Hierarchical Accumulation by Nguyen et al.\nA weakness of the standard Transformer is the absence of inductive biases to account for the hierarchical structure of language. This is due in part to the difficulty in operating with tree-like structures that are usually modeled by recurrent or recursive mechanisms while maintaining the constant parallel time complexity of self-attention.\nThe proposed solution leverages constituency parses of input text to build a tree of hidden states, using hierarchical accumulation to build the value of non-terminals as the aggregation of lower representations in the tree. The final output representation is built by performing a weighted aggregation of branch-level representations.\nAn interesting addition is the use of subtree masking to filter out superfluous noise by constraining the attention of each node query only on its subtree descendants. The cost for this inductive bias is an increased computational and memory cost, which is then mitigated using parameter sharing\nHashed Attention Introduced in: Reformer: The Efficient Transformer by Kitaev et al.\nIn the self-attention equation the factor $QK^T$ represents a bottleneck, taking $\\mathcal{O}(L^2)$ for input sequences of length $L$ both in computational and memory complexity. This effectively hinders the possibility of modeling long sequences.\nReformer proposes to restrict the pool of candidates attended by each query to a small set of neighbors found through locally-sensitive hashing. Since LSH bucketing employs random projections, similar vectors may sometimes fall in different neighborhoods; an approach using multiple parallel rounds of hashing is suggested to mitigate this issue. Using LSH attention reduces the computational cost of the self-attention operation to $\\mathcal{O}(L \\log L)$, allowing the model to operate on longer sequences.\neXtra Hop Attention Introduced in: Transformer-XH: Multi-Evidence Reasoning with eXtra Hop Attention by Zhao et al.\nWhile Transformers were optimized to operate on single sequences or pairs of sequences, they can hardly generalize to settings where evidence is scattered in multiple pieces of text, as in the challenging task of multi-hop question answering.\nTransformer-XH introduces a new variant of attention, eXtra Hop Attention, that can be applied to a graph of text sequences connected by edges (e.g. hyperlinks). This new attention mechanism uses the special [CLS] token at the beginning of each sequence as an attention hub that attends to all other connected sequences in the graph. The resulting representation is then combined to the one obtained by standard self-attention through a linear projection. The resulting model shows significant improvements for tasks requiring reasoning over graphs, at the cost of the extra computations introduced by the new attention mechanism.\nTraining Objectives The pre-training of Transformer models is usually achieved by the mean of multiple unsupervised objectives, leverage huge quantities of non-annotated texts. The most common tasks used for this purpose are autoregressive language modeling, also known as standard language modeling (LM), and autoencoding of masked input, often referred to as masked language modeling (MLM).\nThe standard Transformer implementation and its GPT variants adopt the autoregressive approach, leveraging a unidirectional context (forward or backward) inside a sequence $\\textbf{x} = (x_1, \\dots, x_L)$ to estimate next token probability:\n$$p(\\textbf{x}) = \\prod_{l=1}^L p(x_l|\\textbf{x}_{\u0026lt; or \u0026gt;l})$$\nInstead, BERT-like approaches use a bidirectional context to recover a small fraction of the input that was artificially replaced by special [MASK] tokens. This variant was shown to be especially effective for downstream natural language understanding tasks.\nBesides word-level modeling, a sentence-level classification task like next sentence prediction (NSP) is usually added to the training procedure since many important language applications require an understanding of the relationship between two sequences.\nWhile those tasks seem to induce meaningful token and sentence-level representation, many of the approaches covered in this section suggest better alternatives that make learning more efficient and grounded in the structure and the content of the input.\nDiscriminative Replacement Task Introduced in: ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators by Clark et al.\nThe masking strategy used in BERT-like models is quite data inefficient, using only ~15% of the input text to complete the MLM task. However, the percentage of masked data can hardly be increased since having too many masked tokens may degrade the overall context information.\nELECTRA proposes a simple yet effective approach to cope with this inefficiency. A small masked language model is trained and then used as a generator to fill the masked tokens in the input with its predictions, as in normal MLM. However, the new task for the main model will be a discriminative one: instead of predicting masked tokens, the model has to detect which tokens have been replaced by the generator. This allows leveraging the entire input sequence for training. As mentioned by the authors, this approach consistently outperforms MLM pre-training given the same compute budget.\nWord and Sentence Structural Tasks Introduced in: StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding by Wang et al.\nAs seen previously, Transformers do not explicitly account for structures present in the input. While tree-structured attention injects a heavy hierarchical bias in the model architecture, StructBERT adopts two lighter but effective approaches to make the resulting representations more aware of the underlying sequentiality of language.\nThe first is a word structural objective where trigrams inside the inputs are randomly shuffled, and their original position must be reconstructed. This is done in parallel with normal MLM. The sentence structural objective is a lighter variant of the sentence reordering introduced in ERNIE 2.0 and equal to the sentence ordering prediction introduced in ALBERT: given a pair of sentences $(S_1, S_2)$ as input, we ask the model to discriminate whether $S_2$ precedes, follows or is unrelated to $S_1$. This new task extends the standard NSP, which was deemed as too easy for learning meaningful sentence relations. These additions result in significant improvements over standard benchmarks for natural language understanding.\nType-Constrained Entity Replacement Introduced in: Pretrained Encyclopedia: Weakly Supervised Knowledge-Pretrained Language Model by Xiong et al.\nWhile it was shown that pre-trained Transformer models implicitly capture real-world knowledge, their standard training objectives do not explicitly take into account the entity-centric information needed for robust reasoning over real-world settings.\nType-constrained entity replacement is a weakly supervised approach where random entities in the text are replaced with other entities taken from Wikidata that have the same entity type. The model then uses a discriminative objective similar to the one of ELECTRA to determine which entities were replaced. This is done along with MLM in a multi-task setup, and authors report significant improvements in settings requiring a deeper entity understanding, such as open-domain QA and entity typing.\nEmbeddings The original Transformer relies on two sets of embeddings to represent the input sequence:\n  Learned word embeddings for each token present in the vocabulary, used as token vector representations for the model.\n  Fixed positional embeddings (PE), used to inject information about the position of tokens in the sequence. For position $\\text{pos}$ and dimension $i$, those correspond to sinusoidal periodic functions that were empirically shown to perform on par with learned embeddings, and were chosen to enable extrapolation for longer sequences:\n  $$PE_{pos, 2i} = \\sin(\\text{pos}/10000^{2i/d_{model}})$$\n$$PE_{pos, 2i + 1} = \\cos(\\text{pos}/10000^{2i/d_{model}})$$\nFor BERT-like models able to operate on multiple input segments, a third set of learned segment embeddings is used to differentiate tokens belonging to different sentences.\nAll those embeddings have the same dimensions and get summed together to obtain an input representation. Approaches introduced in this section aim to inject more structure in the embeddings, or to optimize their dimension for better efficiency.\nPosition-Aware Complex Word Embeddings Introduced in: Encoding word order in complex embeddings by Wang et al.\nWhile PE capture different positions in the input, they do not explicitly take into account the relation between those positions, i.e. ordered relationships such as adjacency or precedence. This problem was already addressed in Transformer-XL by leveraging relative distances between words instead of raw position indices.\nA proposed improvement is to generalize word embeddings to continuous functions defined over positions, extending the solutions to the complex-valued domain to benefit from richer representations. The resulting complex-valued embeddings introduce new parameters for amplitudes, frequencies and initial phases that determine various properties of the embedding such as position sensitivity. Empirical results show that the complex embeddings with parameter-sharing schemas outperform previous embedding approaches without a significant increase in the number of trainable parameters.\nHierarchical Embeddings Introduced in: Tree-Structured Attention with Hierarchical Accumulation by Nguyen et al.\nIn the overview of tree-structured attention, we saw how hierarchical accumulation is used to form a representation based on descendants for nonterminal nodes. This procedure, however, has the disadvantage of not taking into account the hierarchical structure of descendants.\nHierarchical embeddings are used to inject this structural bias by concatenating vertical and horizontal embeddings matrices representing respectively hierarchical ordering inside branches and relationships between siblings nodes in a subtree. Those embeddings are shared across attention heads, thus accounting only for 0.1% of the total parameters.\nFactorized Embedding Parametrization Introduced in: ALBERT: A Lite BERT for Self-supervised Learning of Language Representations by Lan et al.\nIn recent models based on BERT and Transformer-XL the input embeddings size $E$ is tied with the hidden layer size $H$, i.e. $E \\equiv H$. This is very impractical since, to augment the expressivity of hidden representations used to learn context-dependent representation, one should also increase the size of the embedding matrix $\\textbf{M} = V \\times E$, where $V$ is the vocabulary size. Even for relatively small hidden layer dimensions, this results in billions of parameters that are rarely updated during training.\nALBERT authors propose to insert a projection between $E$ and $V$ to make both dimensions independent, an approach that is especially efficient to reduce the parameter count when $H \\gg E$. As a result, an ALBERT base with $E = 128$ and $H = 768$ obtains performances comparable with a BERT base with the same configuration on many downstream tasks, using 21M fewer parameters (89M in Table 3 vs 110M for BERT).\nModel Architecture The original Transformer architecture is composed of an encoder and a decoder, each composed by a stacked sequence of identical layers that transform input embeddings in outputs having the same dimension (hence the name Transformer).\nEach layer of the Transformer encoder is composed of two sublayers, a multi-head self-attention mechanism and a feed-forward network, surrounded by residual connections and followed by layer normalization. The decoder includes a third layer that performs multi-head self-attention over the encoder output and modifies the original self-attention sublayer to prevent attending to future context, as required by the autoregressive language modeling objective presented above.\nBidirectional variants of the Transformer drop the decoder structure and focus solely on the encoder to generate the contextual embeddings needed for various tasks, including MLM.\nTransformer-XL notably introduces a notion of memory for Transformer networks, where hidden states obtained in previous segments are weighted with attention and reused to better model long-term dependencies, preventing context fragmentation.\nThe following approaches try to build on top of current structures to improve long-range modeling, reduce the parameter count, or optimize the computation performed by the model.\nCompressive Memory Introduced in: Compressive Transformers for Long-Range Sequence Modelling by Rae et al.\nIn Transformer-XL\u0026rsquo;s recurrent memory approach, old memories are discarded to enable the storing of new ones in a first-in-first-out fashion. This method accounts only for recency, not taking into account the relevance of information that might get discarded.\nCompressive Transformers builds upon the memory notion by adding a new compressed memory that stores coarse representations of older memories instead of discarding them. Authors try multiple alternatives for the compression function, finally selecting an attention-reconstruction loss that discards information that is not attended by the network. The use of compressive memory shows large improvements over the modeling of infrequent words, with empirical evidence of the network learning to preserve salient information through the compression mechanism.\nReversible Layers Introduced in: Reformer: The Efficient Transformer by Kitaev et al.\nThe main idea behind reversibility is to enable the recovering of activations in any layer of the network by using only activations of the following layer and model parameters. This feature is especially interesting when applied to Transformer models since they are usually composed of a large pile of stacked layers and their memory complexity grows linearly with the layer count.\nReformer introduces reversibility in the Transformer architecture by combining attention and feed-forward sublayers into a single reversible layer. This allows to store activations only for the topmost layer and recover all the other ones by reversing layers during back-propagation, making the model depth irrelevant memory-wise. Further improvements in memory complexity are achieved by chunking independent computations in feed-forward and reversible layers.\nCross-Layer Parameter Sharing Introduced in: ALBERT: A Lite BERT for Self-supervised Learning of Language Representations by Lan et al.\nA simple yet very effective approach to greatly reduce the parameter count inside deep Transformer models is to share parameters across multiple layers, as it was shown in the Universal Transformer paper at ICLR 2019.\nALBERT authors experiment cross-layer parameter sharing for both self-attention and feed-forward sublayers, finding that sharing both weight matrices contributes to bringing down the total parameter count of the model by a factor of $7\\times$ (for embedding size $E = 128$) while only slightly affecting final performances. The use of parameter sharing leads to smoother transition across layers and effectively stabilizes network parameters.\nAdaptive Depth Estimation Introduced in: Depth-Adaptive Transformer by Elbayad et al.\nCurrent models perform a fixed number of computations for each input, regardless of the underlying complexity specific to each sequence. This problem was already highlighted in the Universal Transformer, which proposes a repeated application of the same layer with adaptive computation time (ACT), but the resulting increase in per-layer weights considerably reduce the overall network speed.\nDepth-adaptive Transformer solves this issue by encoding a sequence with a standard Transformer encoder and decoding it with a variable number of steps. To do so, a classifier is attached to each repeated layer of the decoder and the whole set is then trained with aligned and mixed training (see image) using the anytime prediction approach first introduced in the field of computer vision. Authors explore different mechanisms to adaptively control the amount of computation both on sequence level and on a per-token basis and conclude that an adaptive reduction of more than 75% of decoder layers can be applied without any loss in accuracy on machine translation tasks.\nConclusion Many of the approaches introduced at ICLR 2020 offer widely applicable solutions to specific problems that characterize the original Transformer architecture, ranging from the self-attention computation to the model structure itself.\nMany of these approaches seem promising for future developments of the Transformer and, most importantly, are likely to bring complementary improvements once many of them included in a single architecture.\nMy hope for ICLR 2021 is to see more incremental work that puts together already-existing strategies to highlight the most effective combinations between them.\nSee also: Whatâ€™s new for Transformers at the ICLR 2020 Conference? by Sergi Castella\n","date":1588526856,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588526856,"objectID":"754ac8a82f8a2d875490e527ac58aa9b","permalink":"https://gsarti.com/post/iclr2020-transformers/","publishdate":"2020-05-03T19:27:36+02:00","relpermalink":"/post/iclr2020-transformers/","section":"post","summary":" A summary of promising directions from ICLR 2020 for better and faster pretrained tranformers language models. ","tags":["Natural Language Processing","Language Modeling","Deep Learning","Transformers","ICLR2020","Word Embeddings","Self-Attention"],"title":"ICLR 2020 Trends: Better \u0026 Faster Transformers for Natural Language Processing","type":"post"},{"authors":["Gabriele Sarti","Francesco Zuppichini","Tommaso Rodani","Marco Franzon","Mirko Lai"],"categories":["Natural Language Processing"],"content":"In 2020, more than 3,000 scientific studies have been published on the SARS-CoV-2 virus and on the Covid-19 pathology. The total number of articles on the topic of coronaviruses exceeds 40,000 units. Such a volume of scientific production makes it impossible for doctors and researchers to keep up with the latest discoveries without the support of adequate digital platforms that are currently nowhere in sight.\nTo make up for this shortcoming, we propose an artificial intelligence system associated with a web application to perform natural language semantic querying inside the COVID-19 Open Research Dataset published by the American nonprofit AllenAI. The system leverages state-of-the-art neural language models trained on scientific publications in the biomedical domain for optimal retrieval performances. The adoption of the system aims to facilitate knowledge sharing across the scientific community and to accelerate the development of adequate drugs and vaccines to counter the ongoing pandemic.\nThe project is led by Gabriele Sarti in collaboration with Area Science Park and the Italian Association of Computational Linguistics. The project is currently being tested and publicly available at covidbrowser.areasciencepark.it.\n","date":1586563200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586563200,"objectID":"af817b3031f8d4070d0147dac0c93688","permalink":"https://gsarti.com/project/covid-browser/","publishdate":"2020-04-11T00:00:00Z","relpermalink":"/project/covid-browser/","section":"project","summary":"A semantic browser for SARS-CoV-2 and COVID-19 powered by neural language models.","tags":["Natural Language Processing","Deep Learning","Information Extraction","Kaggle Competition","AREA Science Park","Italian Association for Computational Linguistics"],"title":"Covid-19 Semantic Browser","type":"project"},{"authors":["Gabriele Sarti"],"categories":["Natural Language Processing","Academic"],"content":"","date":1574380800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1574380800,"objectID":"01e69dceb562087e59061477e0d6505e","permalink":"https://gsarti.com/talk/neural-lm/","publishdate":"2019-10-27T17:59:41+01:00","relpermalink":"/talk/neural-lm/","section":"talk","summary":"An overview of the latest advances in the field of NLP, with a focus on neural models and language understanding.","tags":["Natural Language Processing","Natural Language Understanding","Language Modeling","Deep Learning","StaTalk","University of Trieste"],"title":"Neural Language Models: the New Frontier of Natural Language Understanding","type":"talk"},{"authors":["Gabriele Sarti","Felice Dell'Orletta","Cristina Fenu"],"categories":["Digital Humanities","Divulgative"],"content":"","date":1572519641,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572519641,"objectID":"3519e5984477ceede5ff698b8a0d638c","permalink":"https://gsarti.com/talk/literary-ordnance/","publishdate":"2019-10-27T17:59:41+01:00","relpermalink":"/talk/literary-ordnance/","section":"talk","summary":"Discussing the applications of AI and NLP in the fields of literature and digital humanities.","tags":["Natural Language Processing","Digital Humanities","Natural Language Generation","Science+Fiction Festival"],"title":"The Literary Ordnance: When the Writer is an AI","type":"talk"},{"authors":["Gabriele Sarti","Cristina Fenu","Eric Medvet"],"categories":["Digital Humanities"],"content":"We developed an interactive experience putting together NLP and literature to raise awareness on the latest developments in language modeling and natural language generation. Participants received a printed letter written by a neural language model (GPT-2) fine-tuned on the Italian epistolary corpus of Italo Svevo, an italian writer of the 20th century. Participants could choose among several topics discussed by the author in his letters, and were also given some context on author\u0026rsquo;s life and literary production by Cristina Fenu, a digital humanist working at the Svevian Museum of Trieste. An open-source implementation will soon be available on Github.\n","date":1569542400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569542400,"objectID":"effd515fe0067a47ad486378929beac6","permalink":"https://gsarti.com/project/aitalo-svevo/","publishdate":"2019-09-27T00:00:00Z","relpermalink":"/project/aitalo-svevo/","section":"project","summary":"Generating letters with a neural language model in the style of Italo Svevo, a famous italian writer of the 20th century.","tags":["Natural Language Generation","Language Modeling","Italian NLP","Trieste Next"],"title":"AItalo Svevo: Letters from an Artificial Intelligence","type":"project"},{"authors":["Gabriele Sarti"],"categories":["Deep Learning","Academic"],"content":"","date":1563408000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563408000,"objectID":"0de45716e8e1d68d3d4fce6d94bc1eac","permalink":"https://gsarti.com/talk/lth/","publishdate":"2019-11-22T10:42:03+01:00","relpermalink":"/talk/lth/","section":"talk","summary":"Is it possible to induce sparseness in neural networks while preserving its performances? An overview of latest advances in making neural approaches more parsimonious","tags":["Deep Learning","Course Presentation","University of Trieste"],"title":"Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks","type":"talk"},{"authors":["Gabriele Sarti","Leonardo Stincone","Andrea Lorenzon"],"categories":["Computer Vision"],"content":"As our final project for the course of Statistical Machine Learning held by Prof. Luca Bortolussi we explored different statistical and deep approaches to the problem of detecting tumors in histopathologic scans. We notably tried a random forest on distributional features extracted by pigment segmentation, a state-of-the-art DenseNet and a Capsule Network with Dynamic Routing.\n","date":1561680000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561680000,"objectID":"0964199e2bac61b39ed684cfcebe5fa0","permalink":"https://gsarti.com/project/cancer-detection/","publishdate":"2019-06-28T00:00:00Z","relpermalink":"/project/cancer-detection/","section":"project","summary":"A journey into the state of the art of histopathologic cancer detection approaches.","tags":["Computer Vision","Capsule Networks","Cancer Detection","Kaggle Competition","University of Trieste"],"title":"Histopathologic Cancer Detection with Neural Networks","type":"project"}]