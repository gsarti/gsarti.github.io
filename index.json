[{"authors":["gsarti"],"categories":null,"content":"Welcome to my website! 👋 I am a PhD student at the Computational Linguistics Group of the University of Groningen and member of the InDeep consortium, working on user-centric interpretability for neural machine translation. I am also the main developer of the Inseq library. My supervisors are Arianna Bisazza, Malvina Nissim and Grzegorz Chrupała.\nPreviously, I was a research intern at Amazon Translate NYC, a research scientist at Aindo, a Data Science MSc student at the University of Trieste and a co-founder of the AI Student Society.\nMy research focuses on interpretability for generative language models, with a particular interest to end-users\u0026rsquo; benefits and the usage of human behavioral signals. I am also into causality topics and open source collaboration.\nYour (anonymous) feedback is always welcome! 🙂\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"9a59029f2594915e13ca7ed4b3a51fc3","permalink":"https://gsarti.com/authors/gsarti/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/gsarti/","section":"authors","summary":"Welcome to my website! 👋 I am a PhD student at the Computational Linguistics Group of the University of Groningen and member of the InDeep consortium, working on user-centric interpretability for neural machine translation. I am also the main developer of the Inseq library. My supervisors are Arianna Bisazza, Malvina Nissim and Grzegorz Chrupała.\nPreviously, I was a research intern at Amazon Translate NYC, a research scientist at Aindo, a Data Science MSc student at the University of Trieste and a co-founder of the AI Student Society.","tags":null,"title":"Gabriele Sarti","type":"authors"},{"authors":["Gabriele Sarti","Vilém Zouhar","Grzegorz Chrupała","Ana Guerberof Arenas","Malvina Nissim","Arianna Bisazza"],"categories":["Natural Language Processing"],"content":"","date":1741215600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1741215600,"objectID":"6b6ea543dd2b2eb3a75431ac96fdcd8c","permalink":"https://gsarti.com/publication/qe4pe/","publishdate":"2025-03-06T01:00:00+02:00","relpermalink":"/publication/qe4pe/","section":"publication","summary":"We investigate the impact of word-level quality estimation on MT post-editing with 42 professional post-editors.","tags":["Natural Language Processing","Post-editing","Machine Translation","Quality Estimation","Dataset","Behavioral Data","Human Evaluation"],"title":"QE4PE: Word-level Quality Estimation for Human Post-Editing","type":"publication"},{"authors":["Gabriele Sarti"],"categories":["Natural Language Processing","Divulgative"],"content":"","date":1733770800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1733770800,"objectID":"852f21fa9d0194e69e32e5805717c342","permalink":"https://gsarti.com/talk/ai2s-talk-2024/","publishdate":"2022-05-23T00:00:00Z","relpermalink":"/talk/ai2s-talk-2024/","section":"talk","summary":"Questo intervento sarà mirato a demistificare il funzionamento dei modelli del linguaggio (Large Language Models), ed evidenziare come lo studio di questi sistemi come 'artefatti cognitivi' possa contribuire a una migliore comprensione dei meccanismi di ragionamento (umani e non), e dei bias nella società che ci circonda.","tags":["Natural Language Processing","Interpretability"],"title":"Aprire la scatola nera dei modelli del linguaggio: rischi e opportunità","type":"talk"},{"authors":["Gabriele Sarti"],"categories":["Natural Language Processing","Academic"],"content":"","date":1733407200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1733407200,"objectID":"7365c7cea1904382821c750fbb02ad76","permalink":"https://gsarti.com/talk/verbalized-rebus-clic2024/","publishdate":"2024-02-26T00:00:00Z","relpermalink":"/talk/verbalized-rebus-clic2024/","section":"talk","summary":"Oral presentation at CLiC-it 2024","tags":["Natural Language Processing","HuggingFace","Italian","Large Language Models","Evaluation","Word Games","Rebus"],"title":"Non Verbis, Sed Rebus: Large Language Models are Weak Solvers of Italian Rebuses","type":"talk"},{"authors":["Gabriele Sarti"],"categories":["Natural Language Processing","Academic"],"content":"","date":1730800800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1730800800,"objectID":"f0849e164261e1b72edabb0d556576b6","permalink":"https://gsarti.com/talk/sapienza-data-science-phd-course-2024/","publishdate":"2024-02-26T00:00:00Z","relpermalink":"/talk/sapienza-data-science-phd-course-2024/","section":"talk","summary":"In this presentation, I will provide an overview of the interpretability research landscape and describe various promising methods for exploring and controlling the inner mechanisms of generative language models. I will focus specifically on post-hoc attribution technique and their usage to identify relevant input and model components, showcasing their usage with our Inseq open-source toolkit. A practical application of attribution techniques will be presented with the PECoRe data-driven framework for context usage attribution and its adaptation to produce internals-based citations for model answers in retrieval-augmented generation settings (MIRAGE).","tags":["Natural Language Processing","Interpretability","Sequence-to-sequence","Language Modeling","Feature Attribution"],"title":"Interpretability for Language Models: Current Trends and Applications","type":"talk"},{"authors":["Gabriele Sarti","Tommaso Caselli","Arianna Bisazza","Malvina Nissim"],"categories":["Natural Language Processing"],"content":"","date":1722553200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1722553200,"objectID":"443c96b88bcdd9e1c3b3624660e725e9","permalink":"https://gsarti.com/publication/verbalized-rebus/","publishdate":"2024-08-02T01:00:00+02:00","relpermalink":"/publication/verbalized-rebus/","section":"publication","summary":"We evaluate the rebus-solving capabilities of large language models on a new Italian dataset.","tags":["Natural Language Processing","HuggingFace","Italian","Large Language Models","Evaluation","Word Games","Rebus"],"title":"Non Verbis, Sed Rebus: Large Language Models are Weak Solvers of Italian Rebuses","type":"publication"},{"authors":["Gabriele Sarti"],"categories":["Natural Language Processing","Academic"],"content":"","date":1721116800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1721116800,"objectID":"4bd1d85ceb034ec2a69b811d43685993","permalink":"https://gsarti.com/talk/cis-lmu-inseq-pecore-2024/","publishdate":"2024-07-15T00:00:00Z","relpermalink":"/talk/cis-lmu-inseq-pecore-2024/","section":"talk","summary":"This presentation focuses on applying post-hoc interpretability techniques to analyze how language models (LMs) use input information throughout the generation process. We briefly introduce Inseq, our open-source toolkit designed to simplify advanced feature attribution analyses for LMs. Then, our Plausibility Evaluation of Context Reliance (PECoRe) interpretability framework is introduced to conduct data-driven analyses of context usage in LMs. In conclusion, we showcase how PECoRe can easily be adapted to retrieval-augmented generation (RAG) settings to produce internals-based citations for model answers. Our proposed Model Internals for RAG Explanations (MIRAGE) method achieves citation quality comparable to supervised answer validators with no additional training, producing citations that are faithful to actual context usage during generation.","tags":["Natural Language Processing","Interpretability","Sequence-to-sequence","Language Modeling","Feature Attribution","Retrieval-augmented Generation"],"title":"Interpreting Context Usage in Generative Language Models with Inseq, PECoRe and MIRAGE","type":"talk"},{"authors":["Daniel Scalena","Gabriele Sarti","Malvina Nissim"],"categories":["Natural Language Processing"],"content":"","date":1719352800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1719352800,"objectID":"c0ec02089212f5a1928996dcf920a294","permalink":"https://gsarti.com/publication/dynamic-activation-composition/","publishdate":"2024-06-26T00:00:00+02:00","relpermalink":"/publication/dynamic-activation-composition/","section":"publication","summary":"We propose Dynamic Activation Composition, an adaptive approach for multi-property activation steering of LLMs","tags":["Natural Language Processing","Deep Learning","Interpretability","Style Transfer","Multilingual","Prompting","Large Language Models"],"title":"Multi-property Steering of Large Language Models with Dynamic Activation Composition","type":"publication"},{"authors":["Jirui Qi*","Gabriele Sarti*","Raquel Fernández","Arianna Bisazza"],"categories":["Natural Language Processing"],"content":"","date":1718402400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1718402400,"objectID":"ba24af9603f80f4f264da63b24f636f9","permalink":"https://gsarti.com/publication/mirage/","publishdate":"2024-06-15T00:00:00+02:00","relpermalink":"/publication/mirage/","section":"publication","summary":"MIRAGE uses model internals for faithful answer attribution in retrieval-augmented generation applications.","tags":["Natural Language Processing","Deep Learning","Interpretability","Question Answering","Feature Attribution","Context Usage","Retrieval-Augmented Generation"],"title":"Model Internals-based Answer Attribution for Trustworthy Retrieval-Augmented Generation","type":"publication"},{"authors":["Gabriele Sarti"],"categories":["Natural Language Processing","Academic"],"content":"","date":1716192000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1716192000,"objectID":"e2a5407bd27ea1fc7c94bcc5f4ec06a2","permalink":"https://gsarti.com/talk/polito-inseq-pecore-2024/","publishdate":"2024-02-26T00:00:00Z","relpermalink":"/talk/polito-inseq-pecore-2024/","section":"talk","summary":"This talk discusses the challenges and opportunities in conducting interpretability analyses of generative language models. We begin by presenting Inseq, an open-source toolkit for advanced feature attribution analyses of language models. The usage of Inseq is illustrated through examples of state-of-the-art approaches contrastive attribution, input dependence and locating factual knowledge in intermediate model representations. Then, we introduce Plausibility Evaluation of Context Reliance (PECoRe), an end-to-end interpretability framework using model internals  to detect context-dependent spans in model generations and trace their prediction back to salient tokens in the available context. The usage of PECoRe is showcased on various generative tasks, including machine translation, story generation and retrieval-augmented question answering.","tags":["Natural Language Processing","Interpretability","Sequence-to-sequence","Language Modeling","Feature Attribution"],"title":"Interpreting Context Usage in Generative Language Models with Inseq and PECoRe","type":"talk"},{"authors":["Gabriele Sarti","Malvina Nissim"],"categories":["Natural Language Processing"],"content":"","date":1716159600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1716159600,"objectID":"2eaf51058a67b91ea3a1147275051ab7","permalink":"https://gsarti.com/publication/it5/","publishdate":"2024-05-20T01:00:00+02:00","relpermalink":"/publication/it5/","section":"publication","summary":"IT5s are the first encoder-decoder transformers pretrained on more than 40 billion Italian words.","tags":["Natural Language Processing","Pre-training","Italian","HuggingFace","Deep Learning","T5","Conditional Language Generation","Multilingual"],"title":"IT5: Text-to-text Pretraining for Italian Language Understanding and Generation","type":"publication"},{"authors":["Gabriele Sarti"],"categories":["Natural Language Processing","Academic"],"content":"","date":1715950800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1715950800,"objectID":"9176e0bac04b017e925991ea2140f6bd","permalink":"https://gsarti.com/talk/area-pecore-2024/","publishdate":"2022-05-17T00:00:00Z","relpermalink":"/talk/area-pecore-2024/","section":"talk","summary":"This talk presents the PECoRe framework for quantifying the plausibility of context reliance in neural machine translation. The framework is applied to a case study on the impact of context on the translation of gendered pronouns and other contextual phenomena in English-to-French translation. Finally, the online demo allowing users to try PECoRe with any generative language model is presented.","tags":["Natural Language Processing","Neural Machine Translation","Interpretability","Sequence-to-sequence"],"title":"Quantifying the Plausibility of Context Reliance in Neural Machine Translation","type":"talk"},{"authors":["Javier Ferrando","Gabriele Sarti","Arianna Bisazza","Marta Costa-jussà"],"categories":[],"content":"","date":1714514400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1714514400,"objectID":"1df6ac103ff228ddd22066dfe121db42","permalink":"https://gsarti.com/publication/transformer-lm-inner-workings/","publishdate":"2024-05-01T00:00:00+02:00","relpermalink":"/publication/transformer-lm-inner-workings/","section":"publication","summary":"This primer provides a concise technical introduction to the current techniques used to interpret the inner workings of Transformer-based language models, focusing on the generative decoder-only architecture.","tags":["Natural Language Processing","Deep Learning","Interpretability","Mechanistic Interpretability","Language Modeling","Transformers"],"title":"A Primer on the Inner Workings of Transformer-based Language Models","type":"publication"},{"authors":["Gabriele Sarti"],"categories":["Natural Language Processing","Academic"],"content":"","date":1714132800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1714132800,"objectID":"5e5f7521c30385e3efa4cf4d77250d45","permalink":"https://gsarti.com/talk/gronlp-rg-pecore/","publishdate":"2022-05-19T00:00:00Z","relpermalink":"/talk/gronlp-rg-pecore/","section":"talk","summary":"This talk presents the PECoRe framework for quantifying the plausibility of context reliance in neural machine translation. The framework is applied to a case study on the impact of context on the translation of gendered pronouns and other contextual phenomena in English-to-French translation. Finally, the online demo allowing users to try PECoRe with any generative language model is presented.","tags":["Natural Language Processing","Neural Machine Translation","Interpretability","Sequence-to-sequence"],"title":"Quantifying the Plausibility of Context Reliance in Neural Machine Translation","type":"talk"},{"authors":["Gabriele Sarti"],"categories":["Natural Language Processing","Academic"],"content":"","date":1709308800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1709308800,"objectID":"cfdf8f940a9360384d55e261581567f0","permalink":"https://gsarti.com/talk/sheffield-seminar-2024/","publishdate":"2024-02-26T00:00:00Z","relpermalink":"/talk/sheffield-seminar-2024/","section":"talk","summary":"This talk discusses the challenges of interpreting generative language models and presents Inseq, a toolkit for interpreting sequence generation models. The usage of Inseq is illustrated with examples introducing state-of-the-art approaches for interpreting language models such as contrastive attribution. Finally, the PECoRe framework is presented as a mean to evaluate the plausibility of context usage in language models.","tags":["Natural Language Processing","Interpretability","Sequence-to-sequence","Language Modeling","Feature Attribution"],"title":"Post-hoc Interpretability for Generative Language Models: Explaining Context Usage in Transformers","type":"talk"},{"authors":["Gabriele Sarti","Grzegorz Chrupała","Arianna Bisazza"],"categories":["Natural Language Processing","Industry-oriented"],"content":"","date":1698913800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1698913800,"objectID":"7a1654ca871a84e1742a8545760dc153","permalink":"https://gsarti.com/talk/indeep-masterclass-nov23/","publishdate":"2022-05-19T00:00:00Z","relpermalink":"/talk/indeep-masterclass-nov23/","section":"talk","summary":"In recent years, Transformer-based language models have achieved remarkable progress in most language generation and understanding tasks. However, the internal computations of these models are hardly interpretable due to their highly nonlinear structure, hindering their usage for mission-critical applications requiring trustworthiness and transparency guarantees. This presentation will introduce interpretability methods used for tracing the predictions of language models back to their inputs and discuss how these can be used to gain insights into model biases and behaviors. Several concrete examples of language model attributions will be presented throughout the presentation using the Inseq interpretability library.","tags":["Natural Language Processing","Interpretability","Sequence-to-sequence","Language Modeling","Feature Attribution"],"title":"Explaining Language Models with Inseq","type":"talk"},{"authors":["Gabriele Sarti"],"categories":["Natural Language Processing","Academic"],"content":"","date":1698319800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1698319800,"objectID":"7732d34e03caf633dad7e3b06b10844d","permalink":"https://gsarti.com/talk/escience-signlp-seminar-oct23/","publishdate":"2023-10-11T00:00:00+02:00","relpermalink":"/talk/escience-signlp-seminar-oct23/","section":"talk","summary":"This talk discusses the challenges of interpreting generative language models and presents Inseq, a toolkit for interpreting sequence generation models. The usage of Inseq is illustrated with examples introducing state-of-the-art approaches for interpreting language models such as contrastive attribution. Finally, the PECoRe framework is presented as a mean to evaluate the plausibility of context usage in language models.","tags":["Natural Language Processing","Interpretability","Sequence-to-sequence","Language Modeling","Feature Attribution"],"title":"Post-hoc Interpretability for Language Models","type":"talk"},{"authors":["Anna Langedijk","Hosein Mohebbi","Gabriele Sarti","Willem Zuidema","Jaap Jumelet"],"categories":["Natural Language Processing"],"content":"","date":1696456800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1696456800,"objectID":"e95b465a0d79d75292719202c6ce66d5","permalink":"https://gsarti.com/publication/decoderlens/","publishdate":"2023-10-05T00:00:00+02:00","relpermalink":"/publication/decoderlens/","section":"publication","summary":"We propose DecoderLens, a method to interpret the iterative refinement of representations in encoder-decoder Transformer models.","tags":["Natural Language Processing","Deep Learning","Interpretability","Machine Translation","Automatic Speech Recognition","Factual Knowledge","Logical Reasoning"],"title":"DecoderLens: Layerwise Interpretation of Encoder-Decoder Transformers","type":"publication"},{"authors":["Gabriele Sarti","Grzegorz Chrupała","Malvina Nissim","Arianna Bisazza"],"categories":["Natural Language Processing"],"content":"","date":1696197600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1696197600,"objectID":"38a8115956ac23410526fbdae1817cf6","permalink":"https://gsarti.com/publication/pecore/","publishdate":"2023-10-02T00:00:00+02:00","relpermalink":"/publication/pecore/","section":"publication","summary":"We introduce PECoRe, an interpretability framework for identifying context dependence in language model generations.","tags":["Natural Language Processing","Deep Learning","Interpretability","Machine Translation","Feature Attribution","Context Usage"],"title":"Quantifying the Plausibility of Context Reliance in Neural Machine Translation","type":"publication"},{"authors":["Gabriele Sarti"],"categories":["Natural Language Processing","Academic"],"content":"","date":1688306400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1688306400,"objectID":"e784075e5c7c8e2dafb733d37d32c58d","permalink":"https://gsarti.com/talk/restcl-2023/","publishdate":"2023-07-02T00:00:00Z","relpermalink":"/talk/restcl-2023/","section":"talk","summary":"In recent years, Transformer-based language models have achieved remarkable progress in most language generation and understanding tasks. However, the internal computations of these models are hardly interpretable due to their highly nonlinear structure, hindering their usage for mission-critical applications requiring trustworthiness and transparency guarantees. This presentation will introduce interpretability methods used for tracing the predictions of language models back to their inputs and discuss how these can be used to gain insights into model biases and behaviors. Several concrete examples of language model attributions will be presented throughout the presentation using the Inseq interpretability library.","tags":["Natural Language Processing","Interpretability","Sequence-to-sequence","Language Modeling","Feature Attribution"],"title":"Post-hoc Interpretability for NLG \u0026 Inseq: an Interpretability Toolkit for Sequence Generation Models","type":"talk"},{"authors":["Gabriele Sarti"],"categories":["Natural Language Processing","Academic"],"content":"","date":1685626200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1685626200,"objectID":"40f924278ce1527a2c3ff77015dcad9b","permalink":"https://gsarti.com/talk/cosmo-units-2023/","publishdate":"2023-06-01T00:00:00Z","relpermalink":"/talk/cosmo-units-2023/","section":"talk","summary":"In recent years, Transformer-based language models have achieved remarkable progress in most language generation and understanding tasks. However, the internal computations of these models are hardly interpretable due to their highly nonlinear structure, hindering their usage for mission-critical applications requiring trustworthiness and transparency guarantees. This presentation will introduce interpretability methods used for tracing the predictions of language models back to their inputs and discuss how these can be used to gain insights into model biases and behaviors. Several concrete examples of language model attributions will be presented throughout the presentation using the Inseq interpretability library.","tags":["Natural Language Processing","Interpretability","Sequence-to-sequence","Language Modeling","Feature Attribution"],"title":"Post-hoc Interpretability for Neural Language Models","type":"talk"},{"authors":["Gabriele Sarti","Alessio Miaschi"],"categories":["Natural Language Processing","Academic"],"content":"","date":1685538000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1685538000,"objectID":"9827ebe3cb95764f688a8a0d3c27b6b9","permalink":"https://gsarti.com/talk/ailc-lcl-2023/","publishdate":"2023-06-01T00:00:00Z","relpermalink":"/talk/ailc-lcl-2023/","section":"talk","summary":"As language models become increasingly complex and sophisticated, the processes leading to their predictions are growing increasingly difficult to understand. Research in NLP interpretability focuses on explaining the rationales driving model predictions and is crucial for building trust and transparency in the usage of these systems in real-world scenarios. In this laboratory, we will explore various techniques for analyzing Neural Language Models, such as feature attribution methods and diagnostic classifiers. Besides common approaches to inspect models’ internal representations, we will also introduce prompting techniques to elicit model responses and motivate their usage as alternative methods for the behavioral study of model generations.","tags":["Natural Language Processing","Interpretability","Sequence-to-sequence","Language Modeling","Feature Attribution","Probing Classifiers"],"title":"Explaining Neural Language Models from Internal Representations to Model Predictions","type":"talk"},{"authors":["Gabriele Sarti","Phu Mon Htut","Xing Niu","Benjamin Hsu","Anna Currey","Georgiana Dinu","Maria Nadejde"],"categories":["Natural Language Processing"],"content":"","date":1685346458,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1685346458,"objectID":"f764626e4a7e1b3763e570ec10931a00","permalink":"https://gsarti.com/publication/ramp/","publishdate":"2023-05-29T09:47:38+02:00","relpermalink":"/publication/ramp/","section":"publication","summary":"We introduce Retrieval and Attribute-Marking enhanced Prompting (RAMP) to perform attribute-controlled MT with multilingual LLMs.","tags":["Natural Language Processing","Deep Learning","Machine Translation","Style Transfer","Multilingual","Prompting","Large Language Models"],"title":"RAMP: Retrieval and Attribute-Marking Enhanced Prompting for Attribute-Controlled Translation","type":"publication"},{"authors":["Gabriele Sarti"],"categories":["Natural Language Processing","Academic"],"content":"","date":1684854000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1684854000,"objectID":"d1304ee92794525e9505ee9e49c12104","permalink":"https://gsarti.com/talk/ailo-xai-2023/","publishdate":"2022-05-23T00:00:00Z","relpermalink":"/talk/ailo-xai-2023/","section":"talk","summary":"In recent years, Transformer-based language models have achieved remarkable progress in most language generation and understanding tasks. However, the internal computations of these models are hardly interpretable due to their highly nonlinear structure, hindering their usage for mission-critical applications requiring trustworthiness and transparency guarantees. This presentation will introduce interpretability methods used for tracing the predictions of language models back to their inputs and discuss how these can be used to gain insights into model biases and behaviors. Throughout the presentation, several concrete examples of language model attributions will be presented using the Inseq interpretability library.","tags":["Natural Language Processing","Interpretability","Sequence-to-sequence","Language Modeling","Feature Attribution"],"title":"Post-hoc Interpretability for Neural Language Models","type":"talk"},{"authors":["Gabriele Sarti"],"categories":["Natural Language Processing","Academic"],"content":"","date":1680789600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1680789600,"objectID":"90ecc524b920b4ff4bfed8380d9f3a4b","permalink":"https://gsarti.com/talk/sapienzanlp-seminar-2023/","publishdate":"2022-05-19T00:00:00Z","relpermalink":"/talk/sapienzanlp-seminar-2023/","section":"talk","summary":"This talk introduces the Inseq toolkit for interpreting sequence generation models. The usage of Inseq is illustrated with examples introducing state-of-the-art approaches for interpreting language models such as contrastive attribution, tuned lenses and causal mediation analysis.","tags":["Natural Language Processing","Interpretability","Sequence-to-sequence","Language Modeling","Feature Attribution"],"title":"Inseq: An Interpretability Toolkit for Sequence Generation Models","type":"talk"},{"authors":["Gabriele Sarti"],"categories":["Natural Language Processing","Academic"],"content":"","date":1679576400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1679576400,"objectID":"9cc42f7e04713e71674a03a25da1c5e3","permalink":"https://gsarti.com/talk/indeep-meeting-mar23/","publishdate":"2022-05-19T00:00:00Z","relpermalink":"/talk/indeep-meeting-mar23/","section":"talk","summary":"This talk introduces the Inseq toolkit for interpreting sequence generation models. The usage of Inseq is illustrated with examples introducing state-of-the-art approaches for interpreting language models such as contrastive attribution, tuned lenses and causal mediation analysis.","tags":["Natural Language Processing","Interpretability","Sequence-to-sequence","Language Modeling","Feature Attribution"],"title":"Advanced XAI Techniques and Inseq: An Interpretability Toolkit for Sequence Generation Models","type":"talk"},{"authors":["Gabriele Sarti"],"categories":["Natural Language Processing","Academic"],"content":"","date":1678449600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1678449600,"objectID":"68ef1717380d867e8f1e067f51a3cf9e","permalink":"https://gsarti.com/talk/gronlp-rg-inseq/","publishdate":"2022-05-19T00:00:00Z","relpermalink":"/talk/gronlp-rg-inseq/","section":"talk","summary":"After motivating the usage of interpretability methods in NLP, this talk introduces the Inseq toolkit for interpreting sequence generation models. The usage of Inseq is illustrated on two case studies related to gender bias in machine translation and locating factual knowledge withing GPT-2 representations.","tags":["Natural Language Processing","Neural Machine Translation","Interpretability","Sequence-to-sequence"],"title":"Introducing Inseq: An Interpretability Toolkit for Sequence Generation Models","type":"talk"},{"authors":["Lukas Edman","Gabriele Sarti","Antonio Toral","Gertjan van Noord","Arianna Bisazza"],"categories":["Natural Language Processing"],"content":"","date":1677570458,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1677570458,"objectID":"f98c2caee75dcef07cdb5a3b43bb224d","permalink":"https://gsarti.com/publication/char-mt-analysis/","publishdate":"2023-02-28T09:47:38+02:00","relpermalink":"/publication/char-mt-analysis/","section":"publication","summary":"We analyze input contributions of char-level MT models and show how they modulate word and character-level information.","tags":["Natural Language Processing","Deep Learning","Interpretability","Machine Translation","Feature Attribution","Character-level"],"title":"Are Character-level Translations Worth the Wait? Comparing ByT5 and mT5 for Machine Translation","type":"publication"},{"authors":["Gabriele Sarti","Nils Feldhus","Ludwig Sickert","Oskar van der Wal","Malvina Nissim","Arianna Bisazza"],"categories":["Natural Language Processing"],"content":"","date":1677484058,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1677484058,"objectID":"f849025865e410b201efae161a65ba5b","permalink":"https://gsarti.com/publication/inseq/","publishdate":"2023-02-27T09:47:38+02:00","relpermalink":"/publication/inseq/","section":"publication","summary":"We present Inseq, a Python library to democratize access to interpretability analyses of sequence generation models.","tags":["Natural Language Processing","HuggingFace","Deep Learning","Interpretability","Machine Translation","Transformers","Feature Attribution","Natural Language Generation","Library"],"title":"Inseq: An Interpretability Toolkit for Sequence Generation Models","type":"publication"},{"authors":["Gabriele Sarti","Jirui Qi","Grzegorz Chrupała","Malvina Nissim","Raquel Fernández","Arianna Bisazza"],"categories":["Interpretability"],"content":"PECoRe is a framework using the internal properties of generative language models to identify and attribute context usage in their generations. In particular, the framework is composed by two steps: Context-sensitive Token Identification (CTI), where generated tokens are classified as context-sensitive by contrastively comparing their probabilities with and without context, and Contextual Cues Imputation (CCI), where the dependence of token selected in the CTI step is highlighted by using contrastive attribution. The framework is integrated in the Inseq interpretability library and can be easily used thanks to the inseq attribute-context command. The framework is described in detail in the paper Quantifying the Plausibility of Context Reliance in Neural Machine Translation, published at ICLR 2024, and its extension MIRAGE was created to support answer attribution in RAG applications Model Internals-based Answer Attribution for Trustworthy Retrieval-Augmented Generation.\n","date":1670889600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1670889600,"objectID":"58a2800d81b3159cdf8677d4d9fdf5ad","permalink":"https://gsarti.com/project/pecore/","publishdate":"2022-12-13T00:00:00Z","relpermalink":"/project/pecore/","section":"project","summary":"An interpretability framework to detect and attribute context usage in language models' generations","tags":["Natural Language Processing","Interpretability","Deep Learning","Natural Language Generation"],"title":"Attributing Context Usage in Language Models","type":"project"},{"authors":["Gabriele Sarti","Nils Feldhus","Oskar van der Waals","Ludwig Sickert"],"categories":["Interpretability"],"content":"Inseq is a Pytorch-based hackable toolkit to democratize the study of interpretability for sequence generation models. Inseq supports a wide set of models from the 🤗 Transformers library and an ever-growing set of feature attribution methods, leveraging in part the widely-used Captum library. For a quick introduction to common use cases, see the Getting started with Inseq page.\nUsing Inseq, feature attribution maps that can be saved, reloaded, aggregated and visualized either as HTMLs (with Jupyter notebook support) or directly in the console using rich. Besides simple attribution, Inseq also supports features like step score extraction, attribution aggregation and attributed functions customization for more advanced use cases.\n","date":1670889600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1670889600,"objectID":"cd10ca553e806492c56344928f6a2ff5","permalink":"https://gsarti.com/project/inseq/","publishdate":"2022-12-13T00:00:00Z","relpermalink":"/project/inseq/","section":"project","summary":"An open-source library to democratize access to model interpretability for sequence generation models","tags":["Natural Language Processing","Interpretability","HuggingFace","Deep Learning","Natural Language Generation"],"title":"Inseq: An Interpretability Toolkit for Sequence Generation Models","type":"project"},{"authors":["Gabriele Sarti"],"categories":["Natural Language Processing","Academic"],"content":"","date":1666224000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1666224000,"objectID":"f4cd8c6b7944c34ec70686b56b431fd5","permalink":"https://gsarti.com/talk/linguistics-lunch/","publishdate":"2022-05-19T00:00:00Z","relpermalink":"/talk/linguistics-lunch/","section":"talk","summary":"With the astounding advances of artificial intelligence in recent years, interpretability research has emerged as a fundamental effort to ensure the development of robust and transparent AI systems aligned with human needs. This talk will focus on user-centric interpretability applications aimed at improving our understanding of machine translation systems, with the ultimate goal of improving post-editing productivity and enjoyability.","tags":["Natural Language Processing","Neural Machine Translation","Interpretability","Sequence-to-sequence","Behavioral Data","Linguistic Complexity"],"title":"Towards User-centric Interpretability of Machine Translation Models","type":"talk"},{"authors":["Alessio Miaschi","Gabriele Sarti","Dominique Brunato","Felice Dell’Orletta","Giulia Venturi"],"categories":[],"content":"","date":1656630000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1656630000,"objectID":"a1f30cd72145308e12876ba2e93c61b3","permalink":"https://gsarti.com/publication/italian-transformers/","publishdate":"2022-07-01T01:00:00+02:00","relpermalink":"/publication/italian-transformers/","section":"publication","summary":"We investigate whether and how using different architectures of probing models affects the performance of Italian transformers in encoding a wide spectrum of linguistic features.","tags":["Natural Language Processing","Deep Learning","Interpretability","Italian Language","Transformers","Neural Language Models","Probing Task"],"title":"Probing Linguistic Knowledge in Italian Neural Language Models across Language Varieties","type":"publication"},{"authors":["Gabriele Sarti","Arianna Bisazza","Ana Guerberof Arenas","Antonio Toral"],"categories":["Natural Language Processing"],"content":"","date":1653346800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1653346800,"objectID":"e5094ae8a5a8e1cbd4dcf58f57f16540","permalink":"https://gsarti.com/publication/divemt/","publishdate":"2022-05-24T01:00:00+02:00","relpermalink":"/publication/divemt/","section":"publication","summary":"DivEMT is a publicly available post-editing study of Neural Machine Translation over a typologically diverse set of target languages.","tags":["Natural Language Processing","Post-editing","Machine Translation","Multilingual","Dataset","Behavioral Data"],"title":"DivEMT: Neural Machine Translation Post-Editing Effort Across Typologically Diverse Languages","type":"publication"},{"authors":["Gabriele Sarti"],"categories":["Natural Language Processing","Industry-oriented"],"content":"","date":1652832000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1652832000,"objectID":"198c29ce48c766630f2d098b0fabba46","permalink":"https://gsarti.com/talk/tech-talk-translated-2022/","publishdate":"2022-05-18T00:00:00Z","relpermalink":"/talk/tech-talk-translated-2022/","section":"talk","summary":"With the astounding advances of artificial intelligence in recent years, the field of interpretability research has emerged as a fundamental effort to ensure the development of robust AI systems aligned with human values. In this talk, two perspectives on AI interpretability will be presented alongside two case studies in natural language processing. The first study leverages behavioral data and probing tasks to study the perception and encoding of linguistic complexity in humans and language models. The second introduces a user-centric interpretability perspective for neural machine translation to improve post-editing productivity and enjoyability. The need for such application-driven approaches will be emphasized in light of current challenges in faithfully evaluating advances in this field of study.","tags":["Natural Language Processing","Neural Machine Translation","Interpretability","Sequence-to-sequence","Behavioral Data","Linguistic Complexity"],"title":"Towards User-centric Interpretability of NLP Models","type":"talk"},{"authors":["Gabriele Sarti"],"categories":["Natural Language Processing","Academic"],"content":"","date":1639440000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1639440000,"objectID":"41b3e023570d136e585b718b2a39a360","permalink":"https://gsarti.com/talk/xai4debugging21/","publishdate":"2021-11-09T10:30:41+01:00","relpermalink":"/talk/xai4debugging21/","section":"talk","summary":"Discussing the potential applications of interpretability research to the field of neural machine translation.","tags":["Natural Language Processing","Neural Machine Translation","Interpretability","Sequence-to-sequence"],"title":"Empowering Human Translators via Interpretable Interactive Neural Machine Translation","type":"talk"},{"authors":["Gabriele Sarti"],"categories":["Natural Language Processing","Academic"],"content":"","date":1636070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636070400,"objectID":"bbd061d18d46550ef3bdae8746637187","permalink":"https://gsarti.com/talk/aperitivo-bocconi/","publishdate":"2021-11-09T10:30:41+01:00","relpermalink":"/talk/aperitivo-bocconi/","section":"talk","summary":"Presenting my work on studying different metrics of linguistic complexity and how they correlate with linguistic phenomena and learned representations in neural language models","tags":["Natural Language Processing","Eye-tracking","Deep Learning","Interpretability","Probing Tasks"],"title":"Characterizing Linguistic Complexity in Humans and Language Models","type":"talk"},{"authors":["Federico Bianchi","Giuseppe Attanasio","Raphael Pisoni","Silvia Terragni","Gabriele Sarti"],"categories":["Multimodality"],"content":"","date":1629359258,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1629359258,"objectID":"e222ff2d16d7fe8780871ce179aab3b6","permalink":"https://gsarti.com/publication/clip-italian/","publishdate":"2021-08-19T09:47:38+02:00","relpermalink":"/publication/clip-italian/","section":"publication","summary":"We present the first CLIP model for the Italian Language (CLIP-Italian), trained on more than 1.4 million image-text pairs.","tags":["Computer Vision","Natural Language Processing","Multimodality","Italian","HuggingFace","Deep Learning","Contrastive Learning","CLIP"],"title":"Contrastive Language-Image Pre-training for the Italian Language","type":"publication"},{"authors":["Federico Bianchi","Raphael Pisoni","Giuseppe Attanasio","Silvia Terragni","Dario Balestri","Gabriele Sarti","Sri Lakshmi"],"categories":["Multimodality"],"content":"CLIP is a multimodel model that can learn to represent images and text jointly in the same space. In this project, we aim to propose the first CLIP model trained on Italian data, that in this context can be considered a low resource language. Using a few techniques, we have been able to fine-tune a SOTA Italian CLIP model with only 1.4 million training samples.\nFor more information, refer to our demo.\n","date":1626998400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1626998400,"objectID":"ec6905b548c40d40acfcf6c214b3821a","permalink":"https://gsarti.com/project/clip-italian/","publishdate":"2021-07-23T00:00:00Z","relpermalink":"/project/clip-italian/","section":"project","summary":"The first CLIP model pretrained on the Italian language.","tags":["Computer Vision","Natural Language Processing","Multimodality","Italian","HuggingFace","Deep Learning","Contrastive Learning","CLIP"],"title":"Contrastive Image-Text Pretraining for Italian","type":"project"},{"authors":["Ludovica Pannitto","Lucia Busso","Claudia Roberta Combei","Lucio Messina","Alessio Miaschi","Gabriele Sarti","Malvina Nissim"],"categories":[],"content":"","date":1622965658,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1622965658,"objectID":"36e466633662949775e675e2ab632ed0","permalink":"https://gsarti.com/publication/teaching-nlp-bracelets-menus/","publishdate":"2021-06-06T09:47:38+02:00","relpermalink":"/publication/teaching-nlp-bracelets-menus/","section":"publication","summary":"We developed an interactive workshop designed to illustrate the NLP and computational linguistics to Italian high schoolers.","tags":["Natural Language Processing","Teaching NLP","Computational Linguistics","Italian"],"title":"Teaching NLP with Bracelets and Restaurant Menus: An Interactive Workshop for Italian Students","type":"publication"},{"authors":["Gabriele Sarti","Dominique Brunato","Felice Dell'Orletta"],"categories":[],"content":"","date":1622965658,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1622965658,"objectID":"b504c42ac08dbbf6345d0363e2895c7d","permalink":"https://gsarti.com/publication/that-looks-hard/","publishdate":"2021-06-06T09:47:38+02:00","relpermalink":"/publication/that-looks-hard/","section":"publication","summary":"This paper investigates the relationship between two complementary perspectives in the human assessment of sentence complexity and how they are modeled in a neural language model (NLM), highlighting how linguistic information encoded in representations changes when the model learns to predict complexity.","tags":["Natural Language Processing","Eye-tracking","Deep Learning","Interpretability","Probing Tasks","Behavioral Data"],"title":"That Looks Hard: Characterizing Linguistic Complexity in Humans and Language Models","type":"publication"},{"authors":null,"categories":null,"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"a76f61f3cbd395de8d63d4d95dd62b47","permalink":"https://gsarti.com/activities/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/activities/","section":"","summary":"A summary of my professional activities","tags":null,"title":"Activities","type":"widget_page"},{"authors":["Gabriele Sarti"],"categories":[],"content":"","date":1608364058,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608364058,"objectID":"818dc3dd24b63aad9f4d61d2ae11ddd9","permalink":"https://gsarti.com/publication/interpreting-nlms-for-lca/","publishdate":"2020-08-26T09:47:38+02:00","relpermalink":"/publication/interpreting-nlms-for-lca/","section":"publication","summary":"This thesis presents a model-driven study of multiple phenomena associated with linguistic complexity, and how those get encoded by neural language models' learned representations.","tags":["Natural Language Processing","Deep Learning","Interpretability","Language Modeling","Transformers","Canonical Correlation Analysis","Garden-path Sentences","Probing Tasks","Representational Similarity Analysis","SyntaxGym","Surprisal","Eye-tracking"],"title":"Interpreting Neural Language Models for Linguistic Complexity Assessment","type":"publication"},{"authors":["Gabriele Sarti"],"categories":[],"content":"","date":1608364058,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608364058,"objectID":"39f32791b458e436d803a421acc3279f","permalink":"https://gsarti.com/publication/umberto-mtsa/","publishdate":"2020-12-19T09:47:38+02:00","relpermalink":"/publication/umberto-mtsa/","section":"publication","summary":"This work describes a self-supervised data augmentation approach used to improve learning models' performances when only a moderate amount of labeled data is available.","tags":["Natural Language Processing","Deep Learning","Multi-task Learning","Transformers","Neural Language Models","Self-training"],"title":"UmBERTo-MTSA@ AcCompl-It: Improving Complexity and Acceptability Prediction with Multi-task Learning on Self-Supervised Annotations","type":"publication"},{"authors":["Ginevra Carbone","Gabriele Sarti"],"categories":[],"content":"","date":1606808858,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606808858,"objectID":"0659fcb83a8eb334bdd07368b02cfb99","permalink":"https://gsarti.com/publication/etc-nlg/","publishdate":"2020-08-26T09:47:38+02:00","relpermalink":"/publication/etc-nlg/","section":"publication","summary":"We present ETC-NLG, an approach leveraging topic modeling annotations to enable fully-unsupervised End-to-end Topic-Conditioned Natural Language Generation over emergent topics in unlabeled document collections.","tags":["Natural Language Processing","Deep Learning","Natural Language Generation","Topic Modeling","Transformers"],"title":"ETC-NLG: End-to-end Topic-Conditioned Natural Language Generation","type":"publication"},{"authors":["Gabriele Sarti"],"categories":[],"content":"(For an overview of the Transformer, see The Illustrated Transformer by Jay Alammar )\nThe Transformer architecture was first proposed in Attention is All you Need as a valid alternative to sequential language modeling approaches like LSTMs and has since then become ubiquitous in the field of Natural Language Processing, pushing the state-of-the-art in most downstream language-related tasks.\nThis year\u0026rsquo;s edition of the International Conference on Learning Representation (ICLR) brought a lot of promising revisions to the original Transformer and its more recent variants BERT and Transformer-XL. Proposed improvements address the well-known weaknesses of Transformers, namely:\nOptimizing the self-attention computation. Injecting linguistically-motivated inductive biases in the model architecture. Making the model more parameter and data-efficient. This post wants to summarize and provide a high-level overview of those contributions, highlighting current trends in the development of better and faster models for Natural Language Processing. All image credits go to their respective paper authors.\nIndex Self-Attention Variants Long-Short Range Attention Tree-Structured Attention with Subtree Masking Hashed Attention eXtra Hop Attention Training Objectives Discriminative Replacement Task Word and Sentence Structural Tasks Type-Constrained Entity Replacement Embeddings Position-Aware Complex Word Embeddings Hierarchical Embeddings Factorized Embedding Parametrization Model Architecture Compressive Memory Reversible Layers Cross-Layer Parameter Sharing Adaptive Depth Estimation Conclusion Self-Attention Variants Scaled dot-product self-attention is one of the main components in the standard Transformer layer, enabling the modelling of dependencies regardless of their distance in the input. The self-attention operation projects an input activation tensor $\\bf A$ to queries $Q$ of dimension $d_k$, keys $K$ of dimension $d_k$ and values $V$ of dimension $d_v$, returning a weighted version of $V$:\n$$\\tag{1} \\text{Attention}(Q,K,V) = \\text{softmax}\\Big(\\frac{QK^T}{\\sqrt d_k}\\Big)V$$\nIn the multi-head self-attention variant, the attention function is applied in parallel to $h$ version of queries, keys and values projected with learned projections $W$, and outputs are finally concatenated and projected again to obtain final values:\n$$\\text{MultiHead}(Q,K,V) = \\text{Concat}(\\text{head}_1,\\dots, \\text{head}_h)W^O$$\n$$\\tag{2} \\text{where } \\text{head}_i = \\text{Attention}(QW_i^Q,KW_i^K,VW_i^V)$$\nThis section presents some variants of the self-attention component that make it more efficient and effective in the context of language applications.\nLong-Short Range Attention Introduced in: Lite Transformer with Long-Short Range Attention by Wu, Liu et al.\nConventional self-attention is deemed as redundant since it was empirically shown to put excessive emphasis on local relations inside a sentence, which can be modeled more efficiently by a standard convolution, as shown also in On the Relationship between Self-Attention and Convolutional Layers. While the redundancy may help model performances in some cases, it is not suitable for lighter applications.\nLong-Short Range Attention (LSRA) makes the computation more efficient by splitting the input into two parts along channel dimensions and feeding each to two modules: a global extractor using standard self-attention and a local extractor using light depth-wise convolutions. Authors report a $2\\times$ reduced overall computation for the model, making it suitable for mobile settings.\nTree-Structured Attention with Subtree Masking Introduced in: Tree-Structured Attention with Hierarchical Accumulation by Nguyen et al.\nA weakness of the standard Transformer is the absence of inductive biases to account for the hierarchical structure of language. This is due in part to the difficulty in operating with tree-like structures that are usually modeled by recurrent or recursive mechanisms while maintaining the constant parallel time complexity of self-attention.\nThe proposed solution leverages constituency parses of input text to build a tree of hidden states, using hierarchical accumulation to build the value of non-terminals as the aggregation of lower representations in the tree. The final output representation is built by performing a weighted aggregation of branch-level representations.\nAn interesting addition is the use of subtree masking to filter out superfluous noise by constraining the attention of each node query only on its subtree descendants. The cost for this inductive bias is an increased computational and memory cost, which is then mitigated using parameter sharing\nHashed Attention Introduced in: Reformer: The Efficient Transformer by Kitaev et al.\nIn the self-attention equation the factor $QK^T$ represents a bottleneck, taking $\\mathcal{O}(L^2)$ for input sequences of length $L$ both in computational and memory complexity. This effectively hinders the possibility of modeling long sequences.\nReformer proposes to restrict the pool of candidates attended by each query to a small set of neighbors found through locally-sensitive hashing. Since LSH bucketing employs random projections, similar vectors may sometimes fall in different neighborhoods; an approach using multiple parallel rounds of hashing is suggested to mitigate this issue. Using LSH attention reduces the computational cost of the self-attention operation to $\\mathcal{O}(L \\log L)$, allowing the model to operate on longer sequences.\neXtra Hop Attention Introduced in: Transformer-XH: Multi-Evidence Reasoning with eXtra Hop Attention by Zhao et al.\nWhile Transformers were optimized to operate on single sequences or pairs of sequences, they can hardly generalize to settings where evidence is scattered in multiple pieces of text, as in the challenging task of multi-hop question answering.\nTransformer-XH introduces a new variant of attention, eXtra Hop Attention, that can be applied to a graph of text sequences connected by edges (e.g. hyperlinks). This new attention mechanism uses the special [CLS] token at the beginning of each sequence as an attention hub that attends to all other connected sequences in the graph. The resulting representation is then combined to the one obtained by standard self-attention through a linear projection. The resulting model shows significant improvements for tasks requiring reasoning over graphs, at the cost of the extra computations introduced by the new attention mechanism.\nTraining Objectives The pre-training of Transformer models is usually achieved by the mean of multiple unsupervised objectives, leverage huge quantities of non-annotated texts. The most common tasks used for this purpose are autoregressive language modeling, also known as standard language modeling (LM), and autoencoding of masked input, often referred to as masked language modeling (MLM).\nThe standard Transformer implementation and its GPT variants adopt the autoregressive approach, leveraging a unidirectional context (forward or backward) inside a sequence $\\textbf{x} = (x_1, \\dots, x_L)$ to estimate next token probability:\n$$p(\\textbf{x}) = \\prod_{l=1}^L p(x_l|\\textbf{x}_{\u0026lt; or \u0026gt;l})$$\nInstead, BERT-like approaches use a bidirectional context to recover a small fraction of the input that was artificially replaced by special [MASK] tokens. This variant was shown to be especially effective for downstream natural language understanding tasks.\nBesides word-level modeling, a sentence-level classification task like next sentence prediction (NSP) is usually added to the training procedure since many important language applications require an understanding of the relationship between two sequences.\nWhile those tasks seem to induce meaningful token and sentence-level representation, many of the approaches covered in this section suggest better alternatives that make learning more efficient and grounded in the structure and the content of the input.\nDiscriminative Replacement Task Introduced in: ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators by Clark et al.\nThe masking strategy used in BERT-like models is quite data inefficient, using only ~15% of the input text to complete the MLM task. However, the percentage of masked data can hardly be increased since having too many masked tokens may degrade the overall context information.\nELECTRA proposes a simple yet effective approach to cope with this inefficiency. A small masked language model is trained and then used as a generator to fill the masked tokens in the input with its predictions, as in normal MLM. However, the new task for the main model will be a discriminative one: instead of predicting masked tokens, the model has to detect which tokens have been replaced by the generator. This allows leveraging the entire input sequence for training. As mentioned by the authors, this approach consistently outperforms MLM pre-training given the same compute budget.\nWord and Sentence Structural Tasks Introduced in: StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding by Wang et al.\nAs seen previously, Transformers do not explicitly account for structures present in the input. While tree-structured attention injects a heavy hierarchical bias in the model architecture, StructBERT adopts two lighter but effective approaches to make the resulting representations more aware of the underlying sequentiality of language.\nThe first is a word structural objective where trigrams inside the inputs are randomly shuffled, and their original position must be reconstructed. This is done in parallel with normal MLM. The sentence structural objective is a lighter variant of the sentence reordering introduced in ERNIE 2.0 and equal to the sentence ordering prediction introduced in ALBERT: given a pair of sentences $(S_1, S_2)$ as input, we ask the model to discriminate whether $S_2$ precedes, follows or is unrelated to $S_1$. This new task extends the standard NSP, which was deemed as too easy for learning meaningful sentence relations. These additions result in significant improvements over standard benchmarks for natural language understanding.\nType-Constrained Entity Replacement Introduced in: Pretrained Encyclopedia: Weakly Supervised Knowledge-Pretrained Language Model by Xiong et al.\nWhile it was shown that pre-trained Transformer models implicitly capture real-world knowledge, their standard training objectives do not explicitly take into account the entity-centric information needed for robust reasoning over real-world settings.\nType-constrained entity replacement is a weakly supervised approach where random entities in the text are replaced with other entities taken from Wikidata that have the same entity type. The model then uses a discriminative objective similar to the one of ELECTRA to determine which entities were replaced. This is done along with MLM in a multi-task setup, and authors report significant improvements in settings requiring a deeper entity understanding, such as open-domain QA and entity typing.\nEmbeddings The original Transformer relies on two sets of embeddings to represent the input sequence:\nLearned word embeddings for each token present in the vocabulary, used as token vector representations for the model.\nFixed positional embeddings (PE), used to inject information about the position of tokens in the sequence. For position $\\text{pos}$ and dimension $i$, those correspond to sinusoidal periodic functions that were empirically shown to perform on par with learned embeddings, and were chosen to enable extrapolation for longer sequences:\n$$PE_{pos, 2i} = \\sin(\\text{pos}/10000^{2i/d_{model}})$$\n$$PE_{pos, 2i + 1} = \\cos(\\text{pos}/10000^{2i/d_{model}})$$\nFor BERT-like models able to operate on multiple input segments, a third set of learned segment embeddings is used to differentiate tokens belonging to different sentences.\nAll those embeddings have the same dimensions and get summed together to obtain an input representation. Approaches introduced in this section aim to inject more structure in the embeddings, or to optimize their dimension for better efficiency.\nPosition-Aware Complex Word Embeddings Introduced in: Encoding word order in complex embeddings by Wang et al.\nWhile PE capture different positions in the input, they do not explicitly take into account the relation between those positions, i.e. ordered relationships such as adjacency or precedence. This problem was already addressed in Transformer-XL by leveraging relative distances between words instead of raw position indices.\nA proposed improvement is to generalize word embeddings to continuous functions defined over positions, extending the solutions to the complex-valued domain to benefit from richer representations. The resulting complex-valued embeddings introduce new parameters for amplitudes, frequencies and initial phases that determine various properties of the embedding such as position sensitivity. Empirical results show that the complex embeddings with parameter-sharing schemas outperform previous embedding approaches without a significant increase in the number of trainable parameters.\nHierarchical Embeddings Introduced in: Tree-Structured Attention with Hierarchical Accumulation by Nguyen et al.\nIn the overview of tree-structured attention, we saw how hierarchical accumulation is used to form a representation based on descendants for nonterminal nodes. This procedure, however, has the disadvantage of not taking into account the hierarchical structure of descendants.\nHierarchical embeddings are used to inject this structural bias by concatenating vertical and horizontal embeddings matrices representing respectively hierarchical ordering inside branches and relationships between siblings nodes in a subtree. Those embeddings are shared across attention heads, thus accounting only for 0.1% of the total parameters.\nFactorized Embedding Parametrization Introduced in: ALBERT: A Lite BERT for Self-supervised Learning of Language Representations by Lan et al.\nIn recent models based on BERT and Transformer-XL the input embeddings size $E$ is tied with the hidden layer size $H$, i.e. $E \\equiv H$. This is very impractical since, to augment the expressivity of hidden representations used to learn context-dependent representation, one should also increase the size of the embedding matrix $\\textbf{M} = V \\times E$, where $V$ is the vocabulary size. Even for relatively small hidden layer dimensions, this results in billions of parameters that are rarely updated during training.\nALBERT authors propose to insert a projection between $E$ and $V$ to make both dimensions independent, an approach that is especially efficient to reduce the parameter count when $H \\gg E$. As a result, an ALBERT base with $E = 128$ and $H = 768$ obtains performances comparable with a BERT base with the same configuration on many downstream tasks, using 21M fewer parameters (89M in Table 3 vs 110M for BERT).\nModel Architecture The original Transformer architecture is composed of an encoder and a decoder, each composed by a stacked sequence of identical layers that transform input embeddings in outputs having the same dimension (hence the name Transformer).\nEach layer of the Transformer encoder is composed of two sublayers, a multi-head self-attention mechanism and a feed-forward network, surrounded by residual connections and followed by layer normalization. The decoder includes a third layer that performs multi-head self-attention over the encoder output and modifies the original self-attention sublayer to prevent attending to future context, as required by the autoregressive language modeling objective presented above.\nBidirectional variants of the Transformer drop the decoder structure and focus solely on the encoder to generate the contextual embeddings needed for various tasks, including MLM.\nTransformer-XL notably introduces a notion of memory for Transformer networks, where hidden states obtained in previous segments are weighted with attention and reused to better model long-term dependencies, preventing context fragmentation.\nThe following approaches try to build on top of current structures to improve long-range modeling, reduce the parameter count, or optimize the computation performed by the model.\nCompressive Memory Introduced in: Compressive Transformers for Long-Range Sequence Modelling by Rae et al.\nIn Transformer-XL\u0026rsquo;s recurrent memory approach, old memories are discarded to enable the storing of new ones in a first-in-first-out fashion. This method accounts only for recency, not taking into account the relevance of information that might get discarded.\nCompressive Transformers builds upon the memory notion by adding a new compressed memory that stores coarse representations of older memories instead of discarding them. Authors try multiple alternatives for the compression function, finally selecting an attention-reconstruction loss that discards information that is not attended by the network. The use of compressive memory shows large improvements over the modeling of infrequent words, with empirical evidence of the network learning to preserve salient information through the compression mechanism.\nReversible Layers Introduced in: Reformer: The Efficient Transformer by Kitaev et al.\nThe main idea behind reversibility is to enable the recovering of activations in any layer of the network by using only activations of the following layer and model parameters. This feature is especially interesting when applied to Transformer models since they are usually composed of a large pile of stacked layers and their memory complexity grows linearly with the layer count.\nReformer introduces reversibility in the Transformer architecture by combining attention and feed-forward sublayers into a single reversible layer. This allows to store activations only for the topmost layer and recover all the other ones by reversing layers during back-propagation, making the model depth irrelevant memory-wise. Further improvements in memory complexity are achieved by chunking independent computations in feed-forward and reversible layers.\nCross-Layer Parameter Sharing Introduced in: ALBERT: A Lite BERT for Self-supervised Learning of Language Representations by Lan et al.\nA simple yet very effective approach to greatly reduce the parameter count inside deep Transformer models is to share parameters across multiple layers, as it was shown in the Universal Transformer paper at ICLR 2019.\nALBERT authors experiment cross-layer parameter sharing for both self-attention and feed-forward sublayers, finding that sharing both weight matrices contributes to bringing down the total parameter count of the model by a factor of $7\\times$ (for embedding size $E = 128$) while only slightly affecting final performances. The use of parameter sharing leads to smoother transition across layers and effectively stabilizes network parameters.\nAdaptive Depth Estimation Introduced in: Depth-Adaptive Transformer by Elbayad et al.\nCurrent models perform a fixed number of computations for each input, regardless of the underlying complexity specific to each sequence. This problem was already highlighted in the Universal Transformer, which proposes a repeated application of the same layer with adaptive computation time (ACT), but the resulting increase in per-layer weights considerably reduce the overall network speed.\nDepth-adaptive Transformer solves this issue by encoding a sequence with a standard Transformer encoder and decoding it with a variable number of steps. To do so, a classifier is attached to each repeated layer of the decoder and the whole set is then trained with aligned and mixed training (see image) using the anytime prediction approach first introduced in the field of computer vision. Authors explore different mechanisms to adaptively control the amount of computation both on sequence level and on a per-token basis and conclude that an adaptive reduction of more than 75% of decoder layers can be applied without any loss in accuracy on machine translation tasks.\nConclusion Many of the approaches introduced at ICLR 2020 offer widely applicable solutions to specific problems that characterize the original Transformer architecture, ranging from the self-attention computation to the model structure itself.\nMany of these approaches seem promising for future developments of the Transformer and, most importantly, are likely to bring complementary improvements once many of them included in a single architecture.\nMy hope for ICLR 2021 is to see more incremental work that puts together already-existing strategies to highlight the most effective combinations between them.\nSee also: What’s new for Transformers at the ICLR 2020 Conference? by Sergi Castella\n","date":1588526856,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588526856,"objectID":"754ac8a82f8a2d875490e527ac58aa9b","permalink":"https://gsarti.com/post/iclr2020-transformers/","publishdate":"2020-05-03T19:27:36+02:00","relpermalink":"/post/iclr2020-transformers/","section":"post","summary":" A summary of promising directions from ICLR 2020 for better and faster pretrained tranformers language models. ","tags":["Natural Language Processing","Language Modeling","Deep Learning","Transformers","ICLR2020","Word Embeddings","Self-Attention"],"title":"ICLR 2020 Trends: Better \u0026 Faster Transformers for Natural Language Processing","type":"post"},{"authors":["Gabriele Sarti","Francesco Zuppichini","Tommaso Rodani","Marco Franzon","Mirko Lai"],"categories":["Natural Language Processing"],"content":"In 2020, more than 3,000 scientific studies have been published on the SARS-CoV-2 virus and on the Covid-19 pathology. The total number of articles on the topic of coronaviruses exceeds 40,000 units. Such a volume of scientific production makes it impossible for doctors and researchers to keep up with the latest discoveries without the support of adequate digital platforms that are currently nowhere in sight.\nTo make up for this shortcoming, we propose an artificial intelligence system associated with a web application to perform natural language semantic querying inside the COVID-19 Open Research Dataset published by the American nonprofit AllenAI. The system leverages state-of-the-art neural language models trained on scientific publications in the biomedical domain for optimal retrieval performances. The adoption of the system aims to facilitate knowledge sharing across the scientific community and to accelerate the development of adequate drugs and vaccines to counter the ongoing pandemic.\nThe project is led by Gabriele Sarti in collaboration with Area Science Park and the Italian Association of Computational Linguistics. The project used to be publicly available at covidbrowser.areasciencepark.it. You can now refer to the code implementation on GitHub.\n","date":1586563200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586563200,"objectID":"af817b3031f8d4070d0147dac0c93688","permalink":"https://gsarti.com/project/covid-browser/","publishdate":"2020-04-11T00:00:00Z","relpermalink":"/project/covid-browser/","section":"project","summary":"A semantic browser for SARS-CoV-2 and COVID-19 powered by neural language models.","tags":["Natural Language Processing","Deep Learning","Information Extraction","Kaggle Competition","AREA Science Park","Italian Association for Computational Linguistics"],"title":"Covid-19 Semantic Browser","type":"project"},{"authors":["Gabriele Sarti"],"categories":["Natural Language Processing","Academic"],"content":"","date":1574380800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1574380800,"objectID":"01e69dceb562087e59061477e0d6505e","permalink":"https://gsarti.com/talk/neural-lm/","publishdate":"2019-10-27T17:59:41+01:00","relpermalink":"/talk/neural-lm/","section":"talk","summary":"An overview of the latest advances in the field of NLP, with a focus on neural models and language understanding.","tags":["Natural Language Processing","Natural Language Understanding","Language Modeling","Deep Learning","StaTalk","University of Trieste"],"title":"Neural Language Models: the New Frontier of Natural Language Understanding","type":"talk"},{"authors":["Gabriele Sarti","Felice Dell'Orletta","Cristina Fenu"],"categories":["Digital Humanities","Divulgative"],"content":"","date":1572519641,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572519641,"objectID":"3519e5984477ceede5ff698b8a0d638c","permalink":"https://gsarti.com/talk/literary-ordnance/","publishdate":"2019-10-27T17:59:41+01:00","relpermalink":"/talk/literary-ordnance/","section":"talk","summary":"Discussing the applications of AI and NLP in the fields of literature and digital humanities.","tags":["Natural Language Processing","Digital Humanities","Natural Language Generation","Science+Fiction Festival"],"title":"The Literary Ordnance: When the Writer is an AI","type":"talk"},{"authors":["Gabriele Sarti","Cristina Fenu","Eric Medvet"],"categories":["Digital Humanities"],"content":"We developed an interactive experience putting together NLP and literature to raise awareness on the latest developments in language modeling and natural language generation. Participants received a printed letter written by a neural language model (GPT-2) fine-tuned on the Italian epistolary corpus of Italo Svevo, an italian writer of the 20th century. Participants could choose among several topics discussed by the author in his letters, and were also given some context on author\u0026rsquo;s life and literary production by Cristina Fenu, a digital humanist working at the Svevian Museum of Trieste. An open-source implementation will soon be available on Github.\n","date":1569542400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569542400,"objectID":"effd515fe0067a47ad486378929beac6","permalink":"https://gsarti.com/project/aitalo-svevo/","publishdate":"2019-09-27T00:00:00Z","relpermalink":"/project/aitalo-svevo/","section":"project","summary":"Generating letters with a neural language model in the style of Italo Svevo, a famous italian writer of the 20th century.","tags":["Natural Language Generation","Language Modeling","Italian NLP","Trieste Next"],"title":"AItalo Svevo: Letters from an Artificial Intelligence","type":"project"},{"authors":["Gabriele Sarti"],"categories":["Deep Learning","Academic"],"content":"","date":1563408000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563408000,"objectID":"0de45716e8e1d68d3d4fce6d94bc1eac","permalink":"https://gsarti.com/talk/lth/","publishdate":"2019-11-22T10:42:03+01:00","relpermalink":"/talk/lth/","section":"talk","summary":"Is it possible to induce sparseness in neural networks while preserving its performances? An overview of latest advances in making neural approaches more parsimonious","tags":["Deep Learning","Course Presentation","University of Trieste"],"title":"Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks","type":"talk"},{"authors":["Gabriele Sarti","Leonardo Stincone","Andrea Lorenzon"],"categories":["Computer Vision"],"content":"As our final project for the course of Statistical Machine Learning held by Prof. Luca Bortolussi we explored different statistical and deep approaches to the problem of detecting tumors in histopathologic scans. We notably tried a random forest on distributional features extracted by pigment segmentation, a state-of-the-art DenseNet and a Capsule Network with Dynamic Routing.\n","date":1561680000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561680000,"objectID":"0964199e2bac61b39ed684cfcebe5fa0","permalink":"https://gsarti.com/project/cancer-detection/","publishdate":"2019-06-28T00:00:00Z","relpermalink":"/project/cancer-detection/","section":"project","summary":"A journey into the state of the art of histopathologic cancer detection approaches.","tags":["Computer Vision","Capsule Networks","Cancer Detection","Kaggle Competition","University of Trieste"],"title":"Histopathologic Cancer Detection with Neural Networks","type":"project"},{"authors":null,"categories":null,"content":"\u003c!DOCTYPE html\u003e F CO2 Emissions Related to Experiments | Cognitively-informed Natural Language Processing Systems for Linguistic Complexity Assessment Introduction 1 Linguistic Complexity 1.1 Categorizing Linguistic Complexity Measures 1.2 Intrinsic Perspective 1.2.1 Structural Linguistic Complexity 1.2.2 Language Modeling Surprisal 1.3 Extrinsic Perspective 1.3.1 Automatic Readability Assessment 1.3.2 Perceived Complexity Prediction 1.3.3 Gaze Metrics Prediction 1.4 Garden-path Sentences 2 Models of Linguistic Complexity 2.1 Desiderata for Models of Linguistic Complexity 2.2 Neural Language Models: Unsupervised Multitask Learners 2.2.1 Emergent Linguistic Structures in Neural Language Models 2.3 Analyzing Neural Models of Complexity 2.3.1 Probing classifiers 2.3.2 Representational Similarity Analysis 2.3.3 Projection-Weighted Canonical Correlation Analysis 3 Complexity Phenomena in Linguistic Annotations and Language Models 3.1 Data and Preprocessing 3.2 Analysis of Linguistic Phenomena 3.2.1 Linguistic Phenomena in Length-controlled Bins 3.3 Modeling Online and Offline Linguistic Complexity 3.3.1 Modeling Complexity in Length-controlled Bins 3.4 Probing Linguistic Phenomena in ALBERT Representations 3.5 Summary 4 Representational Similarity in Models of Complexity 4.1 Knowledge-driven Requirements for Learning Models 4.2 Experiments 4.2.1 Data 4.2.2 Inter-model Representational Similarity 4.2.3 Intra-model Representational Similarity 4.3 Summary 5 Cognitive Phenomena in Cognitively Informed Language Models 5.1 Data and Preprocessing 5.2 Summary Conclusion Future Directions Appendix A Linguistic Features A.1 Raw Text Properties and Lexical Variety A.2 Morpho-syntacting Information A.3 Verbal Predicate Structure A.4 Global and Local Parsed Tree Structures A.5 Syntactic Relations A.6 Subordination Phenomena B Additional Precisions on Eye-tracking Metrics and Preprocessing C Multi-task Token-level Modeling for Gaze Metrics Prediction D Intra-model Similarity for All Models E Model parametrization F CO2 Emissions Related to Experiments References Back to my website Cognitively-informed\nNatural Language Processing Systems\nfor Linguistic Complexity Assessment F CO2 Emissions Related to Experiments Experiments were conducted using the private infrastructure of the ItaliaNLP Lab22 at the Institute for Computational Linguistics “A. Zampolli” (ILC-CNR) in Pisa, which has an estimated carbon efficiency of 0.358 kgCO\\(_2\\)eq/kWh (Moro and Lonza 2018). A cumulative of roughly 500 hours of computation was performed on hardware of type Tesla K40 (TDP of 245W). Total emissions are estimated to be 52.63 kgCO\\(_2\\)eq\nEstimations were conducted using the Machine Learning Impact Calculator23 presented in Lacoste et al. (2019).\nReferences Lacoste, Alexandre, Alexandra Luccioni, Victor Schmidt, and Thomas Dandres. 2019. “Quantifying the Carbon Emissions of Machine Learning.” arXiv Preprint arXiv:1910.09700.\nMoro, Alberto, and Laura Lonza. 2018. “Electricity Carbon Intensity in European Member States: Impacts on Ghg Emissions of Electric Vehicles.” Transportation Research Part D: Transport and Environment 64. Elsevier: 5–14.\nhttps://www.italianlp.it↩\nhttps://mlco2.github.io/impact#compute↩\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"ea1057b1d10098cc3467883d648d89b9","permalink":"https://gsarti.com/msc-thesis/app-co2/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/msc-thesis/app-co2/","section":"msc-thesis","summary":"\u003c!DOCTYPE html\u003e F CO2 Emissions Related to Experiments | Cognitively-informed Natural Language Processing Systems for Linguistic Complexity Assessment Introduction 1 Linguistic Complexity 1.1 Categorizing Linguistic Complexity Measures 1.2 Intrinsic Perspective 1.2.1 Structural Linguistic Complexity 1.2.2 Language Modeling Surprisal 1.3 Extrinsic Perspective 1.3.1 Automatic Readability Assessment 1.3.2 Perceived Complexity Prediction 1.3.3 Gaze Metrics Prediction 1.4 Garden-path Sentences 2 Models of Linguistic Complexity 2.1 Desiderata for Models of Linguistic Complexity 2.2 Neural Language Models: Unsupervised Multitask Learners 2.","tags":null,"title":"","type":"msc-thesis"},{"authors":null,"categories":null,"content":"\u003c!DOCTYPE html\u003e B Precisions on Eye-tracking Metrics and Preprocessing | Interpreting Neural Language Models for Linguistic Complexity Assessment Introduction 1 Linguistic Complexity 1.1 Categorizing Linguistic Complexity Measures 1.2 Intrinsic Perspective 1.2.1 Structural Linguistic Complexity 1.2.2 Language Modeling Surprisal 1.3 Extrinsic Perspective 1.3.1 Automatic Readability Assessment 1.3.2 Perceived Complexity Prediction 1.3.3 Gaze Metrics Prediction 1.4 Garden-path Sentences 2 Models of Linguistic Complexity 2.1 Desiderata for Models of Linguistic Complexity 2.2 Neural Language Models: Unsupervised Multitask Learners 2.2.1 Emergent Linguistic Structures in Neural Language Models 2.3 Analyzing Neural Models of Complexity 2.3.1 Probing classifiers 2.3.2 Representational Similarity Analysis 2.3.3 Projection-Weighted Canonical Correlation Analysis 3 Complexity Phenomena in Linguistic Annotations and Language Models 3.1 Data and Preprocessing 3.2 Analysis of Linguistic Phenomena 3.2.1 Linguistic Phenomena in Length-controlled Bins 3.3 Modeling Online and Offline Linguistic Complexity 3.3.1 Modeling Complexity in Length-controlled Bins 3.4 Probing Linguistic Phenomena in ALBERT Representations 3.5 Summary 4 Representational Similarity in Models of Complexity 4.1 Knowledge-driven Requirements for Learning Models 4.2 Experimentsl Evaluation 4.2.1 Data 4.2.2 Inter-model Representational Similarity 4.2.3 Intra-model Representational Similarity 4.3 Summary 5 Gaze-informed Models for Cognitive Processing Prediction 5.1 Experimental Setup 5.2 Experimental Evaluation 5.2.1 Estimating Magnitudes of Garden-path Delays 5.2.2 Predicting Delays with Surprisal and Gaze Metrics 5.3 Summary Conclusion Broader Impact and Ethical Perspectives Future Directions Appendix A Linguistic Features A.1 Raw Text Properties and Lexical Variety A.2 Morpho-syntacting Information A.3 Verbal Predicate Structure A.4 Global and Local Parsed Tree Structures A.5 Syntactic Relations A.6 Subordination Phenomena B Precisions on Eye-tracking Metrics and Preprocessing C Multi-task Token-level Regression for Gaze Metrics Prediction D Intra-model Similarity for All Models E Gaze Metrics Predictions for Garden Path Sentences F Reproducibility and Environmental Impact References Back to my website Interpreting Neural Language Models\nfor Linguistic Complexity Assessment B Precisions on Eye-tracking Metrics and Preprocessing Table B.1: Eye-tracking mappings from dataset-specific fields to the shared set of metrics. Metrics Dundee GECO ZuCo 1 \u0026amp; 2 First fix. dur. (FFD) First_fix_dur FIRST_FIXATION_DURATION FFD First pass dur. (FPD) First_pass_dur GAZE_DURATION GD Fix. prob. (FXP) Fix_prob ¬ WORD_SKIP FXC \u0026gt; 0 Fix. count (FXC) nFix FIXATION_COUNT FXC Tot. fix. Dur. (TFD) Tot_fix_dur TOT_READ_TIME TRT Tot. Regres. Dur. (TRD) Tot_regres_from_dur GO_PAST - SEL._GO_PAST GPT - GD Univocal gaze metrics conversion Table B.1 present the conversion scheme used to obtain a unified set of eye-tracking metrics from different corpora annotations. This method follows closely the approach adopted by Hollenstein and Zhang (2019). While the mapping is straightforward for shared metrics, the TRD metric needs to be computed for GECO and ZuCo. For GECO, the difference between go-past time (i.e. total time elapsed between the first access of a word boundary and the first access of subsequent words, including regressions) and its selective variant (i.e. go-past time only relative to the specific word, without accounting for regressions) gives an exact conversion to regression duration. Instead, in the ZuCo case, an approximate conversion using gaze duration (i.e. first pass duration) instead of selective go-past time is used since selective go-past time is not provided. ZuCo’s TRD estimate should be deemed an upper bound for regressions’ duration since gaze duration is always smaller than the selective go-past time when regressions are present and is precisely equal to it in the complete absence of regressions.\nAveraging across participants Gaze metrics are averaged across participants for all experiments of this thesis work. Metrics missing for some participants due to skipping are replaced with the lowest recorded value across participants for that word before averaging. This procedure is preferred to zero-filling missing values since the latter produces significant drops in metrics associated with tokens skipped by multiple participants, making averaged values inconsistent with empirical observations.\nReferences Hollenstein, Nora, and Ce Zhang. 2019. “Entity Recognition at First Sight: Improving NER with Eye Movement Information.” In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), 1–10. Minneapolis, Minnesota: Association for Computational Linguistics. https://doi.org/10.18653/v1/N19-1001.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"9045e16cf8c8b69e6ceff40d0174715a","permalink":"https://gsarti.com/msc-thesis/app-et-metrics/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/msc-thesis/app-et-metrics/","section":"msc-thesis","summary":"\u003c!DOCTYPE html\u003e B Precisions on Eye-tracking Metrics and Preprocessing | Interpreting Neural Language Models for Linguistic Complexity Assessment Introduction 1 Linguistic Complexity 1.1 Categorizing Linguistic Complexity Measures 1.2 Intrinsic Perspective 1.2.1 Structural Linguistic Complexity 1.2.2 Language Modeling Surprisal 1.3 Extrinsic Perspective 1.3.1 Automatic Readability Assessment 1.3.2 Perceived Complexity Prediction 1.3.3 Gaze Metrics Prediction 1.4 Garden-path Sentences 2 Models of Linguistic Complexity 2.1 Desiderata for Models of Linguistic Complexity 2.2 Neural Language Models: Unsupervised Multitask Learners 2.","tags":null,"title":"","type":"msc-thesis"},{"authors":null,"categories":null,"content":"\u003c!DOCTYPE html\u003e C Multi-task Token-level Regression for Gaze Metrics Prediction | Interpreting Neural Language Models for Linguistic Complexity Assessment Introduction 1 Linguistic Complexity 1.1 Categorizing Linguistic Complexity Measures 1.2 Intrinsic Perspective 1.2.1 Structural Linguistic Complexity 1.2.2 Language Modeling Surprisal 1.3 Extrinsic Perspective 1.3.1 Automatic Readability Assessment 1.3.2 Perceived Complexity Prediction 1.3.3 Gaze Metrics Prediction 1.4 Garden-path Sentences 2 Models of Linguistic Complexity 2.1 Desiderata for Models of Linguistic Complexity 2.2 Neural Language Models: Unsupervised Multitask Learners 2.2.1 Emergent Linguistic Structures in Neural Language Models 2.3 Analyzing Neural Models of Complexity 2.3.1 Probing classifiers 2.3.2 Representational Similarity Analysis 2.3.3 Projection-Weighted Canonical Correlation Analysis 3 Complexity Phenomena in Linguistic Annotations and Language Models 3.1 Data and Preprocessing 3.2 Analysis of Linguistic Phenomena 3.2.1 Linguistic Phenomena in Length-controlled Bins 3.3 Modeling Online and Offline Linguistic Complexity 3.3.1 Modeling Complexity in Length-controlled Bins 3.4 Probing Linguistic Phenomena in ALBERT Representations 3.5 Summary 4 Representational Similarity in Models of Complexity 4.1 Knowledge-driven Requirements for Learning Models 4.2 Experimentsl Evaluation 4.2.1 Data 4.2.2 Inter-model Representational Similarity 4.2.3 Intra-model Representational Similarity 4.3 Summary 5 Gaze-informed Models for Cognitive Processing Prediction 5.1 Experimental Setup 5.2 Experimental Evaluation 5.2.1 Estimating Magnitudes of Garden-path Delays 5.2.2 Predicting Delays with Surprisal and Gaze Metrics 5.3 Summary Conclusion Broader Impact and Ethical Perspectives Future Directions Appendix A Linguistic Features A.1 Raw Text Properties and Lexical Variety A.2 Morpho-syntacting Information A.3 Verbal Predicate Structure A.4 Global and Local Parsed Tree Structures A.5 Syntactic Relations A.6 Subordination Phenomena B Precisions on Eye-tracking Metrics and Preprocessing C Multi-task Token-level Regression for Gaze Metrics Prediction D Intra-model Similarity for All Models E Gaze Metrics Predictions for Garden Path Sentences F Reproducibility and Environmental Impact References Back to my website Interpreting Neural Language Models\nfor Linguistic Complexity Assessment C Multi-task Token-level Regression for Gaze Metrics Prediction Figure C.1: Multi-task token-level regression on eye-tracking annotations. Preceding punctuation is removed (1), and the sentence is tokenized while keeping track of non-initial tokens (2). Embeddings are fed to the ALBERT model (3), and non-initial representations are masked to ensure a one-to-one mapping between labels and predictions (4). Finally, task-specific prediction heads are used to predict gaze metrics in a multitask setting with hard parameter sharing (5). A multitask token-level regression fine-tuning approach was adopted throughout this study to predict eye-tracking metrics using neural language models. This novel approach’s choice stems from the fact that the regression task of predicting gaze metrics is inherently word-based given the granularity of eye-tracking annotations and that different gaze metrics provide complementary viewpoints over multiple stages of cognitive processing and can as such be modeled more precisely in a multitask learning setting. Figure C.1 presents the model’s training and inference procedure, closely matching other approaches used to train neural language models for sequence tagging tasks like POS tagging and named entity recognition.\nThe most defining detail in the procedure is the need to preserve an exact one-to-one mapping between input words and gaze metrics annotations, which is non-trivial in light of subword tokenization approaches that represent nowadays the de facto standard for training modern neural language models. To enforce such mapping, two steps are taken. First, all initial punctuation (e.g. the open parenthesis before processing in Figure C.1 example) is removed to make the initial subword token for that word (i.e. the one preceded by whitespace) equal to the word’s first characters. Then, all non-initial subword tokens are identified in step (2), and their respective embeddings are masked in step (4) before passing the remaining initial embeddings (one per whitespace-tokenized word at this point, as for gaze metrics) to the set of prediction heads responsible for inferring individual gaze metrics. While this procedure can be regarded as suboptimal since not all learned representations are used for prediction, it is essential to remember that all the embeddings produced by attention-based neural language models are contextualized and encode information about the entire sentence and surrounding context to some extent. In this sense, initial token embeddings can be trained in this setting to predict gaze metrics relative to the whole word, effectively bypassing the issues about information loss raised by the masking procedure.\nAnother important detail in the training and inference procedure is the standardization of metrics, which plays a key role in this setup due to the different ranges of different metrics (e.g. fixation probability is always defined in the interval \\([0,1]\\), while gaze durations are integers in the scale of hundreds/thousands of milliseconds). Specifically, considering the set \\(X\\) of values assumed by a specific metric for all tokens in the eye-tracking datasets, the average \\(\\mu_X\\) and standard deviation \\(\\sigma_X\\) of those values are computed, and each value is transformed as: \\[\\begin{equation} X_i\u0026#39; = \\frac{X_i - \\mu_X}{\\sigma_X} \\end{equation}\\] to produce a new range \\(X\u0026#39;\\) with average equal to \\(0\\) and standard deviation equal to \\(1\\). Predicted values are then reconverted to the original scale as \\(X_i = (X\u0026#39;_i \\cdot \\sigma_X) + \\mu_X\\) when performing inference, and training and testing metrics are computed on each metric’s original scale.\nSpillover concatenation Cognitive processing literature reports evidence of reading times for a word being shaped not only by the predictability of the word itself but also by the predictability of the words that precede it (Smith and Levy 2013) in what is commonly referred to as the spillover effect (Mitchell 1984). The existence of spillover has important implications in the context of this gaze metrics prediction approach since the embeddings for a single word may not contain enough information to predict the influence of preceding tokens in shaping reading behaviors. Notably, Schijndel and Linzen (2020) include the surprisal of the three previous words in a mixed-effect model used to estimate a surprisal-to-reading-times conversion coefficient. While it can be hypothesized that in this approach, the usage of contextualized word embeddings can automatically account for this type of interaction, the effect of leveraging preceding tokens for the current token’s metric prediction is assessed to confirm this hypothesis. A new procedure defined as spillover concatenation is introduced for this purpose, in which token embeddings are augmented by performing a rolling concatenation of the \\(n\\) preceding embeddings before feeding the final representation to prediction heads. Initial tokens are padded with \\(0\\) vectors to match the fixed size defined by embedding size and the \\(n\\) parameter. For example, using spillover concatenation with \\(n = 3\\) within a BERT model with a hidden size of 768 involves having prediction heads taking input size of \\(768 \\cdot (3 + 1) = 3072\\), the size of the token embedding for which gaze metrics should be predicted plus the size of the three preceding token embeddings. In this way, information about preceding tokens is explicitly included at prediction time.\nFigure C.2 shows the validation losses during training for the two models used in the experiments of Chapter 5 with their counterparts using spillover concatenation. Model performances are not positively influenced by introducing the concatenation technique and remain very similar for both architectures.\nFigure C.2: Validation total loss for GPT-2 and ALBERT over a split of the eye-tracking merged corpora with and without spillover concatenation. Model predictive performances were comparable across training and testing for the two models. Model performances Table C.1 presents the test performances of ALBERT and GPT-2 models trained with and without the spillover concatenation approach on the merge of all eye-tracking corpora. The top two rows present descriptive statistics about extreme values, the mean and standard deviation in annotations averaged across participants for each metric. It is interesting to observe that the maximum value observed for first pass duration (FPD) is higher than the one for total fixation duration (TFD). While this situation would not be possible in practice due to first pass duration being included in total reading times, it reminds us about the approximate nature of our filling-and-averaging procedure described in Appendix B. Comparing results to those of Table 3.2, where gaze metrics were modeled at the sentence level, we observe much worse results in terms of explained variance for both models: while fixations and first pass duration (FXC, FXP, FPD) are generally well modeled, worse results are obtained for first and total fixation durations (FFD, TFD), and in particular for the duration of regression (TRD). These results can be attributed to the merging of different corpora that, being annotated by different participants, present very different properties, as shown in Table 1.4 and Figure 5.2. While on the one hand, this choice harms modeling performances, on the other hand, it provides us with more representative results for the general setting.\nTable C.1: Descriptive statistics and model performances for the merged eye-tracking training corpus. Model scores are in format \\(\\text{RMSE}_{\\text{MAX}}|R^2\\), where RMSE is the root-mean-squared error and MAX is the max error for model predictions. FFD FPD FXP FXC TFD TRD min-max value \\(0-986\\) \\(0-2327\\) \\(0-1\\) \\(0-8.18\\) \\(0-1804\\) \\(0-4055\\) \\(\\mu|\\sigma\\) statistics \\(162|50\\) \\(188|86\\) \\(.56|.27\\) \\(.85|.53\\) \\(206|87\\) \\(90|122\\) ALBERT \\(41_{78}|.33\\) \\(61_{121}|.50\\) \\(.17_{.32}|.60\\) \\(.31_{.62}|.66\\) \\(65_{132}|.44\\) \\(110_{207}|.19\\) ALBERT Spillover \\(41_{78}|.33\\) \\(61_{122}|.50\\) \\(.17_{.33}|.60\\) \\(.31_{.62}|.66\\) \\(65_{132}|.44\\) \\(110_{208}|.19\\) GPT-2 \\(44_{83}|.23\\) \\(68_{136}|.37\\) \\(.18_{.35}|.56\\) \\(.36_{.70}|.54\\) \\(74_{149}|.28\\) \\(115_{222}|.11\\) GPT-2 Spillover \\(43_{83}|.26\\) \\(68_{135}|.37\\) \\(.19_{.35}|.50\\) \\(.36_{.70}|.54\\) \\(73_{146}|.30\\) \\(116_{220}|.10\\) In general, better performances are observed for the masked language model ALBERT, suggesting the importance of having access to bidirectional context for gaze metrics prediction. Results present additional evidence supporting the superfluity of the spillover concatenation procedure, which was henceforth dropped in the context of Chapters 4 and 5’s experiments. Although good scores in terms of average and maximal errors are observed for all metrics, the relatively low \\(R^2\\) seem to suggest that large margins of improvement are still available in the context of gaze metrics predictions with neural language models.\nReferences Mitchell, Don C. 1984. “An Evaluation of Subject-Paced Reading Tasks and Other Methods for Investigating Immediate Processes in Reading.” New Methods in Reading Comprehension Research, 69–89.\nSchijndel, Marten van, and Tal Linzen. 2020. “Single-Stage Prediction Models Do Not Explain the Magnitude of Syntactic Disambiguation Difficulty.” PsyArXiv Pre-Print sgbqy. https://psyarxiv.com/sgbqy/.\nSmith, Nathaniel J, and Roger Levy. 2013. “The Effect of Word Predictability on Reading Time Is Logarithmic.” Cognition 128 (3). Elsevier: 302–19.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"e54a69c10214d5383864ccac27c37e57","permalink":"https://gsarti.com/msc-thesis/app-et-modeling/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/msc-thesis/app-et-modeling/","section":"msc-thesis","summary":"\u003c!DOCTYPE html\u003e C Multi-task Token-level Regression for Gaze Metrics Prediction | Interpreting Neural Language Models for Linguistic Complexity Assessment Introduction 1 Linguistic Complexity 1.1 Categorizing Linguistic Complexity Measures 1.2 Intrinsic Perspective 1.2.1 Structural Linguistic Complexity 1.2.2 Language Modeling Surprisal 1.3 Extrinsic Perspective 1.3.1 Automatic Readability Assessment 1.3.2 Perceived Complexity Prediction 1.3.3 Gaze Metrics Prediction 1.4 Garden-path Sentences 2 Models of Linguistic Complexity 2.1 Desiderata for Models of Linguistic Complexity 2.2 Neural Language Models: Unsupervised Multitask Learners 2.","tags":null,"title":"","type":"msc-thesis"},{"authors":null,"categories":null,"content":"\u003c!DOCTYPE html\u003e E Gaze Metrics Predictions for Garden Path Sentences | Interpreting Neural Language Models for Linguistic Complexity Assessment Introduction 1 Linguistic Complexity 1.1 Categorizing Linguistic Complexity Measures 1.2 Intrinsic Perspective 1.2.1 Structural Linguistic Complexity 1.2.2 Language Modeling Surprisal 1.3 Extrinsic Perspective 1.3.1 Automatic Readability Assessment 1.3.2 Perceived Complexity Prediction 1.3.3 Gaze Metrics Prediction 1.4 Garden-path Sentences 2 Models of Linguistic Complexity 2.1 Desiderata for Models of Linguistic Complexity 2.2 Neural Language Models: Unsupervised Multitask Learners 2.2.1 Emergent Linguistic Structures in Neural Language Models 2.3 Analyzing Neural Models of Complexity 2.3.1 Probing classifiers 2.3.2 Representational Similarity Analysis 2.3.3 Projection-Weighted Canonical Correlation Analysis 3 Complexity Phenomena in Linguistic Annotations and Language Models 3.1 Data and Preprocessing 3.2 Analysis of Linguistic Phenomena 3.2.1 Linguistic Phenomena in Length-controlled Bins 3.3 Modeling Online and Offline Linguistic Complexity 3.3.1 Modeling Complexity in Length-controlled Bins 3.4 Probing Linguistic Phenomena in ALBERT Representations 3.5 Summary 4 Representational Similarity in Models of Complexity 4.1 Knowledge-driven Requirements for Learning Models 4.2 Experimentsl Evaluation 4.2.1 Data 4.2.2 Inter-model Representational Similarity 4.2.3 Intra-model Representational Similarity 4.3 Summary 5 Gaze-informed Models for Cognitive Processing Prediction 5.1 Experimental Setup 5.2 Experimental Evaluation 5.2.1 Estimating Magnitudes of Garden-path Delays 5.2.2 Predicting Delays with Surprisal and Gaze Metrics 5.3 Summary Conclusion Broader Impact and Ethical Perspectives Future Directions Appendix A Linguistic Features A.1 Raw Text Properties and Lexical Variety A.2 Morpho-syntacting Information A.3 Verbal Predicate Structure A.4 Global and Local Parsed Tree Structures A.5 Syntactic Relations A.6 Subordination Phenomena B Precisions on Eye-tracking Metrics and Preprocessing C Multi-task Token-level Regression for Gaze Metrics Prediction D Intra-model Similarity for All Models E Gaze Metrics Predictions for Garden Path Sentences F Reproducibility and Environmental Impact References Back to my website Interpreting Neural Language Models\nfor Linguistic Complexity Assessment E Gaze Metrics Predictions for Garden Path Sentences Figure E.1: Average GPT2-ET gaze metrics predictions for the “NP/Z Ambiguity with Verb Transitivity” SyntaxGym test suite. Bars show 95% confidence intervals. Units are in ms for durations, % for FXP, and raw counts for FXC. Figure E.2: Average GPT2-ET gaze metrics predictions for the “NP/Z Ambiguity with Overt Object” SyntaxGym test suite. Bars show 95% confidence intervals. Units are in ms for durations, % for FXP, and raw counts for FXC. Figure E.3: Average GPT2-ET gaze metrics predictions for the “MV/RR Ambiguity” SyntaxGym test suite. Bars show 95% confidence intervals. Units are in ms for durations, % for FXP, and raw counts for FXC. ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"579dd4e196b080de2ae442f0b65c1c8d","permalink":"https://gsarti.com/msc-thesis/app-garden-paths-et/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/msc-thesis/app-garden-paths-et/","section":"msc-thesis","summary":"\u003c!DOCTYPE html\u003e E Gaze Metrics Predictions for Garden Path Sentences | Interpreting Neural Language Models for Linguistic Complexity Assessment Introduction 1 Linguistic Complexity 1.1 Categorizing Linguistic Complexity Measures 1.2 Intrinsic Perspective 1.2.1 Structural Linguistic Complexity 1.2.2 Language Modeling Surprisal 1.3 Extrinsic Perspective 1.3.1 Automatic Readability Assessment 1.3.2 Perceived Complexity Prediction 1.3.3 Gaze Metrics Prediction 1.4 Garden-path Sentences 2 Models of Linguistic Complexity 2.1 Desiderata for Models of Linguistic Complexity 2.2 Neural Language Models: Unsupervised Multitask Learners 2.","tags":null,"title":"","type":"msc-thesis"},{"authors":null,"categories":null,"content":"\u003c!DOCTYPE html\u003e D Intra-model Similarity for All Models | Interpreting Neural Language Models for Linguistic Complexity Assessment Introduction 1 Linguistic Complexity 1.1 Categorizing Linguistic Complexity Measures 1.2 Intrinsic Perspective 1.2.1 Structural Linguistic Complexity 1.2.2 Language Modeling Surprisal 1.3 Extrinsic Perspective 1.3.1 Automatic Readability Assessment 1.3.2 Perceived Complexity Prediction 1.3.3 Gaze Metrics Prediction 1.4 Garden-path Sentences 2 Models of Linguistic Complexity 2.1 Desiderata for Models of Linguistic Complexity 2.2 Neural Language Models: Unsupervised Multitask Learners 2.2.1 Emergent Linguistic Structures in Neural Language Models 2.3 Analyzing Neural Models of Complexity 2.3.1 Probing classifiers 2.3.2 Representational Similarity Analysis 2.3.3 Projection-Weighted Canonical Correlation Analysis 3 Complexity Phenomena in Linguistic Annotations and Language Models 3.1 Data and Preprocessing 3.2 Analysis of Linguistic Phenomena 3.2.1 Linguistic Phenomena in Length-controlled Bins 3.3 Modeling Online and Offline Linguistic Complexity 3.3.1 Modeling Complexity in Length-controlled Bins 3.4 Probing Linguistic Phenomena in ALBERT Representations 3.5 Summary 4 Representational Similarity in Models of Complexity 4.1 Knowledge-driven Requirements for Learning Models 4.2 Experimentsl Evaluation 4.2.1 Data 4.2.2 Inter-model Representational Similarity 4.2.3 Intra-model Representational Similarity 4.3 Summary 5 Gaze-informed Models for Cognitive Processing Prediction 5.1 Experimental Setup 5.2 Experimental Evaluation 5.2.1 Estimating Magnitudes of Garden-path Delays 5.2.2 Predicting Delays with Surprisal and Gaze Metrics 5.3 Summary Conclusion Broader Impact and Ethical Perspectives Future Directions Appendix A Linguistic Features A.1 Raw Text Properties and Lexical Variety A.2 Morpho-syntacting Information A.3 Verbal Predicate Structure A.4 Global and Local Parsed Tree Structures A.5 Syntactic Relations A.6 Subordination Phenomena B Precisions on Eye-tracking Metrics and Preprocessing C Multi-task Token-level Regression for Gaze Metrics Prediction D Intra-model Similarity for All Models E Gaze Metrics Predictions for Garden Path Sentences F Reproducibility and Environmental Impact References Back to my website Interpreting Neural Language Models\nfor Linguistic Complexity Assessment D Intra-model Similarity for All Models Figure D.1: Intra-model RSA (left) and PWCCA (right) scores across layers’ combinations for the ALBERT model fine-tuned on perceived complexity annotations (PC) using the [CLS] token (top), the all-token average (middle), and all tokens (bottom) representations. Layer -1 corresponds to the last layer before prediction heads. Figure D.2: Intra-model RSA (left) and PWCCA (right) scores across layers’ combinations for the ALBERT model fine-tuned in parallel on gaze metrics (ET) using the [CLS] token (top), the all-token average (middle), and all tokens (bottom) representations. Layer -1 corresponds to the last layer before prediction heads. Figure D.3: Intra-model RSA (left) and PWCCA (right) scores across layers’ combinations for the ALBERT model fine-tuned on readability assessment annotations (RA) using the [CLS] token (top), the all-token average (middle), and all tokens (bottom) representations. Layer -1 corresponds to the last layer before prediction heads. ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"75957a0fecd385e9b84fa27d90b76121","permalink":"https://gsarti.com/msc-thesis/app-intra-sim/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/msc-thesis/app-intra-sim/","section":"msc-thesis","summary":"\u003c!DOCTYPE html\u003e D Intra-model Similarity for All Models | Interpreting Neural Language Models for Linguistic Complexity Assessment Introduction 1 Linguistic Complexity 1.1 Categorizing Linguistic Complexity Measures 1.2 Intrinsic Perspective 1.2.1 Structural Linguistic Complexity 1.2.2 Language Modeling Surprisal 1.3 Extrinsic Perspective 1.3.1 Automatic Readability Assessment 1.3.2 Perceived Complexity Prediction 1.3.3 Gaze Metrics Prediction 1.4 Garden-path Sentences 2 Models of Linguistic Complexity 2.1 Desiderata for Models of Linguistic Complexity 2.2 Neural Language Models: Unsupervised Multitask Learners 2.","tags":null,"title":"","type":"msc-thesis"},{"authors":null,"categories":null,"content":"\u003c!DOCTYPE html\u003e A Linguistic Features | Interpreting Neural Language Models for Linguistic Complexity Assessment Introduction 1 Linguistic Complexity 1.1 Categorizing Linguistic Complexity Measures 1.2 Intrinsic Perspective 1.2.1 Structural Linguistic Complexity 1.2.2 Language Modeling Surprisal 1.3 Extrinsic Perspective 1.3.1 Automatic Readability Assessment 1.3.2 Perceived Complexity Prediction 1.3.3 Gaze Metrics Prediction 1.4 Garden-path Sentences 2 Models of Linguistic Complexity 2.1 Desiderata for Models of Linguistic Complexity 2.2 Neural Language Models: Unsupervised Multitask Learners 2.2.1 Emergent Linguistic Structures in Neural Language Models 2.3 Analyzing Neural Models of Complexity 2.3.1 Probing classifiers 2.3.2 Representational Similarity Analysis 2.3.3 Projection-Weighted Canonical Correlation Analysis 3 Complexity Phenomena in Linguistic Annotations and Language Models 3.1 Data and Preprocessing 3.2 Analysis of Linguistic Phenomena 3.2.1 Linguistic Phenomena in Length-controlled Bins 3.3 Modeling Online and Offline Linguistic Complexity 3.3.1 Modeling Complexity in Length-controlled Bins 3.4 Probing Linguistic Phenomena in ALBERT Representations 3.5 Summary 4 Representational Similarity in Models of Complexity 4.1 Knowledge-driven Requirements for Learning Models 4.2 Experimentsl Evaluation 4.2.1 Data 4.2.2 Inter-model Representational Similarity 4.2.3 Intra-model Representational Similarity 4.3 Summary 5 Gaze-informed Models for Cognitive Processing Prediction 5.1 Experimental Setup 5.2 Experimental Evaluation 5.2.1 Estimating Magnitudes of Garden-path Delays 5.2.2 Predicting Delays with Surprisal and Gaze Metrics 5.3 Summary Conclusion Broader Impact and Ethical Perspectives Future Directions Appendix A Linguistic Features A.1 Raw Text Properties and Lexical Variety A.2 Morpho-syntacting Information A.3 Verbal Predicate Structure A.4 Global and Local Parsed Tree Structures A.5 Syntactic Relations A.6 Subordination Phenomena B Precisions on Eye-tracking Metrics and Preprocessing C Multi-task Token-level Regression for Gaze Metrics Prediction D Intra-model Similarity for All Models E Gaze Metrics Predictions for Garden Path Sentences F Reproducibility and Environmental Impact References Back to my website Interpreting Neural Language Models\nfor Linguistic Complexity Assessment A Linguistic Features The following list of features was used in the context of Chapter 3 experiments and is a summary of the full set of features presented in Brunato et al. (2020):\nA.1 Raw Text Properties and Lexical Variety Sentence length (n_tokens): Length of the sentence in terms of number of tokens.\nWord length (char_per_tok): Average number of characters per word in a sentence, excluding punctuation.\nType/Token Ratio for forms and lemmas (ttr_form, ttr_lemma): Ratio between the number of lexical types and the number of tokens within a sentence.\nA.2 Morpho-syntacting Information Distribution of grammatical categories (upos_dist_*, xpos_dist_*): Percentage distribution in the sentence of the 17 core part-of-speech categories present in the Universal POS tagset (adjective, adverb, interjection, noun, proper noun, verb, adposition, auxiliary, coordinating conjunction, determiner, numeral, particle, pronoun and subordinating conjunction, punctuation, and symbols).\nLexical density (lexical_density): Ratio of content words (verbs, nouns, adjectives, and adverbs) over the total number of words in a sentence.\nInflectional morphology (aux_mood_*, aux_tense_*): Percentage distribution in the sentence of a set of inflectional features (Mood, Number, Person, Tense and Verbal Form*) over lexical verbs and auxiliaries of each sentence.\nA.3 Verbal Predicate Structure Distribution of verbal heads (vb_head_per_sent): Number of verbal heads in the sentence, corresponding to the number of main or subordinate clauses co-occurring in it.\nDistribution of verbal roots (dep_dist_root): Percentage of verbal roots out of the total sentence roots.\nVerb arity (verb_arity): Average number of dependency links sharing the same verbal head per sentence, excluding punctuation and copula dependencies.\nA.4 Global and Local Parsed Tree Structures Syntactic tree depth (parse_depth): Maximum syntactic tree depth extracted for the sentence, i.e., the longest path in terms of dependency links from the root of the dependency tree to some leaf.\nAverage and maximum length of dependency links (avg_links_len, max_links_len)\nNumber and average length of prepositional chains (n_prep_chains, prep_chain_len), with the latter expressed in number of tokens.\nSubject-object ordering (subj_pre, subj_post, obj_pre, obj_post): Relative order of the subject and object arguments with respect to the verbal root of the clause in the sentence.\nA.5 Syntactic Relations Distribution of dependency relations (dep_dist_*): Percentage distribution of the 37 universal relations in the UD dependency annotation scheme. A.6 Subordination Phenomena Distribution of main and subordinate clauses (princ_prop_dist, sub_prop_dist): Percentage distribution of main vs subordinate clauses in the sentence.\nRelative ordering of subordinates (sub_pre, sub_post): As for subjects and objects, whether the subordinate occurs in pre-verbal or post-verbal position in the sentence.\nAverage length of embedded subordinates (sub_chain_len): Average length of subordinate clauses recursively embedded into each other to form a subordinate chain.\nReaders are referred to the original paper by Brunato et al. (2020) and the Profiling-UD webpage24 for additional details on linguistic features.\nReferences Brunato, Dominique, Andrea Cimino, Felice Dell’Orletta, Giulia Venturi, and Simonetta Montemagni. 2020. “Profiling-UD: A Tool for Linguistic Profiling of Texts.” In Proceedings of the 12th Language Resources and Evaluation Conference, 7145–51. Marseille, France: European Language Resources Association. https://www.aclweb.org/anthology/2020.lrec-1.883.\nhttp://linguistic-profiling.italianlp.it↩\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"1391b0112f4256acd64fe2ffe9a0c6f2","permalink":"https://gsarti.com/msc-thesis/app-ling-feats/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/msc-thesis/app-ling-feats/","section":"msc-thesis","summary":"\u003c!DOCTYPE html\u003e A Linguistic Features | Interpreting Neural Language Models for Linguistic Complexity Assessment Introduction 1 Linguistic Complexity 1.1 Categorizing Linguistic Complexity Measures 1.2 Intrinsic Perspective 1.2.1 Structural Linguistic Complexity 1.2.2 Language Modeling Surprisal 1.3 Extrinsic Perspective 1.3.1 Automatic Readability Assessment 1.3.2 Perceived Complexity Prediction 1.3.3 Gaze Metrics Prediction 1.4 Garden-path Sentences 2 Models of Linguistic Complexity 2.1 Desiderata for Models of Linguistic Complexity 2.2 Neural Language Models: Unsupervised Multitask Learners 2.","tags":null,"title":"","type":"msc-thesis"},{"authors":null,"categories":null,"content":"\u003c!DOCTYPE html\u003e F Reproducibility and Environmental Impact | Interpreting Neural Language Models for Linguistic Complexity Assessment Introduction 1 Linguistic Complexity 1.1 Categorizing Linguistic Complexity Measures 1.2 Intrinsic Perspective 1.2.1 Structural Linguistic Complexity 1.2.2 Language Modeling Surprisal 1.3 Extrinsic Perspective 1.3.1 Automatic Readability Assessment 1.3.2 Perceived Complexity Prediction 1.3.3 Gaze Metrics Prediction 1.4 Garden-path Sentences 2 Models of Linguistic Complexity 2.1 Desiderata for Models of Linguistic Complexity 2.2 Neural Language Models: Unsupervised Multitask Learners 2.2.1 Emergent Linguistic Structures in Neural Language Models 2.3 Analyzing Neural Models of Complexity 2.3.1 Probing classifiers 2.3.2 Representational Similarity Analysis 2.3.3 Projection-Weighted Canonical Correlation Analysis 3 Complexity Phenomena in Linguistic Annotations and Language Models 3.1 Data and Preprocessing 3.2 Analysis of Linguistic Phenomena 3.2.1 Linguistic Phenomena in Length-controlled Bins 3.3 Modeling Online and Offline Linguistic Complexity 3.3.1 Modeling Complexity in Length-controlled Bins 3.4 Probing Linguistic Phenomena in ALBERT Representations 3.5 Summary 4 Representational Similarity in Models of Complexity 4.1 Knowledge-driven Requirements for Learning Models 4.2 Experimentsl Evaluation 4.2.1 Data 4.2.2 Inter-model Representational Similarity 4.2.3 Intra-model Representational Similarity 4.3 Summary 5 Gaze-informed Models for Cognitive Processing Prediction 5.1 Experimental Setup 5.2 Experimental Evaluation 5.2.1 Estimating Magnitudes of Garden-path Delays 5.2.2 Predicting Delays with Surprisal and Gaze Metrics 5.3 Summary Conclusion Broader Impact and Ethical Perspectives Future Directions Appendix A Linguistic Features A.1 Raw Text Properties and Lexical Variety A.2 Morpho-syntacting Information A.3 Verbal Predicate Structure A.4 Global and Local Parsed Tree Structures A.5 Syntactic Relations A.6 Subordination Phenomena B Precisions on Eye-tracking Metrics and Preprocessing C Multi-task Token-level Regression for Gaze Metrics Prediction D Intra-model Similarity for All Models E Gaze Metrics Predictions for Garden Path Sentences F Reproducibility and Environmental Impact References Back to my website Interpreting Neural Language Models\nfor Linguistic Complexity Assessment F Reproducibility and Environmental Impact Table F.1: Variable training parameters used in the experiments of this study. MTL stands for multitask learning. Chapter 3 Chapter 4 Chapter 5 PC ET Probes PC ET RA ALBERT GPT-2 fine-tuning standard MTL MTL standard MTL standard MTL MTL granularity sent. sent. sent. sent. word sent. word word freeze LM \\(w\\) ❌ ❌ ✅ ❌ ❌ ❌ ❌ ❌ weighted loss ✅ ❌ ❌ ❌ ❌ CV folds 5 5 5 early stopping ✅ ✅ ❌ ✅ ✅ ✅ ✅ ✅ training epochs 15 15 5 15 15 15 15 15 patience 5 5 5 5 5 5 5 evaluation steps 20 40 20 100 80 100 100 Tools Experiments were executed on a Ubuntu 18.04 LTS server, using a NVIDIA K40 GPU with 12GB RAM and CUDA 10.1. Relevant Python libraries used throughout the study with their respective versions are: 🤗 transformers 2.11.0 for accessing pre-trained Transformer language models, farm 0.4.5 for multitask learning, torch 1.3.0 as a backed for deep learning, and syntaxgym 0.5.3 for Chapter 5 experiments. Python 3.6.3 was used for all training scripts. A custom adaptation of the Oxforddown template was used for this thesis.25 Code for reproducibility purposes is available at the address https://github.com/gsarti/interpreting-complexity.\nModel Training Table F.1 present the set of variable training parameters used in all the experiments of this study. Besides those, a set of fixed parameters was also used: all experiments were performed using a batch size of 32 observations, a maximum sequence length of 128 tokens, a linear training schedule with one-tenth of total steps used as warmup steps, the AdamW optimizer (Loshchilov and Hutter 2019) with weight decay equal to \\(0.01\\), and a learning rate of \\(10^{-5}\\). No hyperparameter search was performed due to time limitations.\nTokenization All tokenizers used in the experiments used cased text and were based respectively on the SentencePiece approach (Kudo and Richardson 2018) for ALBERT and a custom version of Byte-Pair Encoding tokenization (Sennrich, Haddow, and Birch 2016) with token-like whitespaces for GPT-2. Default AlbertTokenizer and GPT2Tokenizer classes available in the 🤗 transformers library with pretrained tokenizers were used for this purpose. The vocabulary used by those had size 30’000 for ALBERT and 50’257 for GPT-2, including special tokens.\nArchitecture The default parameters for the 🤗 transformers checkpoints of ALBERT and GPT-2 (specifically, albert-base-v2 and gpt2 in the Model Hub) were used for this study. Concretely, this means embeddings and hidden sizes of 128 and 3072 for ALBERT and tied embedding-hidden size of 768 for GPT-2, 12 transformer blocks using 12 heads for multi-head self-attention each, and a smoothed variant of the Gaussian Error Linear Unit (GELU) as nonlinearity (Hendrycks and Gimpel 2016). GPT-2 has an embedding and attention dropout rate of 0.1 and a layer normalization (Ba, Kiros, and Hinton 2016) epsilon of \\(10^{-5}\\), while ALBERT employs a classifier dropout rate of 0.1 and a layer normalization epsilon of \\(10^{-12}\\).\nCO2 Emissions Related to Experiments Experiments were conducted using the private infrastructure of the ItaliaNLP Lab26 at the Institute for Computational Linguistics “A. Zampolli” (ILC-CNR) in Pisa, which has an estimated carbon efficiency of 0.321 kgCO\\(_2\\)eq/kWh (Moro and Lonza 2018). A cumulative of roughly 100 hours of computation was performed on a Tesla K40 GPU (TDP of 245W). Total emissions are estimated to be 7.86 kgCO\\(_2\\)eq. Estimations were conducted using the Machine Learning Impact Calculator27 presented in Lacoste et al. (2019).\nIn-detail reports of all experimental runsre produced automatically using the MLFlow28 tool and are available at the following address: https://public-mlflow.deepset.ai/#/experiments/99.\nReferences Ba, Jimmy, J. Kiros, and Geoffrey E. Hinton. 2016. “Layer Normalization.” ArXiv Pre-Print 1607.06450. https://arxiv.org/abs/1607.06450.\nHendrycks, Dan, and Kevin Gimpel. 2016. “Gaussian Error Linear Units (Gelus).” ArXiv Pre-Print 1606.08415. https://arxiv.org/abs/1606.08415.\nKudo, Taku, and John Richardson. 2018. “SentencePiece: A Simple and Language Independent Subword Tokenizer and Detokenizer for Neural Text Processing.” In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, 66–71. Brussels, Belgium: Association for Computational Linguistics. https://doi.org/10.18653/v1/D18-2012.\nLacoste, Alexandre, Alexandra Luccioni, Victor Schmidt, and Thomas Dandres. 2019. “Quantifying the Carbon Emissions of Machine Learning.” ArXiv Pre-Print 1910.09700.\nLoshchilov, I., and F. Hutter. 2019. “Decoupled Weight Decay Regularization.” In Proceeding of the 7th International Conference on Learning Representations (Iclr’19).\nMoro, Alberto, and Laura Lonza. 2018. “Electricity Carbon Intensity in European Member States: Impacts on Ghg Emissions of Electric Vehicles.” Transportation Research Part D: Transport and Environment 64. Elsevier: 5–14.\nSennrich, Rico, Barry Haddow, and Alexandra Birch. 2016. “Neural Machine Translation of Rare Words with Subword Units.” In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 1715–25. Berlin, Germany: Association for Computational Linguistics. https://doi.org/10.18653/v1/P16-1162.\nhttps://github.com/AI-Student-Society/thesisdown-it↩\nhttps://www.italianlp.it↩\nhttps://mlco2.github.io/impact#compute↩\nhttps://mlflow.org/↩\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"5da43832ddda5b755d00db6f8302b13b","permalink":"https://gsarti.com/msc-thesis/app-params/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/msc-thesis/app-params/","section":"msc-thesis","summary":"\u003c!DOCTYPE html\u003e F Reproducibility and Environmental Impact | Interpreting Neural Language Models for Linguistic Complexity Assessment Introduction 1 Linguistic Complexity 1.1 Categorizing Linguistic Complexity Measures 1.2 Intrinsic Perspective 1.2.1 Structural Linguistic Complexity 1.2.2 Language Modeling Surprisal 1.3 Extrinsic Perspective 1.3.1 Automatic Readability Assessment 1.3.2 Perceived Complexity Prediction 1.3.3 Gaze Metrics Prediction 1.4 Garden-path Sentences 2 Models of Linguistic Complexity 2.1 Desiderata for Models of Linguistic Complexity 2.2 Neural Language Models: Unsupervised Multitask Learners 2.","tags":null,"title":"","type":"msc-thesis"},{"authors":null,"categories":null,"content":"\u003c!DOCTYPE html\u003e 3 Complexity Phenomena in Linguistic Annotations and Language Models | Interpreting Neural Language Models for Linguistic Complexity Assessment Introduction 1 Linguistic Complexity 1.1 Categorizing Linguistic Complexity Measures 1.2 Intrinsic Perspective 1.2.1 Structural Linguistic Complexity 1.2.2 Language Modeling Surprisal 1.3 Extrinsic Perspective 1.3.1 Automatic Readability Assessment 1.3.2 Perceived Complexity Prediction 1.3.3 Gaze Metrics Prediction 1.4 Garden-path Sentences 2 Models of Linguistic Complexity 2.1 Desiderata for Models of Linguistic Complexity 2.2 Neural Language Models: Unsupervised Multitask Learners 2.2.1 Emergent Linguistic Structures in Neural Language Models 2.3 Analyzing Neural Models of Complexity 2.3.1 Probing classifiers 2.3.2 Representational Similarity Analysis 2.3.3 Projection-Weighted Canonical Correlation Analysis 3 Complexity Phenomena in Linguistic Annotations and Language Models 3.1 Data and Preprocessing 3.2 Analysis of Linguistic Phenomena 3.2.1 Linguistic Phenomena in Length-controlled Bins 3.3 Modeling Online and Offline Linguistic Complexity 3.3.1 Modeling Complexity in Length-controlled Bins 3.4 Probing Linguistic Phenomena in ALBERT Representations 3.5 Summary 4 Representational Similarity in Models of Complexity 4.1 Knowledge-driven Requirements for Learning Models 4.2 Experimentsl Evaluation 4.2.1 Data 4.2.2 Inter-model Representational Similarity 4.2.3 Intra-model Representational Similarity 4.3 Summary 5 Gaze-informed Models for Cognitive Processing Prediction 5.1 Experimental Setup 5.2 Experimental Evaluation 5.2.1 Estimating Magnitudes of Garden-path Delays 5.2.2 Predicting Delays with Surprisal and Gaze Metrics 5.3 Summary Conclusion Broader Impact and Ethical Perspectives Future Directions Appendix A Linguistic Features A.1 Raw Text Properties and Lexical Variety A.2 Morpho-syntacting Information A.3 Verbal Predicate Structure A.4 Global and Local Parsed Tree Structures A.5 Syntactic Relations A.6 Subordination Phenomena B Precisions on Eye-tracking Metrics and Preprocessing C Multi-task Token-level Regression for Gaze Metrics Prediction D Intra-model Similarity for All Models E Gaze Metrics Predictions for Garden Path Sentences F Reproducibility and Environmental Impact References Back to my website Interpreting Neural Language Models\nfor Linguistic Complexity Assessment 3 Complexity Phenomena in Linguistic Annotations and Language Models This chapter investigates the relationship between online gaze metrics and offline perceived complexity judgments by studying how the two viewpoints are represented by a neural language model trained on human-produced data. First, a preliminary analysis of linguistic phenomena associated with the two complexity viewpoints is performed, highlighting similarities and differences across metrics. The effectiveness of a regressor based on explicit linguistic features is then evaluated for sentence complexity prediction and compared to the results obtained by a fine-tuned neural language model with contextual representations. In conclusion, the linguistic competence inside the language model’s embeddings is probed before and after fine-tuning, showing how linguistic information encoded in representations changes as the model learns to predict complexity.\nGiven the conceptual similarity between raw cognitive processing and human perception of complexity, this chapter investigates whether the relation between eye-tracking metrics and complexity judgments can be highlighted empirically in human annotations and language model representations. With this aim, linguistic features associated with various sentence-level structural phenomena are analyzed in terms of their correlation with offline and online complexity metrics. The performance of models using either complexity-related explicit features or contextualized word embeddings is evaluated, focusing mainly on the neural language model ALBERT (Lan et al. 2020) introduced in Section 2.2. The results highlight how both explicit features and learned representations obtain comparable performances when predicting complexity scores. Finally, the focus is shifted to studying how complexity-related properties are encoded in the representations of ALBERT.\nThis perspective goes in the direction of exploiting human processing data to address the interpretability issues of unsupervised language representations (Hollenstein, Torre, et al. 2019; Gauthier and Levy 2019; Abnar et al. 2019), leveraging the probing task approach introduced in Section 2.3.1. It is observed that online and offline complexity fine-tuning produces a consequent increase in probing performances for complexity-related features during probing experiments. This investigation has the specific purpose of studying whether and how learning a new task affects the linguistic properties encoded in pretrained representations. While pre-trained models have been widely studied using probing methods, the effect of fine-tuning on encoded information was seldom investigated. To my best knowledge, no previous work has taken into account sentence complexity assessment as a fine-tuning task for NLMs. Results suggest that the model’s abilities during training are interpretable from a linguistic perspective and are possibly related to its predictive capabilities for complexity assessment.\nContributions This is the first work displaying the connection between online and offline complexity metrics and studying how a neural language model represents them. This work:\nProvides a comprehensive analysis of linguistic phenomena correlated with eye-tracking data and human perception of complexity, addressing similarities and differences from a linguistically-motivated perspective across metrics and at different levels of granularity;\nCompares the performance of models using both explicit features and unsupervised contextual representations when predicting online and offline sentence complexity; and\nShows the natural emergence of complexity-related linguistic phenomena in the representations of language models trained on complexity metrics.16\n3.1 Data and Preprocessing The experiments of this chapter leverage two corpora, each capturing different aspects of linguistic complexity:\nEye-tracking For online complexity metrics, only the monolingual English portion of GECO (Cop et al. 2017), presented in Section 1.3.3, was used. Four online metrics spanning multiple phases of cognitive processing are selected, respectively: first pass duration (FPD), total fixation count (FXC), total fixation duration (TFD) and total regression duration (TRD) (see Table 1.3 for more details). Metrics are sum-aggregated at sentence-level and averaged across participants to obtain a single label for each metric-sentence pair. As a final step to make the corpus more suitable for linguistic complexity analysis, all utterances with fewer than five words, deemed uninteresting from a cognitive processing perspective, are removed.\nPerceived Complexity For the offline evaluation of sentence complexity, the English portion of the corpus by Brunato et al. (2018) was used (Section 1.3.2). Sentences in the corpus have uniformly-distributed lengths ranging between 10 and 35 tokens. Each sentence is associated with 20 ratings of perceived-complexity on a 1-to-7 point scale. Duplicates and sentences for which less than half of the annotators agreed on a score in the range \\(\\mu_n \\pm \\sigma_n\\), where \\(\\mu_n\\) and \\(\\sigma_n\\) are respectively the average and standard deviation of all annotators’ judgments for sentence \\(n\\) were removed to reduce noise coming from the annotation procedure. Again, scores are averaged across annotators to obtain a single metric for each sentence.\nTable 3.1 presents an overview of the two corpora after preprocessing. The resulting eye-tracking (ET) corpus contains roughly four times more sentences than the perceived complexity (PC) one, with shorter words and sentences on average. The differences in sizes and domains between the two corpora account for multi-genre linguistic phenomena in the following analysis.\nTable 3.1: Descriptive statistics of the two sentence-level corpora after the preprocessing procedure. Perceived Complexity Eye-tracking (GECO) labels PC FPD, FXC, TFD, TRD domain(s) financial news literature aggregation steps avg. annotators sentence sum-aggregation + avg. participants filtering steps filtering by agreement + remove duplicates min. length \u0026gt; 5 # of sentences 1115 4041 # of tokens 21723 52131 avg. sent. length 19.48 12.9 avg. token length 4.95 4.6 Length-binned subsets (# of sentences) Bin 10±1 size 173 899 Bin 15±1 size 163 568 Bin 20±1 size 164 341 Bin 25±1 size 151 215 Bin 30±1 size 165 131 Bin 35±1 size 147 63 3.2 Analysis of Linguistic Phenomena As a first step to investigate the connection between the two complexity paradigms, the correlation of online and offline complexity labels with various linguistic phenomena is evaluated. The Profiling-UD tool (Brunato et al. 2020) introduced in Section 1.2.1 is used to annotate each sentence in our corpora and extract from it ~100 features representing their linguistic structure according to the Universal Dependencies formalism (Nivre et al. 2016). These features capture a comprehensive set of phenomena, from basic information (e.g. sentence and word length) to more complex aspects of sentence structure (e.g. parse tree depth, verb arity), including properties related to sentence complexity at different levels of description. A summary of the most relevant features is presented in Appendix A. Features are ranked using their Spearman’s correlation score with complexity metrics, and scores are leveraged to highlight the relation between linguistic phenomena and complexity paradigms.\nFigure 3.1: Ranking of the most correlated linguistic features for selected metrics. All of Spearman’s correlation coefficients have \\(p\u0026lt;0.001\\). The correlation scores analysis highlights how features showing a significant correlation with eye-tracking metrics are twice as many as those correlating with PC scores and generally tend to have higher coefficients, except for the total regression duration (TRD) metric. Nevertheless, the most correlated features are the same across all metrics. Figure 3.1 reports correlation scores for features showing a strong connection (\\(|\\rho|\u0026gt;0.3\\)) with at least one of the evaluated metrics. As expected, sentence length (n_tokens) and other related features capturing structural complexity aspects occupy the top positions in the ranking. Among those, we can note the length of dependency links (max_links_len, avg_links_len) and the depth of the whole parse tree or selected sub-trees, i.e. nominal chains headed by a preposition (parse_depth, n_prep_chains). Similarly, the distribution of subordinate clauses (sub_prop_dist, sub_post) is positively correlated with all metrics but with a more substantial effect for eye-tracking ones, especially in the presence of longer embedded chains (sub_chain_len). Interestingly, the presence of numbers (upos_NUM, dep_nummod) affects only the offline perception of complexity, while it is never strongly correlated with all eye-tracking metrics. This finding is expected since numbers are very short tokens and, like other functional POS, were never found to be strongly correlated with online reading in our results. Conversely, numerical information has been identified as a factor hampering sentence readability and understanding (Rello et al. 2013).\n3.2.1 Linguistic Phenomena in Length-controlled Bins Unsurprisingly, sentence length is the most correlated predictor for all complexity metrics. Since many linguistic features highlighted in our analysis are strongly related to sentence length, we tested whether they maintain a relevant influence when this parameter is controlled. To this end, Spearman’s correlation was computed between features and complexity tasks, but this time considering bins of sentences having approximately the same length. Specifically, we split each corpus into six bins of sentences with 10, 15, 20, 25, 30, and 35 tokens, respectively, with a range of ±1 tokens per bin to select a reasonable number of sentences for our analysis. Resulting subsets have a relatively constant size for the PC corpus, which was constructed ad-hoc to have such uniform length distribution, but have a sharply decreasing size for the eye-tracking corpus (see Table 3.1, bott. While deemed appropriate in the context of this correlation analysis, the disparity in bin sizes may play a significant role in hampering the performances of models trained on binned linguistic complexity data. This perspective is discussed in Section 3.3.\nFigure 3.2: Rankings of the most correlated linguistic features for metrics within length-binned subsets of the two corpora. Squares show the correlation between features (left axis) and a complexity metric (top) at a specific bin of length (bottom). Coefficients \\(\\geq\\) 0.2 or \\(\\leq\\) -0.2 are highlighted, and have \\(p\u0026lt;0.001\\). Figure 3.2 reports the new rankings of the most correlated linguistic features within each bin across complexity metrics (\\(|\\rho| \u0026gt; 0.2\\)). Again, we observe that features showing a significant correlation with complexity scores are fewer for PC bins than for eye-tracking ones. This fact depends on controlling for sentence length and the small size of bins for the whole dataset. As in the coarse-grained analysis, TRD is the eye-tracking metric less correlated to linguistic features, while the other three (FXC, FPD, TFD) show a homogeneous behavior across bins. For the latter, vocabulary-related features (token-type ratio, average word length, lexical density) are always positive and top-ranked in all bins, especially when considering shorter sentences (i.e. from 10 to 20 tokens). For PC, this is true only for some of them (word length and lexical density). On another note, features encoding numerical information are still highly correlated with the offline perception of complexity in almost all bins.\nInterestingly, features modeling subordination phenomena extracted from fixed-length sentences exhibit a reverse trend than when extracted from the whole corpus, i.e. they are negatively correlated with judgments. If, on the one hand, an increase in the presence of subordination for longer sentences (possibly making sentences more convoluted) was expected, on the other hand, when the length is controlled, findings suggest that subordinate structures are not necessarily perceived as a symptom of sentence complexity.\nThe analysis also highlights how linguistic features relevant to online and offline complexity are different when controlling for sentence length. This aspect, in particular, was not evident from the previous coarse-grained analysis. Despite blocking sentence length, gaze measures are still significantly connected to length-related phenomena (high correlation with n_tokens at various length bins). This observation can be possibly due to the ±1 margin applied for sentence selection and the high sensitivity of behavioral metrics to small input changes.\n3.3 Modeling Online and Offline Linguistic Complexity Given the high correlations reported above, the next step involves quantifying the importance of explicit linguistic features from a modeling standpoint. Table 3.2 presents the RMSE and \\(R^2\\) scores of predictions made by baselines and models for the selected complexity metrics. Performances are tested with a 5-fold cross-validation regression with a fixed random seed on each metric. Our baselines use average metric scores of all training sentences (Avg. score) and average scores of sentences binned by their length, expressed in number of tokens, as predictions (Bin average). The two linear SVM models leverage explicit linguistic features, using respectively only the n_tokens feature (SVM length) and the whole set of linguistic features presented above (SVM feats). Besides those, the performances of a state-of-the-art Transformer neural language model relying entirely on contextual word embeddings are equally tested. ALBERT (Lan et al. (2020); see Section 2.2) as a lightweight yet effective alternative to BERT (Devlin et al. 2019) for obtaining contextual word representations, using its last-layer [CLS] sentence embedding as input for a linear regressor during fine-tuning and testing. We selected the last layer representations, despite strong evidence on the importance of intermediate representation in encoding language properties, because we aim to investigate how superficial layers encode complexity-related competence. Given the availability of parallel eye-tracking annotations, we train ALBERT using multitask learning with hard parameter sharing (Caruana 1997) on gaze metrics.17\nTable 3.2: Average Root-Mean-Square Error (\\(\\sqrt{E^2}\\)) and \\(R^2\\) score values for sentence-level complexity predictions using 5-fold cross-validation. Lower \\(\\sqrt{E^2}\\) and higher \\(R^2\\) are better. PC FXC FPD TFD TRD \\(\\sqrt{E^2}\\) \\(R^2\\) \\(\\sqrt{E^2}\\) \\(R^2\\) \\(\\sqrt{E^2}\\) \\(R^2\\) \\(\\sqrt{E^2}\\) \\(R^2\\) \\(\\sqrt{E^2}\\) \\(R^2\\) Statistical baselines Avg. score 0.87 0 6.17 0.06 1078 0.06 1297 0.06 540 0.03 Bin average 0.53 0.62 2.36 0.86 374 0.89 532 0.85 403 0.45 Explicit features SVM length 0.54 0.62 2.19 0.88 343 0.9 494 0.86 405 0.45 SVM feats 0.44 0.74 1.77 0.92 287 0.93 435 0.92 400 0.46 Learned representations ALBERT 0.44 0.75 1.98 0.92 302 0.93 435 0.9 382 0.49 From Table 3.2 it can be noted that:\nThe length-binned average baseline is very effective in predicting complexity scores and gaze metrics, which is unsurprising given the extreme correlation between length and complexity metrics presented in Figure 3.1;\nThe SVM feats model shows considerable improvements if compared to the length-only SVM model for all complexity metrics, highlighting how length alone accounts for much but not for the entirety of variance in complexity scores;\nALBERT performs on-par with the SVM feats model on all complexity metrics despite the small dimension of the fine-tuning corpora and the absence of explicit linguistic information.\nA possible interpretation of ALBERT’s strong performances is that the model implicitly develops competence related to phenomena encoded by linguistic features while training on online and offline complexity prediction. We explore this perspective in Section 3.4.\n3.3.1 Modeling Complexity in Length-controlled Bins Figure 3.3: Average Root-Mean-Square Error (RMSE) scores for models in Table 3.2, performing 5-fold cross-validation on the length-binned subsets used for Figure 3.2. Lower scores are better. Similarly to the approach adopted in Section 3.2.1, the performances of models are tested on length-binned data to verify their consistency in the context of length-controlled sequences. Figure 3.3 presents RMSE scores averaged with 5-fold cross-validation over the length-binned sentences subsets for all complexity metrics. It can be observed that ALBERT outperforms the SVM with linguistic features on nearly all bins and metrics, showing the largest gains on intermediate bins for PC and gaze durations (FPD, TFD, TRD). Interestingly, models’ overall performances follow a length-dependent increasing trend for eye-tracking metrics, but not for PC. This behavior can be possibly explained in terms of the high sensibility to length previously highlighted for online metrics, as well as the broad variability in bin dimensions. It can also be observed how the SVM model based on explicit linguistic features (SVM feats) performs poorly on larger bins for all tasks, sometimes being even worse than the bin-average baseline. While this behavior seems surprising given the positive influence of features highlighted in Table 3.2, this phenomenon can be attributed to the small dimension of longer bins, which negatively impacts the generalization capabilities of the regressor. The relatively better scores achieved by ALBERT in those, instead, support the effectiveness of information stored in pretrained language representations when a limited number of examples are available.\n3.4 Probing Linguistic Phenomena in ALBERT Representations As shown in the previous section, ALBERT performances in complexity predictions are comparable to those of an SVM relying on explicit linguistic features and even better than those when controlling for length. The probing task interpretability paradigm (Section 2.3.1) is adopted to investigate if ALBERT encodes the linguistic knowledge that we identified as strongly correlated with online and perceived sentence complexity during training and prediction. In particular, the aim of this investigation is two-fold:\nProbing ALBERT’s innate competence in relation to the broad spectrum of linguistic features described in Appendix A; and\nVerifying whether, and in which respect, this competence is affected by a fine-tuning process on the complexity assessment metrics.\nThree UD English treebanks spanning different textual genres – EWT, GUM, and ParTUT respectively by Silveira et al. (2014), Zeldes (2017), and Sanguinetti and Bosco (2015) – were aggregated, obtaining a final corpus of 18,079 sentences with gold linguistic information which was used to conduct probing experiments. The Profiling-UD tool was again leveraged to extract \\(n\\) sentence-level linguistic features \\(\\mathcal{Z}=z_1, \\dots, z_n\\) from gold linguistic annotations. Representations \\(A(x)\\) were generated for all corpus sentences using the last-layer [CLS] embedding of a pretrained ALBERT base model without additional fine-tuning, and \\(n\\) single-layer perceptron regressors \\(g_i: A(x) \\rightarrow z_i\\) are trained to map representations \\(A(x)\\) to each linguistic feature \\(z_i\\). Finally, the error and \\(R^2\\) scores of each \\(g_i\\) were evaluated as proxies for the quality of representations \\(A(x)\\) in encoding their respective linguistic feature \\(z_i\\). The same evaluation is repeated for ALBERTs fine-tuned respectively on perceived complexity labels (PC) and on all eye-tracking labels with multitask learning (ET), averaging scores with 5-fold cross-validation. A selected subset of results is shown on the left side of Table 3.3.\nTable 3.3: Root MSE (\\(\\sqrt{E^2}\\)) and \\(R^2\\) scores for diagnostic regressors trained on ALBERT representations, respectively, without fine-tuning (Base), with PC and eye-tracking (ET) fine-tuning on all data (left) and on the \\(10 \\pm 1\\) length-binned subset (right). values highlight relevant increases in \\(R^2\\) from Base. Base PC ET PC10±1 ET10±1 \\(\\sqrt{E^2}\\) \\(R^2\\) \\(\\sqrt{E^2}\\) \\(R^2\\) \\(\\sqrt{E^2}\\) \\(R^2\\) \\(\\sqrt{E^2}\\) \\(R^2\\) \\(\\sqrt{E^2}\\) \\(R^2\\) n_tokens 8.19 0.26 4.66 0.76 2.87 0.91 8.66 0.18 6.71 0.51 parse_depth 1.47 0.18 1.18 0.48 1.04 0.6 1.50 0.16 1.22 0.43 vb_head_per_sent 1.38 0.15 1.26 0.3 1.14 0.42 1.44 0.09 1.30 0.25 xpos_dist_. 0.05 0.13 0.04 0.41 0.04 0.42 0.04 0.18 0.04 0.38 avg_links_len 0.58 0.12 0.53 0.29 0.52 0.31 0.59 0.1 0.56 0.2 max_links_len 5.20 0.12 4.08 0.46 3.75 0.54 5.24 0.11 4.73 0.28 n_prep_chains 0.74 0.11 0.67 0.26 0.66 0.29 0.72 0.14 0.69 0.21 sub_prop_dist 0.35 0.09 0.33 0.13 0.31 0.22 0.34 0.05 0.32 0.15 upos_dist_PRON 0.08 0.09 0.08 0.14 0.08 0.07 0.07 0.23 0.08 0.15 pos_dist_NUM 0.05 0.08 0.05 0.06 0.05 0.02 0.05 0.16 0.05 0.06 dep_dist_nsubj 0.06 0.08 0.06 0.1 0.06 0.05 0.05 0.17 0.06 0.11 char_per_tok 0.89 0.07 0.87 0.12 0.90 0.05 0.82 0.22 0.86 0.14 prep_chain_len 0.60 0.07 0.57 0.17 0.56 0.19 0.59 0.12 0.56 0.18 sub_chain_len 0.70 0.07 0.67 0.15 0.62 0.26 0.71 0.04 0.66 0.16 dep_dist_punct 0.07 0.06 0.07 0.06 0.07 0.14 0.07 0.06 0.07 0.14 dep_dist_nmod 0.05 0.06 0.05 0.07 0.05 0.06 0.05 0.09 0.05 0.09 sub_post 0.44 0.05 0.46 0.12 0.44 0.18 0.47 0.05 0.45 0.14 dep_dist_case 0.07 0.05 0.06 0.06 0.07 0.08 0.07 0.07 0.07 0.1 lexical_density 0.14 0.05 0.13 0.03 0.13 0.03 0.13 0.13 0.13 0.13 dep_dist_compound 0.06 0.04 0.06 0.05 0.06 0.03 0.06 0.1 0.06 0.07 dep_dist_conj 0.04 0.03 0.04 0.04 0.04 0.04 0.05 0.02 0.04 0.03 ttr_form 0.08 0.03 0.08 0.05 0.08 0.05 0.08 0.05 0.08 0.05 dep_dist_det 0.06 0.03 0.06 0.02 0.06 0.04 0.06 0.03 0.06 0.03 dep_dist_aux 0.04 0.02 0.04 0.01 0.04 0.01 0.04 0.06 0.04 0.04 pos_dist_VBN 0.03 0.01 0.03 0 0.03 0 0.03 0.01 0.03 0 xpos_dist_VBZ 0.04 0.01 0.04 0.01 0.04 0.02 0.04 0.02 0.04 0.02 ttr_lemma 0.09 0.01 0.09 0.06 0.09 0.06 0.09 0.04 0.09 0.03 As it can be observed, ALBERT’s last-layer sentence representations have relatively low knowledge of complexity-related probes, but their performances highly increase after fine-tuning. Specifically, a noticeable improvement was obtained on features that were already better encoded in base pretrained representation, i.e. sentence length and related, suggesting that fine-tuning possibly accentuates only properties already well-known by the model, regardless of the target task. To verify that this isn’t the case, the same probing tests were repeated on ALBERT models fine-tuned on the smallest length-binned subset (i.e. \\(10\\pm1\\) tokens) presented in previous sections. The right side of Table 3.3 presents the resulting scores. From the length-binned correlation analysis of Section 3.2, PC scores were observed to be mostly uncorrelated with length phenomena, while ET scores remain significantly affected despite our controlling of sequence size. This observation also holds for length-binned probing task results, where the PC model seems to neglect length-related properties in favor of task-specific ones that were also highlighted in our fine-grained correlation analysis (e.g. word length, numbers, explicit subjects). The ET-trained model follows the same behavior, retaining strong but lower performances for length-related features.\nIn conclusion, although higher probing task performances after fine-tuning are not direct proof that the neural language model exploits newly-acquired morpho-syntactic and syntactic information, results suggest that training on tasks strongly connected with underlying linguistic structures triggers a change in model representations resulting in a better encoding of related linguistic properties.\n3.5 Summary In this chapter, the connection between eye-tracking metrics and the offline perception of sentence complexity was investigated from an experimental standpoint. An in-depth correlation analysis was performed between complexity scores and sentence linguistic properties at different granularity levels, highlighting the strong relationship between metrics and length-affine properties and revealing different behaviors when controlling for sentence length. Models using explicit linguistic features and unsupervised word embeddings were evaluated on complexity prediction, showing comparable performances across metrics. Finally, the encoding of linguistic properties in a neural language model’s contextual representations was tested with probing tasks. This approach highlighted the natural emergence of task-related linguistic properties within the model’s representations after the fine-tuning process. Thus, it can be conjectured that a relation subsists between the model’s linguistic abilities during the training procedure and its downstream performances on morphosyntactically-related tasks and that linguistic probes may provide a reasonable estimate of the task-oriented quality of representations.\nReferences Abnar, Samira, Lisa Beinborn, Rochelle Choenni, and Willem Zuidema. 2019. “Blackbox Meets Blackbox: Representational Similarity \u0026amp; Stability Analysis of Neural Language Models and Brains.” In Proceedings of the 2019 Acl Workshop Blackboxnlp: Analyzing and Interpreting Neural Networks for Nlp, 191–203. Florence, Italy: Association for Computational Linguistics. https://doi.org/10.18653/v1/W19-4820.\nBrunato, Dominique, Andrea Cimino, Felice Dell’Orletta, Giulia Venturi, and Simonetta Montemagni. 2020. “Profiling-UD: A Tool for Linguistic Profiling of Texts.” In Proceedings of the 12th Language Resources and Evaluation Conference, 7145–51. Marseille, France: European Language Resources Association. https://www.aclweb.org/anthology/2020.lrec-1.883.\nBrunato, Dominique, Lorenzo De Mattei, Felice Dell’Orletta, Benedetta Iavarone, and Giulia Venturi. 2018. “Is This Sentence Difficult? Do You Agree?” In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, 2690–9. Brussels, Belgium: Association for Computational Linguistics. https://doi.org/10.18653/v1/D18-1289.\nCaruana, Rich. 1997. “Multitask Learning.” Machine Learning 28: 41–75. https://www.cs.utexas.edu/~kuipers/readings/Caruana-mlj-97.pdf.\nCop, Uschi, Nicolas Dirix, Denis Drieghe, and Wouter Duyck. 2017. “Presenting Geco: An Eyetracking Corpus of Monolingual and Bilingual Sentence Reading.” Behavior Research Methods 49 (2). Springer: 602–15.\nDevlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. “BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding.” In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), 4171–86. Minneapolis, Minnesota: Association for Computational Linguistics. https://doi.org/10.18653/v1/N19-1423.\nGauthier, Jon, and Roger Levy. 2019. “Linking Artificial and Human Neural Representations of Language.” In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (Emnlp-Ijcnlp), 529–39. Hong Kong, China: Association for Computational Linguistics. https://doi.org/10.18653/v1/D19-1050.\nHollenstein, Nora, Antonio de la Torre, Nicolas Langer, and Ce Zhang. 2019. “CogniVal: A Framework for Cognitive Word Embedding Evaluation.” In Proceedings of the 23rd Conference on Computational Natural Language Learning (Conll), 538–49. Hong Kong, China: Association for Computational Linguistics. https://doi.org/10.18653/v1/K19-1050.\nLan, Zhenzhong, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. 2020. “ALBERT: A Lite BERT for Self-Supervised Learning of Language Representations.” In International Conference on Learning Representations. https://openreview.net/forum?id=H1eA7AEtvS.\nNivre, Joakim, Marie-Catherine de Marneffe, Filip Ginter, Yoav Goldberg, Jan Hajič, Christopher D. Manning, Ryan McDonald, et al. 2016. “Universal Dependencies V1: A Multilingual Treebank Collection.” In Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC’16), 1659–66. Portorož, Slovenia: European Language Resources Association (ELRA). https://www.aclweb.org/anthology/L16-1262.\nRello, Luz, Susana Bautista, Ricardo Baeza-Yates, Pablo Gervás, Raquel Hervás, and Horacio Saggion. 2013. “One Half or 50%? An Eye-Tracking Study of Number Representation Readability.” In Human-Computer Interaction – Interact 2013, edited by Paula Kotzé, Gary Marsden, Gitte Lindgaard, Janet Wesson, and Marco Winckler, 229–45. Berlin, Heidelberg: Springer Berlin Heidelberg.\nSanguinetti, Manuela, and Cristina Bosco. 2015. “PartTUT: The Turin University Parallel Treebank.” In Harmonization and Development of Resources and Tools for Italian Natural Language Processing Within the Parli Project, edited by Roberto Basili, Cristina Bosco, Rodolfo Delmonte, Alessandro Moschitti, and Maria Simi, 51–69. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-319-14206-7\\_3.\nSilveira, Natalia, Timothy Dozat, Marie-Catherine de Marneffe, Samuel Bowman, Miriam Connor, John Bauer, and Chris Manning. 2014. “A Gold Standard Dependency Corpus for English.” In Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC’14), 2897–2904. Reykjavik, Iceland: European Language Resources Association (ELRA). http://www.lrec-conf.org/proceedings/lrec2014/pdf/1089_Paper.pdf.\nZeldes, Amir. 2017. “The GUM Corpus: Creating Multilayer Resources in the Classroom.” Language Resources and Evaluation 51: 581–612.\nCode available at https://github.com/gsarti/interpreting-complexity↩\nTraining procedure and parameters are thoroughly described in Appendix F.↩\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"6c8661af20e8ca94ed8d3d19164e5fc9","permalink":"https://gsarti.com/msc-thesis/chap-ex1/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/msc-thesis/chap-ex1/","section":"msc-thesis","summary":"\u003c!DOCTYPE html\u003e 3 Complexity Phenomena in Linguistic Annotations and Language Models | Interpreting Neural Language Models for Linguistic Complexity Assessment Introduction 1 Linguistic Complexity 1.1 Categorizing Linguistic Complexity Measures 1.2 Intrinsic Perspective 1.2.1 Structural Linguistic Complexity 1.2.2 Language Modeling Surprisal 1.3 Extrinsic Perspective 1.3.1 Automatic Readability Assessment 1.3.2 Perceived Complexity Prediction 1.3.3 Gaze Metrics Prediction 1.4 Garden-path Sentences 2 Models of Linguistic Complexity 2.1 Desiderata for Models of Linguistic Complexity 2.","tags":null,"title":"","type":"msc-thesis"},{"authors":null,"categories":null,"content":"\u003c!DOCTYPE html\u003e 4 Representational Similarity in Models of Complexity | Interpreting Neural Language Models for Linguistic Complexity Assessment Introduction 1 Linguistic Complexity 1.1 Categorizing Linguistic Complexity Measures 1.2 Intrinsic Perspective 1.2.1 Structural Linguistic Complexity 1.2.2 Language Modeling Surprisal 1.3 Extrinsic Perspective 1.3.1 Automatic Readability Assessment 1.3.2 Perceived Complexity Prediction 1.3.3 Gaze Metrics Prediction 1.4 Garden-path Sentences 2 Models of Linguistic Complexity 2.1 Desiderata for Models of Linguistic Complexity 2.2 Neural Language Models: Unsupervised Multitask Learners 2.2.1 Emergent Linguistic Structures in Neural Language Models 2.3 Analyzing Neural Models of Complexity 2.3.1 Probing classifiers 2.3.2 Representational Similarity Analysis 2.3.3 Projection-Weighted Canonical Correlation Analysis 3 Complexity Phenomena in Linguistic Annotations and Language Models 3.1 Data and Preprocessing 3.2 Analysis of Linguistic Phenomena 3.2.1 Linguistic Phenomena in Length-controlled Bins 3.3 Modeling Online and Offline Linguistic Complexity 3.3.1 Modeling Complexity in Length-controlled Bins 3.4 Probing Linguistic Phenomena in ALBERT Representations 3.5 Summary 4 Representational Similarity in Models of Complexity 4.1 Knowledge-driven Requirements for Learning Models 4.2 Experimentsl Evaluation 4.2.1 Data 4.2.2 Inter-model Representational Similarity 4.2.3 Intra-model Representational Similarity 4.3 Summary 5 Gaze-informed Models for Cognitive Processing Prediction 5.1 Experimental Setup 5.2 Experimental Evaluation 5.2.1 Estimating Magnitudes of Garden-path Delays 5.2.2 Predicting Delays with Surprisal and Gaze Metrics 5.3 Summary Conclusion Broader Impact and Ethical Perspectives Future Directions Appendix A Linguistic Features A.1 Raw Text Properties and Lexical Variety A.2 Morpho-syntacting Information A.3 Verbal Predicate Structure A.4 Global and Local Parsed Tree Structures A.5 Syntactic Relations A.6 Subordination Phenomena B Precisions on Eye-tracking Metrics and Preprocessing C Multi-task Token-level Regression for Gaze Metrics Prediction D Intra-model Similarity for All Models E Gaze Metrics Predictions for Garden Path Sentences F Reproducibility and Environmental Impact References Back to my website Interpreting Neural Language Models\nfor Linguistic Complexity Assessment 4 Representational Similarity in Models of Complexity The experiments of this chapter aim to shed light on how the linguistic knowledge encoded in the contextual representations of complexity-trained neural language models varies across layers of abstraction and fine-tuning tasks. Two similarity approaches, Representational Similarity Analysis (RSA) and Projection-Weighted Canonical Correlation Analysis (PWCCA) are used to evaluate the relation subsisting between representations spanning different models and different layers of the same model. The outcomes are finally compared against a set of assumptions aimed at determining a model’s generalization capabilities across language phenomena. Results provide empirical evidence about the inability of state-of-the-art language modeling approaches to effectively represent an abstract hierarchy of linguistic complexity phenomena.\nChapter 3 highlighted how the relation between online and offline complexity perspectives and linguistic phenomena diverge when considering same-length sentences and how those properties of language are adequately captured by a neural language model fine-tuned on complexity metrics. This chapter adopts a complementary perspective on the model-driven study of complexity. Instead of connecting learned representations to the input’s structural properties, it explores how those representations change when the same model is exposed to different training objectives using similarity measures. This approach is used to gain insights on the underlying similarities across complexity metrics, using representations as proxies for the knowledge needed to correctly model various complexity phenomena under a minimal set of assumptions.\nThe same ALBERT (Lan et al. 2020) model introduced in Section 2.2 and used for the last section’s probing task experiments is leveraged for this chapter’s experiments.18 The model is first taken as-is in its pre-trained version without fine-tuning (referred to as Base). Then, three instances of it are fine-tuned respectively on Automatic Readability Assessment (RA, Section 1.3.1), Perceived Complexity Prediction (PC, Section 1.3.2) and Eye-tracking Metrics Prediction (ET, Section 1.3.3) until convergence. The four models are evaluated in two settings: first, by comparing the similarity of same-layer representation across models (inter-model similarity), and then comparing the similarity across different layers of the same model (intra-model similarity). For each setting, two similarity metrics are used: Representational Similarity Analysis (RSA, Section 2.3.2) and Projection-Weighted Canonical Correlation Analysis (PWCCA, Section 2.3.3). RSA and PWCCA were selected since they provide different perspectives over the similarity of representations: if, on the one hand, RSA naively evaluates the similarity across input representations through correlation, PWCCA factors in the importance of sparsity patterns that characterize overparametrized neural networks using a projection operation. Both token and sentence-level representations are evaluated to obtain a fine-grained overview of representational similarity.\nThe models trained on perceived complexity and eye-tracking metrics are again the main subjects of this study, given the logical and empirical relation subsisting between the two complexity perspectives highlighted in previous chapters. The additional use of Base and readability-trained models allows us to verify whether ALBERT representations satisfy a minimal set of assumptions deemed necessary and sufficient for modeling an abstraction hierarchy of linguistic complexity phenomena in an interpretable fashion. Results produced by representational similarity experiments diverge significantly from the initial hypothesis, suggesting the prominence of surface structures and task setups over underlying general knowledge about the nature of the modeled phenomena in shaping representations during the training process.\nContributions While multiple works aimed at inspecting NLM representations by mean of similarity approaches already exist, this is the first work to the best of my knowledge that does so with the explicit purpose of evaluating the impact of linguistic complexity training. This work:\nHighlights similarity and differences in the representations of models trained on different complexity-related tasks to understand how neural network parameters capture different perspectives over linguistic complexity after the training process;\nPresents similarity and differences in the representations found at different layers of the same model to understand how knowledge is distributed hierarchically at various abstraction levels after training;\nProvide evidence about the inability of state-of-the-art NLP approaches to learning to effectively represent an abstract hierarchy of linguistic complexity phenomena in an unsupervised manner, relying solely on complexity-related annotations.19\n4.1 Knowledge-driven Requirements for Learning Models At the beginning of Chapter 2 two prerequisites to any model-driven study were defined: that available annotated corpora should be informative about the underlying phenomena we are trying to model, and that sufficiently elaborate models should be able to represent knowledge to solve phenomena-related tasks after being trained on those corpora effectively. This section formalizes the two assumptions and builds upon them to define a set of fundamental requirements that should be satisfied by models capable of generalizing over unseen linguistic structures after undergoing a learning process. Let:\n\\(\\mathcal{C}^\\phi_\\alpha = \\Big [ (x_1,\\alpha_1)\\dots(x_m,\\alpha_m)\\Big]\\) be an annotated corpus containing some knowledge relative to an abstract phenomenon of interest \\(\\phi\\) encoded in its annotations \\(\\alpha\\). \\(x\\) can represent any \\(i\\)-th linguistic structure or substructure (sentence, word, morpheme). This notation can be generalized to settings where annotations are not explicitly defined (e.g. in the context of language modeling, next structure \\(x_i+1\\) acts as an annotation for \\(x_i\\)) or when multiple annotations are present (e.g. if \\(\\mathcal{C}\\) has two sets of annotations \\(\\alpha, \\beta\\) modeling the same phenomenon \\(\\mathcal{K}\\) is equivalent to two corpora \\(\\mathcal{C}^\\phi_\\alpha, \\mathcal{C}^\\phi_\\beta\\) with shared \\(x\\)’s).\n\\(M\\) be a model that, after being trained on \\(\\mathcal{C}^{\\phi}_\\alpha\\), learns representations (i.e. parameters) that allow him to map correctly linguistic structures to annotations\n\\(\\mathcal{K}^\\phi\\) be a set containing all empirical knowledge that is specifically relevant to phenomenon \\(\\phi\\). \\(\\mathcal{K}^\\phi_\\alpha\\) represents all knowledge relative to \\(\\phi\\) contained in a corpus \\(\\mathcal{C}^\\phi_\\alpha\\). Concretely, given a corpus \\(\\mathcal{C}^\\phi_\\alpha\\), we can logically infer from it some estimate knowledge \\(\\tilde{\\mathcal{K}}^\\phi_\\alpha\\) such that \\(\\tilde{\\mathcal{K}}^\\phi_\\alpha \\simeq \\mathcal{K}^\\phi_\\alpha \\subset \\mathcal{K}^\\phi\\).\n\\(\\varsigma_{\\alpha, \\beta}^{\\phi}(x)\\) be an idealized similarity function reflecting the similarity between two sets of representations in performance-driven terms relative to phenomenon \\(\\phi\\), i.e. measuring their invariance in relation to all knowledge sets \\(\\mathcal{K}^\\varphi\\), with \\(\\phi \\neq \\varphi\\) that are irrelevant to phenomenon \\(\\phi\\).\nFor example, taking linguistic complexity as \\(\\phi\\), and the GECO corpus as \\(\\mathcal{C}^\\phi_\\alpha\\) (with \\(\\alpha\\) being e.g. the total fixation duration annotations), we may have \\(\\tilde{\\mathcal{K}}^\\phi_\\alpha\\) (i.e. our inferred knowledge about linguistic complexity) contains the observation \\(o =\\) “longer structures are more complex” because longer words have longer total fixation durations on average. Note that the relation \\(o \\in \\mathcal{K}^\\phi_\\alpha\\) can only be hypothesized whenever a corpus with different annotations \\(\\mathcal{C}^\\phi_\\beta\\) pertinent to the same phenomenon allows us to infer a \\(\\tilde{\\mathcal{K}}^\\phi_\\beta\\) such that \\(o \\in \\tilde{\\mathcal{K}}^\\phi_\\alpha \\cap \\tilde{\\mathcal{K}}^\\phi_\\beta\\) (e.g. longer sentences are also deemed more complex on average in the perceived complexity corpus, so length is probably related to complexity in general).\nChapter 2 assumptions can now be summarized in a single statement:\nAssumption 4.1 (Learning-driven encodability) A learning process that trains a model \\(M\\) on a corpus \\(\\mathcal{C}^\\phi_\\alpha\\) up to a reasonable accuracy is equivalent to an encoding function that maps \\(\\phi\\)-relevant knowledge contained in \\(\\mathcal{C}^\\phi_\\alpha\\) to \\(M\\)’s learned representations.\nIf Assumption 4.1 is verified, then annotations must be informative, and the model must be able to encode all knowledge present in the corpora relevant to the phenomena. On top of that foundational assumption, three further requirements that are sufficient and necessary for building interpretable learning models able to represent knowledge in a generalizable manner are defined:\nAssumption 4.2 (Knowledge-similarity interrelation) Given two corpora \\(\\mathcal{C}^\\phi_\\alpha, \\mathcal{C}^\\phi_\\beta\\) providing different and possibly complementary knowledge about the same phenomenon \\(\\phi\\) and representations \\(R^{M}_{\\alpha}, R^{M}_{\\beta}\\) learned by a model \\(M\\) trained respectively on the two corpora, the more those representations are similar in relation to \\(\\phi\\), the more \\(\\phi\\)-related shared knowledge is contained in the two corpora. When the two representations are perfectly \\(\\phi\\)-similar, the two corpora share the same \\(\\phi\\)-related knowledge.\nAssumption 4.3 (Pertinence-based preponderance) The amount of knowledge \\(\\mathcal{K}^\\phi_\\alpha\\) related to phenomenon \\(\\phi\\) contained in a corpus \\(\\mathcal{C}^{\\phi}_\\alpha\\) that explicitly encodes some knowledge about \\(\\phi\\) is always larger than the amount of knowledge relative to \\(\\phi\\) contained in any corpus \\(\\mathcal{C}^{\\phi\u0026#39;}_\\beta\\) which explicitly covers a different phenomenon \\(\\phi\u0026#39;\\) by means of its annotations \\(\\beta\\).\nAssumption 4.4 (Knowledge-similarity transitivity) Given three corpora \\(\\mathcal{C}^\\phi_\\alpha, \\mathcal{C}^\\phi_\\beta, \\mathcal{C}^\\phi_\\gamma\\) providing different views over the same phenomenon \\(\\phi\\) and representations \\(R^{M}_{\\alpha}, R^{M}_{\\beta}, R^{M}_{\\gamma}\\) learned by a model \\(M\\) trained on each one of them respectively, if a pair of those representations has higher \\(\\phi\\)-similarity than another, then the respective pair of corpora also have a larger amount of shared \\(\\phi\\)-related knowledge and vice versa.\nThe experimental section of this chapter is aimed at testing whether those requirements are satisfied by ALBERT. Assumption 4.2 enables us to use representational similarity measures to evaluate our corpora’s latent knowledge related to linguistic complexity. In particular, RSA and PWCCA will be used respectively as naive and more advanced approximations of \\(\\varsigma\\) that evaluate representations’ distance in the \\(n\\)-dimensional space across multiple linguistic structures.\nThe first step in this verification process involves comparing representations learned by ALBERT models trained on PC, ET, and RA against those of Base. Since the base model was exposed to a general MLM pre-training, without having access to any complexity-related annotation, it can be hypothesized that the three complexity-trained models had access to more complexity-related information during training (Assumptions 4.1 and 4.3), and thus learned representations that are closer together in similarity terms than those of Base (Assumption 4.2). The other perspective involves evaluating how different views related to the same phenomenon are captured. While perceived complexity annotations and gaze metrics are at the antipodes of the processing spectrum (see Figure 1.1), they should logically contain more complexity-related shared information than readability categories since they are both related to the reader’s viewpoint, while RA captures the writer’s perspective. If Assumption 4.4 is verified, then it can be hypothesized that ALBERT-PC and ALBERT-ET learned representations closer together in similarity terms than those of the ALBERT-RA model.\nBefore moving to the experiments, two crucial aspects should be highlighted. First, corpus size was abstracted away from the verification process despite being commonly known to be an essential factor in shaping neural network training effectiveness. In particular, we should be aware that the size imbalance across available corpora can be a significant source of error in the evaluation process. Secondly, sentence-level training objectives are used for PC and RA tasks, while ALBERT-ET is trained on token-level annotations.20 If, on the one hand, this difference in training approaches can act as an additional confounder when evaluating requirements, from another perspective, it can provide us with some information relative to the generalization abilities of ALBERT beyond task setup.\n4.2 Experimentsl Evaluation This section describes the similarity experiments that have been carried out over model representations across multiple training setups. First, Section 4.2.1 presents the data used to train ALBERT models and evaluate their representational similarity. Then, Section 4.2.2 focuses on validating the assumptions formulated at the beginning of this chapter by evaluating the intra-model similarity across all model pairs. Finally, Section 4.2.3 employs the same similarity approach in an intra-model setting, providing us with some evidence on how linguistic knowledge is encoded hierarchically across ALBERT layers during the training process.\n4.2.1 Data The experiments of this chapter leverage all corpora that were presented in Sections 1.3.1, 1.3.2 and 1.3.3 for fine-tuning the three complexity models whose representations were compared against each other and the Base pre-trained ALBERT. Specifically:\nReadability Assessment The OneStopEnglish corpus (Vajjala and Lučić 2018) is leveraged by splitting each document into sentences and labeling those with the original reading level. A total of 7190 sentences equally distributed across the Elementary, Intermediate, and Advanced levels are used to fine-tune ALBERT-RA in a multiclass classification setting.\nPerceived Complexity The English portion of the corpus by Brunato et al. (2018) was again used to fine-tune ALBERT-PC, following the same preprocessing steps detailed in Section 3.1 of the previous chapter.\nEye-tracking The GECO (Cop et al. 2017), Dundee (Kennedy, Hill, and Pynte 2003), ZuCo (Hollenstein et al. 2018) and ZuCo 2.0 (Hollenstein, Troendle, et al. 2020) corpora were merged (Total column of Table 1.4) and used to train the ALBERT-ET model. As opposed to the previous section’s sentence-level approach, ALBERT-ET is trained to predict gaze metrics at token-level to obtain a fine-grained perspective over the input’s complexity and fully exploit the information available through gaze recordings.21\nEvaluation All models are evaluated by measuring the similarity of their representations of the Stanford Sentiment Treebank (SST, Socher et al. (2013)). The version of the treebank leveraged for this study contained 11,855 sentences and was selected because the movie review genre is different from all textual genres encompassed by the available corpora (except ZuCo, which represent only a small fraction of the whole set of eye-tracking data used). Sentiment annotations were removed, and only sentences were considered.\n4.2.2 Inter-model Representational Similarity Figure 4.1: Inter-model RSA scores across layers for all ALBERT models’ combinations, using the [CLS] token (top-left), the all-token average (top-right), and all tokens (bottom) representations. Layer -1 corresponds to the last layer before prediction heads. Higher scores denote stronger inter-model similarity. The inter-model similarity is evaluated by comparing layer-wise representations of models trained on different tasks using the same ALBERT architecture. Given the representations produced by two ALBERT models trained on different complexity-related annotations for all the sentences in the SST corpus, their similarity is evaluated using both RSA and PWCCA in three settings:\n[CLS] token: Only the sentence-level [CLS] initial embedding is considered when evaluating similarity at each layer for all sentences in the SST corpus.\nTokens’ average: A sentence-level embedding obtained by averaging all the individual subword embeddings produced by ALBERT is considered when evaluating similarity at each layer for all sentences in the SST corpus.\nAll tokens: The subword embeddings produced by ALBERT for all SST sentences are considered when evaluating similarity at each layer, including [CLS], [SEP] and regular token embeddings, for all sentences in the SST corpus. In practice, the number of considered embedding was set to a maximum of 50,000 to limit such an approach’s computational costs.\nFigure 4.1 presents inter-model RSA scores for all model combinations and layers, going from the input layer after initial embeddings (-12) to the last layer before prediction heads (-1).\nGiven the RSA similarity metric has range \\([0,1]\\), it can be observed that representational similarity varies greatly across layers, ranging from very high (\\(\\sim 0.9\\)) across bottom layers of the models to very low (\\(\u0026lt; 0.1\\)) for top layers. This observation supports the widely accepted claim that layers closer to the input in NLMs are almost unaffected by task-specific fine-tuning since they encode low-level properties, while layers closer to prediction heads represent task-related abstract knowledge and tend to diverge rapidly during training.\nIn settings involving the PC-trained model (yellow, red, and green lines in Figure 4.1) no sharp decrease in similarity is observed across the top layer for all three variations. Conversely, spikes of decreasing similarity are observed for top layers of all other model pairs. While in terms of [CLS] all models behave comparably, there is a marked dissimilarity between PC and ET-trained models for top layers when considering all token representations, both with and without averaging (green line in Figures 4.1 a,b). Conversely, RA’s [CLS] representations behave similarly to the ones of other models, but token representations stay very similar to Base even for top layers, i.e. are slightly affected by fine-tuning (purple line in Figures 4.1 b,c). It can be hypothesized that the RA-trained model cannot collect relevant token-level information since it misses the relative perspective that, as saw in Section 1.3.1, plays a key role for readability assessment. In this case, PC and ET-trained models are the only ones building relevant complexity-related knowledge, but they still tend to diverge in terms of representational similarity.\nFigure 4.2: Inter-model PWCCA distances across layers for all ALBERT models’ combinations, using the [CLS] token (top-left), the all-token average (top-right), and all tokens (bottom) representations. Layer -1 corresponds to the last layer before prediction heads. Higher values denote weaker inter-model similarity. Figure 4.2 presents PWCCA scores in the exact same setup as Figure 4.1. It does not come as a surprise that scores, in this case, tend to increase while moving towards prediction heads since the PWCCA distance on the \\(y\\)-axis represents here a function of representational dissimilarity between different layers. Besides this difference, a sharp contrast in behavior is observed in relation to RSA scores, with generally smaller value ranges (\\(\\sim 0.0\\) to \\(0.4\\)).\nIn terms of [CLS] representations, (PC, Base) and (RA, Base) are the two closest pairs, while (PC, ET) and (RA, ET) are furthest. This relation can be rationalized if considering that PC and RA-trained models are trained using the [CLS] token representation for prediction and have relatively few annotations if compared to the token-level trained ET model. The contrast is even more pronounced when PWCCA distances are measured across token averages (Figure 4.2 b). Here, pairs containing the ET model quickly diverge from the common trend and settle to a shared PWCCA distance for top layers. Finally, the comparison of all individual token representation contradicts previous RSA trends by showing a remarkably consistent divergence from Base representations at all layers for all the three complexity-trained models.\nAll in all, both RSA and PWCCA suggest an abstraction hierarchy where the closeness of a representation layer to prediction heads is proportional to the magnitude of changes in parameter values during the training process. While RSA similarity highlights a markedly different behavior for the readability-trained model, the more advanced PWCCA method indicates that representations of models trained with similar objectives stay close in parameter space throughout training, regardless of the conceptual proximity phenomena modeled by their loss functions.\n4.2.3 Intra-model Representational Similarity The intra-model similarity is evaluated in the same setting of the previous section. However, instead of comparing the same layer across two different models, the representations learned by all layer pairs inside the same model are compared using RSA and PWCCA. Again, the three perspectives of [CLS], token’s average, and all tokens introduced in the previous chapter are evaluated to understand the shift in representations across layers at different levels of granularity (two sentence-level and one token-level).\nFigure 4.3: Intra-model RSA scores across layers’ combinations for the pre-trained ALBERT model without fine-tuning (Base), using the [CLS] token (top-left), the all-token average (top-right), and all tokens (bottom) representations. Layer -1 corresponds to the last layer before prediction heads. Higher values denote stronger inter-layer similarity. Figure 4.3 presents intra-model RSA similarity scores for all layer pairs of the Base model, going from the input layer after initial embeddings (-12) to the last layer before prediction heads (-1). Only the Base model results are presented in this chapter since they are very similar to those produced by fine-tuned models. The latter can be found in Appendix D. The first insight relative to RSA intra-model results is that ALBERT layers tend to learn representations that are generally very similar to those of layers in their neighborhood, especially for layers found at the center and close to the input embeddings of the model. While in the case of [CLS] similarity scores fall sharply beyond the preceding/following layer for each layer, suggesting a significant variation in the information encoded across the model structure, the high-similarity range is much broader for tokens’ average and all tokens representations. It is interesting to note that the top two layers (-1 and -2) are almost always very dissimilar in relation to the rest of the model, which is coherent with the spiking behavior around inter-model scores highlighted in the previous section. Another interesting observation is that, while [CLS] and all tokens’ representations are consistently decreasing, the tokens’ average representation similarity follows an undulatory behavior across middle layers for all the tested models, with similarity scores dropping and raising while moving away from reference layer. This fact further supports the evidence that token’s sentence-level average may better integrate language information from lower layers into high-level representations, as highlighted by Miaschi and Dell’Orletta (2020) in the context of morphosyntactic knowledge.\nFigure 4.4: Intra-model PWCCA distances across layers’ combinations for the pre-trained ALBERT model without fine-tuning (Base), using the [CLS] token (top-left), the all-token average (top-right), and all tokens (bottom) representations. Layer -1 corresponds to the last layer before prediction heads. Higher values denote weaker inter-layer similarity. Figure 4.4 presents PWCCA scores in the exact same setup as Figure 4.3. As in the previous section, the inverse trend in scores here is due to PWCCA being a dissimilarity measure, and the range of result scores is smaller than the one of RSA. Conversely to the previous setting, [CLS] representations stay closer across layers when their similarity is measured using PWCCA, and there are no significant spikes in score values. The latter finding is coherent with the effect of cross-layer parameter sharing adopted by ALBERT authors. Quoting Lan et al. (2020): “We observe that the transition from layer to layer [in terms of L2 distances and cosine similarity] are much smoother for ALBERT than for BERT. These results show that weight-sharing affects stabilizing network parameters”. In the context of [CLS] representations, the lowest layer (-12) appears to be slightly closer to the top layers than the subsequent ones. This fact ultimately supports the intuition that ALBERT is heavily overparametrized, and first-level embeddings already capture much information.\nAgain for intra-model similarity, PWCCA highlights an abstraction hierarchy inside ALBERT with smoother and generally more reasonable transitions than those showed by RSA. There is no reason to believe that ALBERT adapts its representation hierarchy as a function of its objective since intra-model similarity scores stay approximately the same before and after fine-tuning for all complexity corpora.\n4.3 Summary In this chapter, the representations learned by a neural language model fine-tuned on multiple complexity-related tasks were compared using two widely-used representational similarity approaches. Token and sentence-level representations were compared both considering the same layer across models exposed to different training corpora and different layer pairs contained in the same model. In the first case, the absence of a preponderant similarity between complexity-trained models when compared to the pre-trained one suggests that those models learn their objective by overfitting annotations and without being able to recognize useful primitives that could be recycled throughout complexity tasks. This fact is highlighted in the comparison between perceived complexity and eye-tracking-trained models, where similarity scores of layers close to prediction heads are very different despite the close relationship between the two complexity perspectives. In conclusion, this work strongly supports the claim that representation learning in ALBERT and other neural language models is mainly driven by training biases like task granularity (token-level vs. sentence-level) that are unrelated to the nature of the task itself. This fact hinders their generalization performances, suggesting that much work still needs to be done beyond language modeling to drive generalizable, hierarchical, and compositional representation learning in models of language.\nReferences Brunato, Dominique, Lorenzo De Mattei, Felice Dell’Orletta, Benedetta Iavarone, and Giulia Venturi. 2018. “Is This Sentence Difficult? Do You Agree?” In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, 2690–9. Brussels, Belgium: Association for Computational Linguistics. https://doi.org/10.18653/v1/D18-1289.\nCop, Uschi, Nicolas Dirix, Denis Drieghe, and Wouter Duyck. 2017. “Presenting Geco: An Eyetracking Corpus of Monolingual and Bilingual Sentence Reading.” Behavior Research Methods 49 (2). Springer: 602–15.\nHollenstein, Nora, Jonathan Rotsztejn, Marius Troendle, Andreas Pedroni, Ce Zhang, and Nicolas Langer. 2018. “ZuCo, a Simultaneous Eeg and Eye-Tracking Resource for Natural Sentence Reading.” Scientific Data 5 (1). Nature Publishing Group: 1–13.\nHollenstein, Nora, Marius Troendle, Ce Zhang, and Nicolas Langer. 2020. “ZuCo 2.0: A Dataset of Physiological Recordings During Natural Reading and Annotation.” In Proceedings of the 12th Language Resources and Evaluation Conference, 138–46. Marseille, France: European Language Resources Association. https://www.aclweb.org/anthology/2020.lrec-1.18.\nKennedy, Alan, Robin Hill, and Joël Pynte. 2003. “The Dundee Corpus.” In Proceedings of the 12th European Conference on Eye Movement.\nLan, Zhenzhong, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. 2020. “ALBERT: A Lite BERT for Self-Supervised Learning of Language Representations.” In International Conference on Learning Representations. https://openreview.net/forum?id=H1eA7AEtvS.\nMiaschi, Alessio, and Felice Dell’Orletta. 2020. “Contextual and Non-Contextual Word Embeddings: An in-Depth Linguistic Investigation.” In Proceedings of the 5th Workshop on Representation Learning for Nlp, 110–19. Online: Association for Computational Linguistics. https://www.aclweb.org/anthology/2020.repl4nlp-1.15.\nSocher, Richard, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. 2013. “Recursive Deep Models for Semantic Compositionality over a Sentiment Treebank.” In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, 1631–42. Seattle, Washington, USA: Association for Computational Linguistics. https://www.aclweb.org/anthology/D13-1170.\nVajjala, Sowmya, and Ivana Lučić. 2018. “OneStopEnglish Corpus: A New Corpus for Automatic Readability Assessment and Text Simplification.” In Proceedings of the Thirteenth Workshop on Innovative Use of NLP for Building Educational Applications, 297–304. New Orleans, Louisiana: Association for Computational Linguistics. https://doi.org/10.18653/v1/W18-0535.\nThe albert-base-v2 checkpoint from 🤗 transformers (Wolf et al. 2020) is used.↩\nCode available at https://github.com/gsarti/interpreting-complexity↩\nMore details on this procedure are provided in Appendix C.↩\nSee Appendix B for additional details on the preprocessing and merging of eye-tracking corpora.↩\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"51f3840604fb645e8c02b1106c3b3221","permalink":"https://gsarti.com/msc-thesis/chap-ex2/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/msc-thesis/chap-ex2/","section":"msc-thesis","summary":"\u003c!DOCTYPE html\u003e 4 Representational Similarity in Models of Complexity | Interpreting Neural Language Models for Linguistic Complexity Assessment Introduction 1 Linguistic Complexity 1.1 Categorizing Linguistic Complexity Measures 1.2 Intrinsic Perspective 1.2.1 Structural Linguistic Complexity 1.2.2 Language Modeling Surprisal 1.3 Extrinsic Perspective 1.3.1 Automatic Readability Assessment 1.3.2 Perceived Complexity Prediction 1.3.3 Gaze Metrics Prediction 1.4 Garden-path Sentences 2 Models of Linguistic Complexity 2.1 Desiderata for Models of Linguistic Complexity 2.2 Neural Language Models: Unsupervised Multitask Learners 2.","tags":null,"title":"","type":"msc-thesis"},{"authors":null,"categories":null,"content":"\u003c!DOCTYPE html\u003e 5 Gaze-informed Models for Cognitive Processing Prediction | Interpreting Neural Language Models for Linguistic Complexity Assessment Introduction 1 Linguistic Complexity 1.1 Categorizing Linguistic Complexity Measures 1.2 Intrinsic Perspective 1.2.1 Structural Linguistic Complexity 1.2.2 Language Modeling Surprisal 1.3 Extrinsic Perspective 1.3.1 Automatic Readability Assessment 1.3.2 Perceived Complexity Prediction 1.3.3 Gaze Metrics Prediction 1.4 Garden-path Sentences 2 Models of Linguistic Complexity 2.1 Desiderata for Models of Linguistic Complexity 2.2 Neural Language Models: Unsupervised Multitask Learners 2.2.1 Emergent Linguistic Structures in Neural Language Models 2.3 Analyzing Neural Models of Complexity 2.3.1 Probing classifiers 2.3.2 Representational Similarity Analysis 2.3.3 Projection-Weighted Canonical Correlation Analysis 3 Complexity Phenomena in Linguistic Annotations and Language Models 3.1 Data and Preprocessing 3.2 Analysis of Linguistic Phenomena 3.2.1 Linguistic Phenomena in Length-controlled Bins 3.3 Modeling Online and Offline Linguistic Complexity 3.3.1 Modeling Complexity in Length-controlled Bins 3.4 Probing Linguistic Phenomena in ALBERT Representations 3.5 Summary 4 Representational Similarity in Models of Complexity 4.1 Knowledge-driven Requirements for Learning Models 4.2 Experimentsl Evaluation 4.2.1 Data 4.2.2 Inter-model Representational Similarity 4.2.3 Intra-model Representational Similarity 4.3 Summary 5 Gaze-informed Models for Cognitive Processing Prediction 5.1 Experimental Setup 5.2 Experimental Evaluation 5.2.1 Estimating Magnitudes of Garden-path Delays 5.2.2 Predicting Delays with Surprisal and Gaze Metrics 5.3 Summary Conclusion Broader Impact and Ethical Perspectives Future Directions Appendix A Linguistic Features A.1 Raw Text Properties and Lexical Variety A.2 Morpho-syntacting Information A.3 Verbal Predicate Structure A.4 Global and Local Parsed Tree Structures A.5 Syntactic Relations A.6 Subordination Phenomena B Precisions on Eye-tracking Metrics and Preprocessing C Multi-task Token-level Regression for Gaze Metrics Prediction D Intra-model Similarity for All Models E Gaze Metrics Predictions for Garden Path Sentences F Reproducibility and Environmental Impact References Back to my website Interpreting Neural Language Models\nfor Linguistic Complexity Assessment 5 Gaze-informed Models for Cognitive Processing Prediction This final experimental chapter aims to study the syntactic generalization capabilities of neural language models by evaluating their performances over atypical linguistic constructions. In particular, architectures pre-trained with masked and causal language modeling are evaluated in their ability to predict garden-path effects on three test suites taken from the SyntaxGym psycholinguistic benchmark. First, the results of previous studies using GPT-2 surprisal to predict garden-path effects are reproduced, and a conversion coefficient is used to evaluate GPT-2 surprisal in terms of human reading times delays. Two neural language models are fine-tuned over gaze metrics from multiple eye-tracking corpora in a multitask token-level setting. Gaze metric predictions on garden-path sentences are evaluated to see whether gaze data fine-tuning can improve garden-path effects prediction. Results highlight how GPT-2 surprisals overestimate the magnitude of MV/RR and NP/Z garden-path effects, and fine-tuning procedures on gaze metrics prediction over typical linguistic structures do not benefit the generalization capabilities of neural language models on out-of-distribution cases like garden-path sentences.\nHuman behavioral data collected during naturalistic reading can provide useful insights into the primary sources of processing difficulties during reading comprehension. Multiple cognitive processing theories were formulated to account for the sources of such difficulties (see Section 1.4). Notably, surprisal theory (Hale 2001; Levy 2008) suggests that processing during reading is the direct result of a single mechanism, that is, the shift in readers’ probability distribution over all possible parses. To evaluate whether this perspective holds empirically, language models defining a probability distribution over a vocabulary given previous context (RNNs in Elman (1991) and Mikolov et al. (2010), recently Transformers in Hu et al. (2020)) are commonly used to obtain accurate predictability estimates that can directly be compared to behavioral recordings (e.g. gaze metrics) acting as proxies of human cognitive processing.\nA computational model that consistently mimics human processing behaviors would provide strong evidence of cognitive processing’s underlying probabilistic-driven nature. For this reason, many studies in the fields of syntax and psycholinguistics have focused on probing the abilities of language models to highlight phenomena related to reading difficulties (Linzen, Dupoux, and Goldberg 2016; Gulordava et al. 2018; Futrell et al. 2019). Peculiar constructions like garden-path sentences are often used in this context to evaluate the generalization capabilities of language models for two main reasons. First, garden-path sentences are rare in naturally-occurring text. As such, they represent out-of-distribution examples for any language model trained on conventional data and can be used to test the latter’s generalization capabilities. Secondly, researchers nowadays have access to reasonably-sized literature describing the impact of garden-path effects on cognitive processing proxies such as gaze recordings, with articles being often released alongside publicly-available resources for reproducible evaluation (Prasad and Linzen 2019a, 2019b) and recently even ad-hoc benchmarks (Gauthier et al. 2020).\nThis final experimental chapter evaluates the ability of neural language models in predicting garden-path effects observed on human subjects, using language modeling surprisal and eye-tracking metrics elicited respectively before and after multitask token-level eye-tracking fine-tuning for garden-path effects prediction. Specifically, an autoregressive (GPT-2, Radford et al. (2019)) and a masked language model (ALBERT, Lan et al. (2020)) are first tested over three garden-path test suites that are part of the SyntaxGym benchmark to evaluate whether their language modeling surprisal before and after eye-tracking fine-tuning (ET) can be used to predict the presence and the magnitude of garden-path effects over disambiguating regions. In particular, GPT-2 and GPT-2 XL results presented in Hu et al. (2020) are reproduced. Finally, the same procedure is repeated using predicted eye-tracking scores predicted by models after fine-tuning instead of language modeling surprisal, following the intuition that an accurate model of gaze measurements should predict such phenomena correctly.\nWhile the usage of surprisal is a common practice for garden-path effect prediction, leveraging eye-tracking scores predicted by a neural language model trained for this purpose is a novel research direction that is deemed interesting as a way to combine the predictive power of modern language models and the strong connection between cognitive processing and gaze metrics. While predicted gaze metrics for garden-path evaluation were used in concurrent studies (Schijndel and Linzen 2020), the approach adopted by this work can be regarded as complementary evidence since eye-tracking metrics predictions are produced as results of an end-to-end supervised fine-tuning procedure involving a neural language model rather than being derived from surprisal values through a conversion coefficient. Findings suggest that, while surprisal scores from autoregressive models accurately reflect garden-path structures both before and after fine-tuning, gaze metrics predictions produced by fine-tuned models do not account for the temporary syntactic ambiguity that characterizes such sentences and makes them difficult to process.\nContributions This study validates the performances of standard and gaze-informed Transformed-based neural language models for garden-path effects prediction. In particular:\nIt reproduces the GPT-2 performances on garden-path test suites reported by Gauthier et al. (2020) and highlights how GPT-2 overestimates reading delays caused by garden-path effects on MV/RR and NP/Z constructions.\nIt highlights masked language models’ inability to consistently predict garden-path effects, using language modeling surprisal and gaze metrics predictions.\nIt introduces a novel gaze metrics multitask token-level fine-tuning approach that, despite being accurate for predicting eye-tracking scores on standard constructions, does not improve models’ performances on garden-path effects predictions.\n5.1 Experimental Setup Fine-tuning data As for the gaze metrics model presented in the previous chapter, all eye-tracking datasets presented in Section 1.3.3 were merged and used to fine-tune neural language models using the multitask token-level approach described in Appendix C. Only the training variant without embedding concatenation (referred to as “surprisal” in the appendix) was evaluated on garden-path test suites given comparable modeling performances.\nModels Two variants of GPT-2 having respectively 117 million and 1.5 billion parameters are evaluated in terms of surprisal-driven predictability, alongside an ALBERT model with 11 million parameters.22 Only the small GPT-2 model and the ALBERT model were fine-tuned for gaze metric predictions due to limited computational resources.\nEvaluation data SyntaxGym (Gauthier et al. 2020) is a recently introduced online platform designed to make the targeted evaluation of language models on psycholinguistic test suites both accessible and reproducible. The MV/RR and NP/Z test suites containing garden paths from Futrell et al. (2019) are used in the context of this work. The MV/RR test suite consists of 28 groups containing a sentence with a main verb/reduced relative ambiguity and its non-ambiguous rewritings. In comparison, the NP/Z test suites consist of 24 groups containing a sentence with a nominal/zero predicate ambiguity, produced either by a misinterpreted transitive use of a verb (Verb Transitivity) or the absence of an object for the main verb (Overt Object). Examples (3), (4), and (5) from Section 1.4 follow the format used in the three SyntaxGym test suites used in this work.\nFigure 5.1: Average GPT-2 surprisal predictions and examples for the NP/Z Ambiguity with Verb Transitivity (top), the NP/Z Ambiguity with Overt Object (middle), and the MV/RR Ambiguity (bottom) SyntaxGym test suites used in this study. Star marks the garden-path disambiguator (bold in examples), and bars show 95% confidence intervals. 5.2 Experimental Evaluation For the first part of the experiments, the smallest version of the model GPT-2 is used. Figure 5.1 reproduces the original setting tested by Hu et al. (2020), showing how predictability estimates produced by the model correctly individuate the presence of garden-path effects.23 Surprisal values are computed using a pre-trained GPT-2 for all tokens in all sentences of the three test suites. Then, those values are aggregated by summing them across all tokens composing a sentence region. For example, for the NP/Z Ambiguity test suite entry shown in example (a) the region “Start” will be associated with the sum of surprisal estimates for all subword tokens in the sequence While the students. It is important to note that the four variants of the same sentence have only minimal variations, but only one of those (the underlined one in all examples) is a garden-path sentence. After computing GPT-2 surprisal scores for all regions of all sentences in the test sets, those are averaged region-wise across sentences belonging to the same test set to obtain the three plots presented in Figure 5.1. The star symbol is used to mark the disambiguating region of garden-path sentences, making evident how predictability estimates are significantly lower (i.e., higher surprisal values) for those and correctly predict the presence of a garden-path effect in most settings and for all the three garden-path variants.\n5.2.1 Estimating Magnitudes of Garden-path Delays An important part of evaluating model predictions over garden-path sentences is determining whether the increase in surprisal scores correctly captures the effect’s magnitude. Schijndel and Linzen (2020) perform this evaluation on RNN language models, finding that they vastly underestimate garden-path effects for MV/RR and NP/Z ambiguities. In their approach, Schijndel and Linzen (2020) estimate the surprisal-to-reading-times conversion rate at 2ms per surprisal bit by fitting a linear mixed-effect model on relevant factors (surprisal, entropy, word length, among others) relative to a word and its three preceding words to account for spillover effects. The approach adopted in this work is different in that it stems from the empirical relation between surprisal scores produced by GPT-2 and reading times produced by eye-tracking experiments’ participants. Figure 5.2 presents the median values over words for the ratio between gaze metrics recorded by participants and GPT-2 surprisal estimates, with the red cross indicating the average median surprisal-to-metric ratio \\(C_{\\text{corpus}}^{\\text{metric}}\\) computed across all participants of a corpus. The following formula is used to produce the surprisal-to-reading-times conversion coefficient: \\[\\begin{equation} C_{S\\rightarrow RT} = w_1 \\cdot C_{\\text{GECO}}^{\\text{FPD}} + w_2 \\cdot C_{\\text{Dundee}}^{\\text{FPD}} + w_3 \\cdot C_{\\text{ZuCo NR}}^{\\text{FPD}} + w_4 \\cdot C_{\\text{ZuCo SR}}^{\\text{FPD}} + w_5 \\cdot C_{\\text{ZuCo 2.0}}^{\\text{FPD}} \\end{equation}\\] with \\(w = [.4, .45, .05, .05, .05]\\) being the weighting coefficients representing the proportion of each corpus’ tokens over the total amount of available gaze-annotated tokens.\nFigure 5.2: Median scores for the ratio between gaze metrics units and GPT-2 surprisal estimates across all participants of all eye-tracking datasets used in this study. The red cross shows the average across participants of a single dataset. Units are in ms for durations, % for FXP, and raw counts for FXC. The resulting value for the conversion coefficient is \\(27.7\\), i.e., each surprisal bit predicted by GPT-2 accounts for roughly 27.7 milliseconds in first pass duration (30.3ms using TFD). When applied to the average effects predicted by GPT-2 in Figure 5.1, it leads to an estimated delay of roughly 64ms for the MV/RR setting and 166ms and 194ms for the NP/Z Ambiguity and NP/Z Overt Object settings, respectively. These computed delays overestimate the literature’s effects: Prasad and Linzen (2019a) and Prasad and Linzen (2019b), for example, report an average garden-path effect of 22ms and 27ms for MV/RR and NP/Z variants, respectively. However, it should be mentioned that precedent studies found higher delays for NP/Z structures: Grodner et al. (2003) find a 64ms delay on disambiguating words, and Sturt, Pickering, and Crocker (1999)‘s delays of 152ms per word are close to the estimates produced by GPT-2 surprisal predictions. Overall, using models’ surprisal on gaze-annotated sentences to directly compute a conversion coefficient produces values that correctly identify delays on disambiguating regions and overestimate the magnitude of garden-path effects conversely to what was found by Schijndel and Linzen (2020). Even with an adjustment of the conversion coefficient to match MV/RR estimates with Prasad and Linzen (2019a) findings, the NP/Z effect prediction would still be much larger than the empirically-observed values collected in comparable settings.\n5.2.2 Predicting Delays with Surprisal and Gaze Metrics The other perspective explored in this study is evaluating whether gaze metric predicted by models fine-tuned on eye-tracking corpora annotations can correctly estimate the presence and magnitude of garden-path effects and how they compare to surprisal-driven approaches. Table 5.1 presents the accuracy of multiple pre-trained Transformer-based language models in respecting a set of three conditions taken from Hu et al. (2020) for each SyntaxGym test suite, namely: \\[\\begin{equation} V_d(b) \u0026lt; V_d(a);\\qquad V_d(c) \u0026lt; V_d(a);\\qquad V_d(c)-V_d(d) \u0026lt; V_d(a)-V_d(b) \\end{equation}\\] Where \\(V_d(a)\\) corresponds to the value, either in terms of surprisal or gaze metrics, assigned by a model to the disambiguating region \\(d\\) of sentence \\(a\\), and \\(a,b,c,d\\) are the same sentence’s variants for each test suite presented in examples (3),(4) and (5) of Section 1.4. Accuracy is computed as the proportion of items in the test suite on which the language model’s predictions conform to the respective criterion. The first three models (GPT-2, GPT-2 XL, and ALBERT) are the pre-trained variants of the three models presented in Table 5.1 without additional fine-tuning. Instead, the GPT-2 ET and ALBERT ET models correspond to the same GPT-2 and ALBERT models as before after a multitask token-level fine-tuning on gaze metrics for all the aggregated corpora. The top part of Table 5.1 shows the five models’ performances while using region-aggregated surprisals as predictors. Focusing on the GPT-2 variants, it can be observed that they all achieve considerably high scores on all evaluated conditions. Conversely, ALBERT masked language models poorly fit the specified criteria. This fact can be intuitively explained by accounting for the different training and evaluation setup used for the two architectures. GPT-2 models are likely to produce high surprisal estimates for garden-path sentences since, processing the input autoregressively and having access only to previous tokens, they incur in the same syntactic ambiguities faced by human readers.\nTable 5.1: Results of experiments using surprisal and gaze metrics as predictors for garden-path effects on the three SyntaxGym test suites. NP/Z Verb Transitivity NP/Z Overt Object MV/RR Ambiguity Cond. 1 1 Cond. 2 2 Cond. 3 3 Cond. 1 a Cond. 2 b Cond. 3 c Cond. 1 * Cond. 2 † Cond. 3 ‡ Surprisal GPT-2 0.96 0.92 0.88 0.96 1 1 1 0.89 0.82 GPT-2 XL 1 0.96 1 0.96 1 1 0.93 0.75 0.75 ALBERT 0.21 0.63 0.58 0.21 0.54 0.46 0.61 0.54 0.38 GPT-2 ET 0.96 0.88 0.79 0.96 1 0.96 0.96 0.79 0.82 ALBERT ET 0.42 0.42 0.58 0.42 0.75 0.62 0.5 0.64 0.64 Eye-tracking metrics GPT-2 ET FFD 0.29 0.38 0.46 0.29 0.54 0.42 0.86 0.57 0.5 FPD 0.13 0.46 0.67 0.13 0.5 0.46 0.86 0.54 0.36 FXP 0.38 0.5 0.42 0.42 0.41 0.42 0.71 0.43 0.57 FXC 0.75 0.5 0.42 0.75 0.63 0.46 0.92 0.46 0.54 TFD 0.5 0.33 0.46 0.5 0.58 0.75 0.79 0.43 0.39 TRD 0.67 0.46 0.54 0.63 0.25 0.54 0.29 0.39 0.5 ALBERT ET FFD 0.67 0.33 0.42 0.42 0.83 0.67 0.68 0.61 0.5 FPD 0.54 0.41 0.33 0.38 0.79 0.75 0.75 0.57 0.46 FXP 0.28 0.46 0.29 0.54 0.38 0.63 0.29 0.5 0.43 FXC 0.63 0.46 0.5 0.38 0.67 0.71 0.86 0.43 0.39 TFD 0.75 0.38 0.29 0.5 0.88 0.83 0.79 0.61 0.54 TRD 0.96 0.42 0.42 0.63 0.75 0.5 0.79 0.5 0.57 Description of the evaluated conditions NP/Z Verb Trans.: 1 [Ambig. No Comma] \u0026gt; [Ambig. Comma]; 2 [Ambig. No Comma] \u0026gt; [Unambig. No Comma]; 3 [Ambig. No Comma] - [Ambig. Comma] \u0026gt; [Unambig. No Comma] - [Unambig. Comma] NP/Z Overt Obj.: a [No Obj. No Comma] \u0026gt; [No Obj. Comma]; b [No Obj. No Comma] \u0026gt; [Obj. No Comma]; c [No Obj. No Comma] - [No Obj. Comma] \u0026gt; [Obj. No Comma] - [Obj. Comma] MV/RR Ambig.: * [Reduced Ambig.] \u0026gt; [Unred. Ambig.]; † [Reduced Ambig.] \u0026gt; [Reduced Unambig.]; ‡ [Reduced Ambig.] - [Unred. Ambig.] \u0026gt; [Reduced Unambig.] - [Unred. Unambig.] Conversely, ALBERT-like masked language models have access to bidirectional contexts and are not exposed to the ambiguity. It is interesting to observe that while the eye-tracking fine-tuning procedure appears to hamper GPT-2 surprisal performances, it generally improves the ALBERT model’s accuracy. This phenomenon may be due to the sequential nature of reading that is being captured by gaze metrics and transferred to the bidirectional ALBERT model as a useful bias for sequential processing. The same procedure performs suboptimally, instead, when associated with an inherently autoregressive model like the GPT-2 decoder\nThe bottom part of Table 5.1 presents the two ET-trained models’ accuracy in matching criteria using predicted gaze metrics. For both GPT-2 and ALBERT, it can be observed that gaze metrics vastly underperform in accuracy terms. We can conclude that, despite the conceptual relation between gaze metrics and predictability observed in humans, the predictions of fine-tuned model cannot generalize to unseen settings, and as such eye-tracking predictions obtained after a fine-tuning on standard constructions do not appear useful to individuate or estimate the magnitude of garden-path effects. This observation suggests that fine-tuned models stick to predicting gaze metric values that are the most likely for each specific token, regardless of the surrounding context’s ambiguities. Plots in Appendix E present the region-aggregated average scores for all metrics predicted by GPT-2 ET in the same format as before and show how predictions on the disambiguator regions are unaffected by the presence of previous ambiguities.\n5.3 Summary This chapter focused on two perspectives related to the evaluation of neural language models for garden-path effects prediction. First, promising results from previous studies using GPT-2 surprisal to evaluate predictability are reproduced, and language modeling surprisal estimates are converted to reading times using a conversion coefficient. Resulting predictions vastly overestimate the magnitude of garden-path effects in all settings, suggesting the presence of additional mechanisms besides predictability in shaping cognitive processing in the presence of ambiguous constructions like garden-path sentences. This evidence is further supported by the second experimental perspective, in which reading times for garden-path sentences are predicted by models fine-tuned on eye-tracking annotations on corpora containing standard constructions. Results suggest that predicted gaze metrics poorly estimate the presence of garden-path effects over disambiguating regions, suggesting that fine-tuned models are once again incapable of out-of-the-box generalization beyond training settings.\nReferences Elman, Jeffrey L. 1991. “Distributed Representations, Simple Recurrent Networks, and Grammatical Structure.” Machine Learning 7 (2-3). Springer: 195–225.\nFutrell, Richard, Ethan Wilcox, Takashi Morita, Peng Qian, Miguel Ballesteros, and Roger Levy. 2019. “Neural Language Models as Psycholinguistic Subjects: Representations of Syntactic State.” In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), 32–42. Minneapolis, Minnesota: Association for Computational Linguistics. https://doi.org/10.18653/v1/N19-1004.\nGauthier, Jon, Jennifer Hu, Ethan Wilcox, Peng Qian, and Roger Levy. 2020. “SyntaxGym: An Online Platform for Targeted Evaluation of Language Models.” In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, 70–76. Online: Association for Computational Linguistics. https://doi.org/10.18653/v1/2020.acl-demos.10.\nGrodner, Daniel, Edward Gibson, Vered Argaman, and Maria Babyonyshev. 2003. “Against Repair-Based Reanalysis in Sentence Comprehension.” Journal of Psycholinguistic Research 32 (2). Springer: 141–66.\nGulordava, Kristina, Piotr Bojanowski, Edouard Grave, Tal Linzen, and Marco Baroni. 2018. “Colorless Green Recurrent Networks Dream Hierarchically.” In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), 1195–1205. New Orleans, Louisiana: Association for Computational Linguistics. https://doi.org/10.18653/v1/N18-1108.\nHale, John. 2001. “A Probabilistic Earley Parser as a Psycholinguistic Model.” In Second Meeting of the North American Chapter of the Association for Computational Linguistics.\nHu, Jennifer, Jon Gauthier, Peng Qian, Ethan Wilcox, and Roger Levy. 2020. “A Systematic Assessment of Syntactic Generalization in Neural Language Models.” In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, 1725–44. Online: Association for Computational Linguistics. https://doi.org/10.18653/v1/2020.acl-main.158.\nLan, Zhenzhong, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. 2020. “ALBERT: A Lite BERT for Self-Supervised Learning of Language Representations.” In International Conference on Learning Representations. https://openreview.net/forum?id=H1eA7AEtvS.\nLevy, Roger. 2008. “Expectation-Based Syntactic Comprehension.” Cognition 106 (3). Elsevier: 1126–77.\nLinzen, Tal, Emmanuel Dupoux, and Yoav Goldberg. 2016. “Assessing the Ability of Lstms to Learn Syntax-Sensitive Dependencies.” Transactions of the Association for Computational Linguistics 4. MIT Press: 521–35.\nMikolov, Tomas, M. Karafiát, L. Burget, J. Cernocký, and S. Khudanpur. 2010. “Recurrent Neural Network Based Language Model.” In INTERSPEECH.\nPrasad, Grusha, and Tal Linzen. 2019a. “Do Self-Paced Reading Studies Provide Evidence for Rapid Syntactic Adaptation?” PsyArXiv Pre-Print. https://tallinzen.net/media/papers/prasad_linzen_2019_adaptation.pdf.\nPrasad, Grusha, and Tal Linzen. 2019b. “How Much Harder Are Hard Garden-Path Sentences Than Easy Ones?” OSF Preprint syh3j. https://osf.io/syh3j/.\nRadford, A., Jeffrey Wu, R. Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. “Language Models Are Unsupervised Multitask Learners.” OpenAI Blog. OpenAI.\nSchijndel, Marten van, and Tal Linzen. 2020. “Single-Stage Prediction Models Do Not Explain the Magnitude of Syntactic Disambiguation Difficulty.” PsyArXiv Pre-Print sgbqy. https://psyarxiv.com/sgbqy/.\nSturt, Patrick, Martin J Pickering, and Matthew W Crocker. 1999. “Structural Change and Reanalysis Difficulty in Language Comprehension.” Journal of Memory and Language 40 (1). Elsevier: 136–50.\nThe gpt2, gpt2-xl and albert-base-v2 pre-trained models from 🤗 transformers (Wolf et al. 2020).↩\nSimilar plots are available on the SyntaxGym website: http://syntaxgym.org/viz/individual↩\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"fc87152c128c68709e5b12012ad47a6a","permalink":"https://gsarti.com/msc-thesis/chap-ex3/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/msc-thesis/chap-ex3/","section":"msc-thesis","summary":"\u003c!DOCTYPE html\u003e 5 Gaze-informed Models for Cognitive Processing Prediction | Interpreting Neural Language Models for Linguistic Complexity Assessment Introduction 1 Linguistic Complexity 1.1 Categorizing Linguistic Complexity Measures 1.2 Intrinsic Perspective 1.2.1 Structural Linguistic Complexity 1.2.2 Language Modeling Surprisal 1.3 Extrinsic Perspective 1.3.1 Automatic Readability Assessment 1.3.2 Perceived Complexity Prediction 1.3.3 Gaze Metrics Prediction 1.4 Garden-path Sentences 2 Models of Linguistic Complexity 2.1 Desiderata for Models of Linguistic Complexity 2.2 Neural Language Models: Unsupervised Multitask Learners 2.","tags":null,"title":"","type":"msc-thesis"},{"authors":null,"categories":null,"content":"\u003c!DOCTYPE html\u003e 1 Linguistic Complexity | Interpreting Neural Language Models for Linguistic Complexity Assessment Introduction 1 Linguistic Complexity 1.1 Categorizing Linguistic Complexity Measures 1.2 Intrinsic Perspective 1.2.1 Structural Linguistic Complexity 1.2.2 Language Modeling Surprisal 1.3 Extrinsic Perspective 1.3.1 Automatic Readability Assessment 1.3.2 Perceived Complexity Prediction 1.3.3 Gaze Metrics Prediction 1.4 Garden-path Sentences 2 Models of Linguistic Complexity 2.1 Desiderata for Models of Linguistic Complexity 2.2 Neural Language Models: Unsupervised Multitask Learners 2.2.1 Emergent Linguistic Structures in Neural Language Models 2.3 Analyzing Neural Models of Complexity 2.3.1 Probing classifiers 2.3.2 Representational Similarity Analysis 2.3.3 Projection-Weighted Canonical Correlation Analysis 3 Complexity Phenomena in Linguistic Annotations and Language Models 3.1 Data and Preprocessing 3.2 Analysis of Linguistic Phenomena 3.2.1 Linguistic Phenomena in Length-controlled Bins 3.3 Modeling Online and Offline Linguistic Complexity 3.3.1 Modeling Complexity in Length-controlled Bins 3.4 Probing Linguistic Phenomena in ALBERT Representations 3.5 Summary 4 Representational Similarity in Models of Complexity 4.1 Knowledge-driven Requirements for Learning Models 4.2 Experimentsl Evaluation 4.2.1 Data 4.2.2 Inter-model Representational Similarity 4.2.3 Intra-model Representational Similarity 4.3 Summary 5 Gaze-informed Models for Cognitive Processing Prediction 5.1 Experimental Setup 5.2 Experimental Evaluation 5.2.1 Estimating Magnitudes of Garden-path Delays 5.2.2 Predicting Delays with Surprisal and Gaze Metrics 5.3 Summary Conclusion Broader Impact and Ethical Perspectives Future Directions Appendix A Linguistic Features A.1 Raw Text Properties and Lexical Variety A.2 Morpho-syntacting Information A.3 Verbal Predicate Structure A.4 Global and Local Parsed Tree Structures A.5 Syntactic Relations A.6 Subordination Phenomena B Precisions on Eye-tracking Metrics and Preprocessing C Multi-task Token-level Regression for Gaze Metrics Prediction D Intra-model Similarity for All Models E Gaze Metrics Predictions for Garden Path Sentences F Reproducibility and Environmental Impact References Back to my website Interpreting Neural Language Models\nfor Linguistic Complexity Assessment 1 Linguistic Complexity Defining linguistic complexity in a univocal way is challenging, despite the subjective intuition that every individual may have about what should be deemed complex in written or spoken language. Indeed, if the faculty of language allows us to produce a possibly infinite set of sentences from a finite vocabulary, there are infinitely many ways in which a sentence may appear difficult to a reader’s eyes. An accurate definition is still debated in research fields like cognitive science, psycholinguistics, and computational linguistics. Nonetheless, it is indisputable that the concept of natural language complexity is closely related to difficulties in knowledge acquisition. This property stands both for human language learners and for computational models learning the distributional behavior of words in a corpus.\nThis introductory chapter begins with a categorization of linguistic complexity annotations following taxonomical definitions found in the literature. Various complexity metrics are then introduced alongside corpora and resources that were used throughout this study. Finally, the focus will be put on garden-path sentences, peculiar syntactically-ambiguous constructs studied in the experiments of Chapter 5.\n1.1 Categorizing Linguistic Complexity Measures In modern literature about linguistic complexity, two positions, each trying to define the nature of linguistic complexity phenomena, can be identified. In Kusters (2008) words:\nOn the one hand, complexity is used as a theory-internal concept, or linguistic tool, that refers only indirectly, by way of the theory, to language reality. On the other hand, complexity is defined as an empirical phenomenon, not part of, but to be explained by a theory.\nThese definitions are coherent with the absolute and relative complexity terminology coined by Miestamo (2004), where relative complexity is seen as a factor characterizing the perceptual experience of specific language users. In contrast, absolute complexity is structurally-defined by language constructs and independent from user evaluation. While these two perspectives seem to identify two opposite viewpoints over linguistic complexity, the distinction between the two becomes blurred when we consider that linguistic theories underlying absolute complexity evaluation are developed by linguists, who still have a subjective perspective despite their competence (Kusters 2003). Two definitions are now introduced to operationalize absolute and relative complexity in the context of complexity measurements:\nIntrinsic Perspective The intrinsic perspective on linguistic complexity is closely related to the notion of absolute complexity. From the intrinsic viewpoint, language productions are evaluated using their distributional and structural properties, without any complexity annotation derived by language users. The linguistic system is characterized by a set of elementary components (lexicon, morphology, syntax inter alia) that interact hierarchically (Cangelosi and Turner 2002), and their interactions can be measured in terms of complexity by fixing a set of rules and descriptions. The focus is on objectivity and automatic evaluation based on the intrinsic properties of language systems.\nExtrinsic Perspective The extrinsic perspective connects to the concept of relative complexity and takes into account the individual perspective of users. Complexity judgments are collected during or after the processing of linguistic productions and are then evaluated in terms of cognitive effort required by language users for comprehension. The extrinsic viewpoint is partaken by cognitive processing theories in psycholinguistics such as the Dependency Locality Theory (Gibson 1998, 2000), the Surprisal Theory (Hale 2001, 2016; Levy 2008), and the more recent Lossy-context Surprisal Theory (Futrell, Gibson, and Levy 2020), aiming to disentangle the source of processing difficulties in sentence comprehension. The focus, in this case, is on the subjectivity of language users and their judgments.\nDespite being different under many aspects, the two perspectives are highly interdependent: a user’s perception of complexity will be strongly influenced by the distributional and structural properties of utterances, and some of those properties will be considered complex in relation to the type of judgments they typically elicit in language users. Provided that the strength of human influence in complexity measurements can vary widely depending on data collection procedures, the two perspectives can be seen as the two ends of a spectrum. A visual representation is provided by the horizontal axis of the complexity measures compass in Figure 1.1.\nFigure 1.1: Complexity measures’ compass. An additional dimension for categorizing linguistic complexity metrics can be introduced by considering the time at which measures are obtained, relative to the incremental processing paradigm that characterizes natural reading in human subjects. In this context, processing is defined as any act aimed at extracting information from linguistic forms and structures, either by employing reasoning (in humans) or through computation (in automatic systems). Again, we can identify the two ends of a spectrum concerning processing modalities, related to the concepts of local and global complexity found in linguistic literature (Edmonds 1999; Miestamo 2004, 2008):\nOnline processing Online complexity judgments are collected while a language user, be it a human subject or a computational system, is sequentially processing a text. Online processing is widely explored in the cognitive science literature, where behavioral metrics such are fMRI data and gaze recordings are collected from subjects exposed to locally and temporally-immediate inputs and tasks that require fast processing (Iverson and Thelen 1999). The act of reading is predominantly performed by online cognition (Meyer and Rice 1992), making online measures especially suitable for complexity evaluation for natural reading.\nOffline processing Offline complexity judgments are collected at a later time when the language user has a complete and contextual view of the text in its entirety. Again, offline complexity is related to the offline cognition paradigm (Day 2004) typically used in re-evaluations and future planning. In practice, offline evaluation accounts for contextual and cultural factors closely related to individual subjectivity and is poorly captured by immediate online metrics.\nFigure 1.1 situates various linguistic complexity metrics in terms of processing modalities and analyzed perspective by including the processing spectrum on the vertical axis. In the next sections, all these measures will be introduced and their use will be motivated in light of this categorization.\n1.2 Intrinsic Perspective Complexity studies where the intrinsic point of view is adopted rely on annotations describing linguistic phenomena and structures in sentences and aim to map those to complexity levels or ratings, often resorting to formulas parametrized through empirical observation. Given the scarcity of experienced human annotators and the cost of a manual annotation process, computational systems have been primarily employed to extract linguistic information from raw text in an automated yet precise way.\nAnother intrinsic viewpoint is based on the intuition that frequent constructs should be deemed as less complex than infrequent ones. In this case, terms’ co-occurrences are extracted from large corpora, and complexity judgments are derived from their probabilistic likelihood of appearance in a given context. Given the infeasibility of tracking co-occurrences for long sequences in large, typologically-varied corpora, computational language models are usually employed to learn approximations of co-occurrence likelihoods for specific constructs.\nWhile this thesis work only partially addresses the use of these approaches, they will be briefly introduced to provide additional context for understanding extrinsic perspectives and their experimental evaluation.\n1.2.1 Structural Linguistic Complexity Language systems can be seen as hierarchies of rules and processes governing various aspects of utterances production and use. For each of those levels, it is possible to identify characteristics leading to higher complexity from a structural standpoint (Sinnemäki 2011):\nA greater number of parts in a specific language level leads to a greater syntagmatic complexity (also known as constitutional complexity). This mode is related to the lexical and “superficial” properties of language, such as the length of words and sentences.\nA greater variety of parts in a specific language level leads to a greater paradigmatic complexity (also known as taxonomic complexity). This mode characterizes, in particular, the phonological level, where the presence of an elaborated tonal system makes a language more complex (McWhorter 2001), the morphologic level, where inflectional morphology is usually associated to a higher degree of complexity (McWhorter 2001; Kusters 2003) when compared to the regularity of derivational rules, and the semantic level, where polysemic words are generally considered more complex than monosemic ones (Voghera 2001).\nA greater variety of interrelation modalities and hierarchical structures leads to greater organizational and hierarchical complexities. Those complexity modes are mainly related to the syntactic level, where recursive and nested constructs are deemed more complex and possibly determinant in distinguishing human language from animal communication (Hauser, Chomsky, and Fitch 2002).\nFocusing on the syntactic level, we can find multiple factors accounting for greater complexity (Berruto and Cerruti 2011):\nSubordinate clauses preceding the main clause, as in If you need help, let me know\u0026quot; as opposed to “Let me know if you need help”.\nPresence of long-range syntactic dependencies between non-contiguous elements, as in “The dog that the cat chased for days ran away” where the subject referent (dog) and its verb (ran) are far apart in the sentence.\nA high degree of nesting between elements and substructures, as in “The mouse that the cat that the dog bit ate was bought at the fair” where two nested subordinate clauses introduced by the preposition that are present.\nRepeated applications of recursive principles to build utterances with different meanings through the compositionality principle, as in “I am a huge fan of fans of fans of … of recursion”, where the number of recursions defines the final meaning of the sentence.\nWhile all those properties are relevant when evaluating an utterance’s complexity, only some can be easily extracted from corpora using automatic approaches. In the specific context of this work, the analysis of complexity-related features in Chapter 3 makes use of the Profiling–UD tool2 (Brunato et al. 2020), implementing a two-stage process: first, the linguistic annotation process is automatically performed by UDPipe (Straka, Hajič, and Straková 2016), a multilingual pipeline leveraging neural parsers and taggers included in the Universal Dependencies initiative (Nivre et al. 2016). During this step, sentences are tokenized, lemmatized, POS-tagged (i.e., words are assigned lexical categories such as “Noun” and “Verb”) and parsed (i.e., the hierarchical structure of syntactic dependencies is inferred). Then, a set of about 130 linguistic features representing underlying linguistic properties of sentences is extracted from various levels of annotation. Those features account for multiple morphological, syntactic, and “superficial” properties related to linguistic complexity. A relevant subset of those features is presented in detail in Appendix A.\nAfter deriving linguistic properties from sentences, either automatically as in this study or by manual annotations, two approaches are viable to determine their complexity while maintaining an intrinsic perspective (no human processing data involved):\nFormula-based Approach This approach treats linguistic properties of input texts as components of a formula used to determine levels or readability grades. Traditional readability formulas consider multiple factors, such as word length, sentence length, and word frequency. Parameters in those formulas are carefully hand-tuned to match human intuition and correlate well with human-graded readability levels.3\nLearning-based Approach This approach casts the complexity prediction problem in the supervised machine learning framework. More specifically, linguistic parsers are used to predict linguistic properties, and their accuracy on a set of gold-labeled instances is taken as an indicator of complexity. In the case of dependency parsers (i.e., models trained to extract the syntactic structure of a sentence), two evaluation metrics can be used: the Unlabeled and Labeled Attachment Scores (UAS and LAS), where the UAS is the percentage of words assigned to the right dependency head and LAS also consider if the dependency relation was labeled correctly.\nBoth approaches are represented in Figure 1.1 under the label “Property-based Automatic LCA” and are considered offline since the text is generally not processed incrementally but instead taken as a whole.\n1.2.2 Language Modeling Surprisal The information-theoretic concept of surprisal, also known as information content of an event, can be seen as a quantification of the level of surprise caused by a specific outcome: an event that is certain yields no information, while the less probable an event is, the more surprising it gets. Formally, an event \\(x\\) with probability \\(p(x)\\) has a surprisal value equal to:\n\\[\\begin{equation} I(x) = - \\log[p(x)] \\end{equation}\\]\nThe idea that probabilistic expectations in the context of language reading are related to greater complexity in terms of cognitive processing was formalized by surprisal theory (Hale 2001, 2016). Surprisal theory defines processing difficulties \\(D\\) (which can be considered as proxies of complexity) as directly proportional to the surprisal produced in readers by a word \\(w\\) given its previous context \\(c\\) (i.e., preceding words in the sentence):\n\\[\\begin{equation} D(w_i|c) \\propto -\\log p(w_i|c) = -\\log p(w_i|w_i-1, w_i-2,\\dots, w_0) \\end{equation}\\]\nWhile processing difficulties imply human subjects’ presence, language models (LM) can be used to estimate the conceptually similar information-theoretic surprisal without the need of human annotations by learning word occurrences and co-occurrences probabilities from large quantities of text. Concretely, a language model is a probabilistic classifier that learns to predict a probability distribution over words of a vocabulary \\(V\\) given a large number of contexts \\(c\\) in which those words occur (Goodman 2001):\n\\[\\begin{equation} p(w_i|c) \\quad \\forall\\ w_i\\in V \\end{equation}\\]\nAfter the training procedure it is possible to estimate the probability \\(p(s)\\) of a sentence \\(s\\) having length \\(m\\) as the product of the conditional probabilities assigned to individual words by the language model, given its context:\n\\[\\begin{equation} p(s) = p(w_1, \\dots, w_m) = \\prod_{i=1}^m p(w_i \\ |c) \\tag{1.1} \\end{equation}\\]\nWe can consider the surprisal \\(I(s) = -\\log p(s)\\) as an intrinsic measure of linguistic complexity since it is a function of the co-occurrence relations derived by the training corpora. Thus, it describes how likely a construct can be observed in a structurally-sound manner, without relying on human processing data. However, automatic surprisal estimation using language models cannot be considered purely intrinsic since it is highly dependent on a multitude of factors that are arguably “less objective” than the linguistic categories of the previous section, such as the type and dimension of the considered context and the corpora employed by the LM to learn words’ distributional behavior.\nWe can categorize modern language models in two broad categories: sequential models (also known as autoregressive or causal LMs) consider as context only preceding words, while bidirectional models (also known as masked LMs) consider both preceding and following words when estimating occurrence probabilities, much like the well-established cloze test (Taylor 1953) in psycholinguistics. Equations (1.2) show how the sentence surprisal equation (1.1) is adapted in both cases, using the product rule for logarithms:\n\\[\\begin{equation} \\begin{split} I_{\\mathrm{sequential}}(s) \u0026amp; = - \\sum_{i=1}^m \\log p(w_i \\ | w_1, w_2, \\dots, w_{i-1})\\\\ I_{\\mathrm{bidirectional}}(s) \u0026amp; = - \\sum_{i=1}^m \\log p(w_i \\ | w_1, \\dots, w_{i-1}, w_{i+1}, \\dots, w_m) \\end{split} \\tag{1.2} \\end{equation}\\]\nIf the LM used to estimate surprisal was sequential, then surprisal estimation could be considered part of the online processing paradigm despite the absence of a human subject.4 In the bidirectional case, the estimation of surprisals from the whole context can be assimilated with offline processing practices.\nThe relation between co-occurrence frequencies estimated by a language model and perception of complexity is one of the aspects that make language models especially suitable for predicting extrinsic complexity metrics, as it will be discussed in Chapter 2.\n1.3 Extrinsic Perspective Extrinsic complexity measures elicited from human-produced signals and annotations are the main focus of this thesis work. In this section, three different viewpoints on linguistic complexity assessment from a human perspective are introduced:\nThe readability point-of-view, as intended in the context of the automatic readability assessment (ARA) task, is concerned with collocating similar textual inputs into difficulty levels that are often predetermined by writers and given a clear semantic interpretation (e.g., easy, medium, hard).\nThe perceptual point-of-view, represented by the perceived complexity prediction (PCP) task, is based on human annotations of complexity on a numeric scale, taking into account disparate textual inputs presented sequentially to obtain more generalizable complexity annotations. Unlike ARA, PCP annotations are produced by readers after sentence comprehension.\nThe cognitive point-of-view, employing cognitive signals collected by specialized machinery (e.g., electrodes, MRI scanners, eye-trackers) as proxies for the linguistic complexity experienced by users. In this work, the focus will be on the gaze metrics prediction task, using gaze data collected from subjects during natural reading.\nAll three complexity-related tasks will be introduced alongside recent results in the literature. The corpora on which each task relies upon will also be presented in their respective sections.\n1.3.1 Automatic Readability Assessment While the term readability assessment is often broadly employed to denote the task of predicting the general reading difficulty of a text, here it is used to describe the typical approach in ARA, relying on corpora categorized by the writer’s perception of what is difficult for readers.\nWe can take as an example the OneStopEnglish (OSE) corpus (Vajjala and Lučić 2018), which will be used later to study the ARA relation with other complexity tasks in Chapter 4. OSE contains 567 weekly articles from The Guardian newspaper rewritten by language teachers to suit three adult English learners’ levels. Each text can be divided into passages spanning one or multiple sentences, each labeled with a readability level (“Elementary”, “Intermediate” or “Advanced”) based on the original writers’ judgment. An example of the same passage at different reading levels is provided in Table 1.1.\nTable 1.1: An OSE Corpus passage at different reading levels. Reading Level Example Advanced (Adv) Amsterdam still looks liberal to tourists, who were recently assured by the Labour Mayor that the city’s marijuana-selling coffee shops would stay open despite a new national law tackling drug tourism. But the Dutch capital may lose its reputation for tolerance over plans to dispatch nuisance neighbours to scum villages made from shipping containers. Intermediate (Int) To tourists, Amsterdam still seems very liberal. Recently the city’s Mayor assured them that the city’s marijuana-selling coffee shops would stay open despite a new national law to prevent drug tourism. But the Dutch capitals plans to send nuisance neighbours to scum villages made from shipping containers may damage its reputation for tolerance. Elementary (Ele) To tourists, Amsterdam still seems very liberal. Recently the city’s Mayor told them that the coffee shops that sell marijuana would stay open, although there is a new national law to stop drug tourism. But the Dutch capital has a plan to send antisocial neighbours to scum villages made from shipping containers, and so maybe now people wont think it is a liberal city any more. From Table 1.1 example, it is evident that the reading level of a specific text should be interpreted only in relation to its other versions, i.e., elementary passages are not necessarily straightforward in absolute terms, but rather less complicated than their intermediate and advanced counterparts. This affirmation holds for the OSE corpus and other widely-used readability corpora such as the Newsela corpus (Xu, Callison-Burch, and Napoles 2015), which contains newspaper articles rewritten by experts to match eleven school grade reading levels. For this reason, and because of its writer-centric perspective relying only on readability judgments formulated by the same writers who composed the passages, readability assessment is fundamentally different from the other extrinsic approaches.5 ARA can be framed as a machine learning task in which a computational model \\(m\\) is trained to predict the readability level \\(y \\in \\mathcal{Y}\\) over a set of labeled examples \\(\\mathcal{S} = (s_1, s_2, \\dots, s_n)\\) in two possible ways:\nA simple multiclass classification setting, where the model predicts the level of a single sentence \\(s\\). In this case, the model outputs a prediction \\(m(s) = \\hat y \\in \\mathcal{Y}\\). We can then minimize the categorical cross-entropy \\(H(y, \\hat y)\\) between gold and predicted labels during the training process and evaluate the model’s performances with standard classification metrics such as precision and recall. This approach is similar to the ones used for other extrinsic metrics but does not account for readability levels’ relative nature.\nA multiple-choice scenario, where the model is provided with two semantically equivalent sentences \\(s_1, s_2\\) at different readability levels (\\(s_1 \\equiv s_2, y_1 \\neq y_2\\)) and needs to predict which of the sentences has the highest readability level. In this case, which is more coherent with the relative nature of readability judgments, the model is trained to minimize the binary cross-entropy between gold and predicted labels \\(y, \\hat y \\in \\mathcal{Y}_{bin} = \\{0,1\\}\\) corresponding to the position of the more complex sentence in the pair.\nExpert annotations’ effectiveness in determining readers’ comprehension was recently questioned, as automatic readability scoring did not show a significant correlation to comprehension scores of participants, at least for the OSE Corpus (Vajjala and Lucic 2019). However, measuring if this observation holds for other corpora and extrinsic approaches is beyond this thesis’s scope.\n1.3.2 Perceived Complexity Prediction While ARA measures linguistic complexity in a context-relative and writer-centric sense, the perceived complexity prediction (PCP) approach focuses on eliciting absolute complexity judgments directly from target readers, aiming at evaluating difficulties in comprehension rather than production. This approach was pioneered by Brunato et al. (2018), who collected crowdsourced complexity ratings from native speakers for Italian and English sentences and evaluated how different structural linguistic properties contribute to human complexity perception. The use of annotators recruited on a crowdsourcing platform was intended to better grasp the layman’s perspective on linguistic complexity, as opposed to ARA expert writers. If collected properly, crowdsourced annotations were shown to be highly reliable for linguistics and computational linguistics research by the survey of Munro et al. (2010).\nBrunato et al. (2018) extracted 1200 sentences from both the newspaper sections of the Italian Universal Dependency Treebank (IUDT) (Simi, Bosco, and Montemagni 2014) and the Penn Treebank (McDonald et al. 2013), such that those are equally distributed in term of length. To collect human complexity judgments, twenty native speakers were recruited for each language on a crowdsourcing platform. Annotators had to rate each sentence’s difficulty on a Likert 7-point scale, with 1 meaning “very simple” and 7 “very complex”. Sentences were randomly shuffled and presented in groups of five per web page, with annotators being given a minimum of ten seconds to complete each page to prevent skimming. The quality of annotations was measured using the Krippendorff alpha reliability, obtaining 26% and 24% for Italian and English. Table 1.2 presents an example of English sentences labeled with multiple annotators’ perceived complexity judgments.\nTable 1.2: Sample of sentences taken from the English portion of the Perceived Complexity (PC) Corpus with complexity scores from crowdsourced annotators. Sentence A1 A2 A3 … A20 In other European markets, share prices closed sharply higher in Frankfurt and Zurich and posted moderate rises in Stockholm, Amsterdam and Milan. 4 6 7 … 1 The pound strengthened to $ 1.5795 from $ 1.5765. 2 1 2 … 1 In Connecticut, however, most state judges are appointed by the governor and approved by the state legislature. 1 3 3 … 5 When the market stabilized, he added, the firm sold the bonds and quickly paid the loans back. 2 3 3 … 3 Paribas already holds about 18.7 % of Navigation Mixte, and the acquisition of the additional 48 % would cost it about 11 billion francs under its current bid. 5 2 3 … 6 As can be expected, PC judgments show significant variability across participants since they cannot be easily framed in a relative setting. Since this work’s focus is related to a general notion of complexity, PC judgments are averaged and filtered to obtain a score reflecting the mean perception of complexity of all participants in experimental chapters. The averaged score is later treated as the gold label in a regression task, with machine learning models trained to minimize the mean square error between their predictions and gold average annotations. Another possibility, which is not explored in this thesis work, would be to consider only single participants’ judgments to model their linguistic complexity perception.\n1.3.3 Gaze Metrics Prediction Gaze data collected from human subjects during reading can provide us with useful insights from an online extrinsic complexity perspective. Patterns found in both saccades, i.e., eye movements from one location to another, and fixations, where eyes are relatively stable while fixating a specific region, were shown to be reliably linked to a multitude of linguistic factors (Demberg and Keller 2008). Because of this, a linking assumption between overt attention and mental processing can be reasonably established, and gaze metrics can be considered as proxies of cognitive effort, and thus of complexity, at various processing levels.6\nGaze metrics are widely employed in cognitive processing research because of their multiple benefits: optical eye-tracking systems are non-invasive and relatively inexpensive compared to other approaches that directly measure brain activity, such as electroencephalography (EEG) and all magnetic resonance imaging (MRI) variants. Moreover, gaze data generally have high spatial and temporal precision, limited only by sampling rates, which are generally in the order of few milliseconds. This aspect is crucial for reading research since it allows us to directly associate gaze measures to specific areas of interest (AOI, also called region), i.e., small portions of the visual input provided to participants.\nGaze data for NLP Eye-tracking data and other cognitive signals were effectively used in many NLP applications such as POS tagging (Barrett et al. 2016), sentiment analysis (Mishra, Dey, and Bhattacharyya 2017), native language identification (Berzak, Katz, and Levy 2018), and dependency parsing (Strzyz, Vilares, and Gómez-Rodríguez 2019) inter alia, often providing modest yet consistent improvements across models and tasks through the combination of gaze features and linguistic features or distributed representations.7 In the context of linguistic complexity assessment, eye-tracking data were applied to the ARA task for both monolingual and bilingual participants, obtaining meaningful results for sentence-level classification in easy and hard-to-read categories (Vasishth, Malsburg, and Engelmann 2013; Ambati, Reddy, and Steedman 2016). For example, Singh et al. (2016) first use a set of linguistic features to learn a reading times model from a set of gaze-annotated sentences and then use models’ predicted times over a second set of sentences to perform multiple-choice ARA. González-Garduño and Søgaard (2018) extend this approach in a multitask learning setting (Caruana 1997; Ruder 2017), using eye-movement prediction tasks to produce models able to predict readability levels both from a native speaker and foreign language learner perspective.\nCollecting Eye-tracking Data A typical procedure to collect gaze data for reading research, as described by Schotter (2020), usually includes the following steps:\nTextual inputs are selected and split by experiment designers, first in areas of interest directly mapped to pixels (for natural reading, usually word boundaries), then over multiple rows, and finally in screens presented to participants. This step should take into account calibration errors to determine the correct level of tolerance for off-word fixations.\nA participant is placed in a room with a display computer used to present visual inputs and a host computer used to record data from the eye-tracker setup. Optical eye-trackers use infrared light beams, which are reflected differently by different parts of the eye, to measure pupil and corneal reflection and track gaze movements at each timestep. The setup is calibrated and validated for each participant to ensure the quality of results.\nEach participant follows the on-screen instructions to complete a reading task trial while remaining at a fixed distance from the screen. A fixation report containing events (saccades, fixations, blinks) is produced for each individual on the host computer.\nFinally, a data preprocessing step is taken for each trial to identify and remove artifacts and possibly decide to reject the trial. Some examples of standard practices are the merge of fixations below 80ms due to eye jittering, the exclusion of fixations caused by track loss after blinks, and vertical drift correction (Carr et al. 2020). An AOI report containing gaze metrics grouped at AOI level can be produced.\nEye-tracking Metrics Metrics derived from the AOI report contain information about the processing phases in which subjects incur during sentence comprehension. Early gaze measures capture information about lexical access and early processing of syntactic structures, while late measures are more likely to reflect comprehension and both syntactic and semantic disambiguation (Demberg and Keller 2008). The third kind of measures, referred to as contextual following the categorization in Hollenstein and Zhang (2019), capture information from surrounding content. Table 1.3 presents a subset of metrics, spanning the three categories, that will be used in the experimental section.8 These metrics represent a minimal group spanning various stages of the reading process and are leveraged to study differences between online and offline processing among extrinsic metrics. In the experimental part, gaze scores are often averaged across participants to reduce noise in measurements and obtain a single label for each metric that can later be used as a reference in a regression setting. The average fixation probability across participants for each AOI is a value comprised in the range \\([0,1]\\) and represents the proportion of subjects that accessed the region during their first gaze pass.\nTable 1.3: Eye-tracking metrics used in this study. Type Metric Name Description Early First Fixation Duration (FFD) Duration of the first fixation over the region, including single fixations. First Pass Duration (FPD) Duration of the first pass over a region. Fixation Probability (FXP) Boolean value reflecting if the region was fixated or skipped during the first pass. Late Fixation Count (FXC) Number of total fixations over a region. Total Fixation Duration (TFD) Sum of all fixation durations over a region. Contextual Total Regression Duration (TRD) Duration of regressive saccades performed after a region’s first access and before going past it. Eye-tracking Corpora The experimental part of this thesis work leverages four widely used eye-tracking resources: the Dundee corpus (Kennedy, Hill, and Pynte 2003), the GECO corpus (Cop et al. 2017), the ZuCo corpus (Hollenstein et al. 2018), and ZuCo 2.0 (Hollenstein, Troendle, et al. 2020). There are multiple reasons behind the choice of using multiple gaze-annotated corpora for this study. First, those corpora span different domains and provide us with a better intuition of what structures are perceived as complex in different settings and by different pools of subjects. Secondly, neural-network-based complexity models used in this work greatly benefit from a broader availability of annotated data to achieve higher performances in predicting eye-tracking metrics. Finally, while all corpora relied on different procedures and instrumentation, they are all derived from very similar experimental settings (i.e., natural reading on multiple lines), and can be easily merged after an individual normalization procedure (Hollenstein and Zhang 2019). Table 1.4 presents some descriptive statistics of the four corpora.\nThe Dundee Corpus developed by Kennedy, Hill, and Pynte (2003) contains gaze data for ten native English speakers tasked with reading twenty newspaper articles from The Independent. The English section of the Dundee corpus includes 51,240 tokens in 2368 sentences. Texts were presented to subjects on a screen five lines at a time and recorded using a Dr. Bois Oculometer Eyetracker with 1 kHz monocular (right) sampling. Dundee corpus data are the oldest among selected corpora and have been extensively used in psycholinguistic research about naturalistic reading.\nThe Ghent Eye-tracking Corpus (GECO) by Cop et al. (2017) was created more recently to study eye movements of both monolingual and bilingual subjects during naturalistic reading of the novel The Mysterious Affair at Styles by Agatha Christie (2003). In the context of this work, only the monolingual portion collected from 14 native English speakers is used, comprising 56,409 tokens in 5,387 sentences. Eye movements were recorded with an EyeLink 1000 system with 1 kHz binocular sampling (only right eye movements were considered), and the text was presented one paragraph at a time.\nThe Zurich Cognitive Language Processing Corpus (ZuCo) by Hollenstein et al. (2018) is a dataset including both eye-tracking and EEG measurements collected simultaneously during both natural and task-oriented reading. The corpus contains 1100 English sentences from the Stanford Sentiment Treebank (Socher et al. 2013) and the Wikipedia dump used in Culotta, McCallum, and Betz (2006) with gaze data for 12 adult native speakers. Only the first two portions are used for the present work since they contain natural reading data, totalizing 700 sentences and 13,630 tokens. The text was presented on-screen one sentence at a time, and data were collected with an EyeLink 1000 as for GECO.\nZuCo 2.0 is an extension of ZuCo, including 739 sentences extracted from the Wikipedia corpus by Culotta, McCallum, and Betz (2006). Only the 349 sentences for which natural reading data were collected are used, and the 100 duplicates shared with ZuCo to evaluate differences in setup and participants are removed. Data were collected from 18 native English speakers using an EyeLink 1000 Plus with 500 kHz sampling.\nTable 1.4: Descriptive statistics of eye-tracking corpora. Dundee GECO ZuCo ZuCo 2.0 Total domain(s) news literature movie reviews, Wiki articles Wiki articles # of sentences 2368 5387 700 349 8804 mean sent. length 21.64 10.47 19.47 19.51 17.77 # of tokens 51240 56409 13630 6810 128089 unique token types 9928 6155 4650 2521 16320 mean token length 4.88 4.6 5.05 5.01 4.89 mean fix. duration 200 210 117 117 161 mean gaze duration 280 234 139 134 197 Tokens are obtained using whitespace tokenization, which is the same approach used to perform gaze annotations across all eye-tracking corpora. Mean sentence length is expressed in number of tokens, and the number of unique types is computed as the size of the vocabulary after removing punctuation from all tokens. Approximately 128,000 tokens annotated with gaze recordings from multiple participants were used in the experiments of Chapters 4 and 5, while only GECO was used for the analysis of Chapter 3. Similarly to the PCP task, scores were averaged across subjects to reduce noise and obtain general estimates: in particular, reading times that were missing due to skipping were considered as having the lowest duration across annotators, which is a practice commonly used in literature. Again, considering individual participants’ scores is deemed attractive in a personalization perspective but far beyond this work’s scope.\n1.4 Garden-path Sentences Figure 1.2: Syntax trees for the initial and complete parse of garden-path example (1). Garden-path sentences, named from the expression “leading down the garden path” implying deception, are grammatically correct sentences that create a momentarily ambiguous interpretation in readers. The initial interpretation is later falsified by words encountered during sequential reading, becoming a significant source of processing difficulties. For this reason, garden-path constructions are used to evaluate models of linguistic complexity in the experiments of Chapter 5. Consider the following recent headline by the newspaper The Guardian:9\nVaccine trials halted after patient fell ill restart. Readers exposed to (1) tend to initially prefer the interpretation in which halted acts as the main verb of the sentence in simple past, i.e., “Vaccine trials halted after patient fell ill.” is interpreted as a well-formed and semantically meaningful sentence. When the verb restart is reached, it suddenly becomes evident that the original parse would lead to an ungrammatical sentence, and a reanalysis requiring nontrivial cognitive processing is triggered. In conclusion, one understands that halted is used as a passive participle, and Vaccine trials are the subordinate clause’s direct object, as shown in Figure 1.2. We can rephrase the sentence with minimal changes to make it unambiguous:\nVaccine trials that were halted after patient fell ill restart. The choice for the initial parse can be explained in terms of frequency of occurrence: subject-verb-object sentences are encountered much more frequently than ones containing reduced relatives in everyday settings, making the first parse more likely (Fine et al. 2013). We refer to the verb causing the reanalysis as disambiguator, and to the difference in cognitive processing between (1) and (2), measured using proxies such as gaze metrics, as garden-path effect (Bever 1970).\nSchijndel and Linzen (2020) present two families of cognitive processing theories trying to motivate the underlying difficulties in which humans incur with garden-path sentences:\nTwo-stage accounts assume that readers consider only one or a subset of possible parses for each sentence that it is reading (Gibson 1991; Jurafsky 1996), and processing difficulties arise as a consequence of the reanalysis process need to reconstruct parses that were initially disregarded or not considered (Frazier and Fodor 1978).\nOne-stage accounts such as surprisal theory (Hale 2001; Levy 2008) instead consider difficulties produced by garden paths as the products of a single processing mechanism. Dispreferred parses are not discarded, but rather associated with a lower probability compared to that of likely ones: “processing difficulty on every word in the sentence, including the disambiguating words in garden-path sentences, arises from the extent to which the word shifts the reader’s subjective probability distribution over possible parses” (Schijndel and Linzen 2020).\nThere are multiple types of garden-path sentences, usually categorized based on their respective syntactic ambiguities (Frazier 1978). In this work, two classic garden-path families are studied in three different settings using examples taken from Futrell et al. (2019). The first type is the MV/RR ambiguity presented in example (1), and repeated in (3a):\nThe woman brought the sandwich fell in the dining room. [RED., AMBIG.] The woman who was brought the sandwich fell in the dining room. [UNRED., AMBIG.] The woman given the sandwich fell in the dining room. [RED., UNAMBIG.] The woman who was given the sandwich fell in the dining room. [UNRED., UNAMBIG.] The label MV/RR indicates that brought can be initially parsed either as the main verb (MV) in the past tense of the clause or as a passive participle introducing a reduced relative (RR) clause, which postmodifies the subject. It is possible to rewrite the sentence by changing the ambiguous verb to an equivalent one having different forms for simple past and past participle (such as gave vs. given). In this case, we expect that the difference in cognitive processing for the disambiguator fell between the reduced (3c) and the unreduced (3d) version is smaller since the ambiguity is ruled out from the beginning.\nThe second type of ambiguity is the NP/Z ambiguity presented in (4a):\nAs the criminal shot the woman yelled at the top of her lungs. [TRANS., NO COMMA] As the criminal fled the woman yelled at the top of her lungs. [INTRANS., NO COMMA] As the criminal shot, the woman yelled at the top of her lungs. [TRANS., COMMA] As the criminal fled, the woman yelled at the top of her lungs. [INTRANS., COMMA] The label NP/Z is used to indicate that the transitive verb shot can initially be understood to have either have a noun phrase (NP) object like the woman or a zero (Z), i.e., null object if used intransitively as it is the case for (4a). The sentence can be rewritten by substituting the transitive verb generating the ambiguity with an intransitive one, e.g., replacing shot with fled in (4b), by adding a disambiguating comma to force the null-object parse as in (4c), or by doing both as in (4d). We expect that the cognitive processing difference for the disambiguator yelled between the ambiguous (4a) and the unambiguous (4b) is smaller since the ambiguity is ruled out from the beginning.\nAs an additional NP/Z setting evaluation, consider the case in which an overt object is added to the verb introducing the ambiguity:\nAs the criminal shot the woman yelled at the top of her lungs. [NO OBJ., NO COMMA] As the criminal shot his gun the woman yelled at the top of her lungs. [OBJ., NO COMMA] As the criminal shot, the woman yelled at the top of her lungs. [NO OBJ., COMMA] As the criminal shot his gun, the woman yelled at the top of her lungs. [OBJ., COMMA] Again, we expect that the difference in cognitive processing for yelled is higher in the non-object pair (5a)-(5c), where the first item is a garden-path sentence, rather than in the pair (5b)-(5d) where both sentences are unambiguous.\nGaze metrics and Garden-path Sentences As can be intuitively assumed, garden-path effects are reflected in gaze metrics collected during natural reading. Multiple studies have focused on quantifying the difference between garden-path sentences and their unambiguous counterparts on reading times in human subjects. Sturt, Pickering, and Crocker (1999) found a massive delay of 152ms for each word in the disambiguating region of NP/Z sentences. Grodner et al. (2003) estimate an average delay of 64ms over the disambiguating region for NP/Z constructs using 53 college students’ reading times over a set of 20 ambiguous sentences. More recently, Prasad and Linzen (2019b) recorded eye measurements for 224 participants recruited through Amazon Mechanical Turk on the same set of NP/Z sentences as Grodner et al. (2003), finding a much lower average delay of 28ms, and suggesting an overestimation in previous studies due to small sample size and publication contingency to significant results. Prasad and Linzen (2019a) collected self-paced reading times from 73 participants recruited on the Prolific Academic crowdsourcing platform and measured an average delay of 22ms over the disambiguating region for MV/RR constructs.\nGiven the high variability in results across studies, it can be hypothesized that the way in which stimuli were presented to subjects plays a significant role in determining the magnitude of garden-path effects (Van Schijndel and Linzen 2018). For example, a sentence presented word-by-word to subjects may yield more ecologically valid reading times estimates than a sentence presented region-by-region. Another problematic factor involves constraining the impact of garden-path effects to the disambiguating region: first, because parafoveal preview effects may slightly anticipate the start of the effect (Schotter, Angele, and Rayner 2012; Schotter 2018); and second, because due to spillover (Mitchell 1984), a phenomenon in which the surprisal of a word influences the reading times for itself and at least three subsequent words (Smith and Levy 2013), reading times of the disambiguating region are influenced by preceding words, and influence subsequent ones, spreading the garden-path effect on a much broader context. For this reason, eye-tracking metrics are studied for all sentence regions in the experiments of Chapter 5.\nReferences Ambati, Bharat Ram, Siva Reddy, and Mark Steedman. 2016. “Assessing Relative Sentence Complexity Using an Incremental CCG Parser.” In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 1051–7. San Diego, California: Association for Computational Linguistics. https://doi.org/10.18653/v1/N16-1120.\nBarrett, Maria, Joachim Bingel, Frank Keller, and Anders Søgaard. 2016. “Weakly Supervised Part-of-Speech Tagging Using Eye-Tracking Data.” In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), 579–84. Berlin, Germany: Association for Computational Linguistics. https://doi.org/10.18653/v1/P16-2094.\nBerruto, Gaetano, and Massimo Simone Cerruti. 2011. La Linguistica. Un Corso Introduttivo. De Agostini.\nBerzak, Yevgeni, Boris Katz, and Roger Levy. 2018. “Assessing Language Proficiency from Eye Movements in Reading.” In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), 1986–96. New Orleans, Louisiana: Association for Computational Linguistics. https://doi.org/10.18653/v1/N18-1180.\nBever, Thomas G. 1970. “The Cognitive Basis for Linguistic Structures.” Cognition and the Development of Language. Wiley.\nBrunato, Dominique, Andrea Cimino, Felice Dell’Orletta, Giulia Venturi, and Simonetta Montemagni. 2020. “Profiling-UD: A Tool for Linguistic Profiling of Texts.” In Proceedings of the 12th Language Resources and Evaluation Conference, 7145–51. Marseille, France: European Language Resources Association. https://www.aclweb.org/anthology/2020.lrec-1.883.\nBrunato, Dominique, Lorenzo De Mattei, Felice Dell’Orletta, Benedetta Iavarone, and Giulia Venturi. 2018. “Is This Sentence Difficult? Do You Agree?” In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, 2690–9. Brussels, Belgium: Association for Computational Linguistics. https://doi.org/10.18653/v1/D18-1289.\nCangelosi, Angelo, and Huck Turner. 2002. “L’emergere Del Linguaggio.” Scienze Della Mente.\nCarr, Jon W, Valentina N Pescuma, Michele Furlan, Maria Ktori, and Davide Crepaldi. 2020. “Algorithms for the Automated Correction of Vertical Drift in Eye Tracking Data.” OSF Preprints, June. osf.io/jg3nc.\nCaruana, Rich. 1997. “Multitask Learning.” Machine Learning 28: 41–75. https://www.cs.utexas.edu/~kuipers/readings/Caruana-mlj-97.pdf.\nChristie, Agatha. 2003. The Mysterious Affair at Styles: A Detective Story. Modern Library.\nCop, Uschi, Nicolas Dirix, Denis Drieghe, and Wouter Duyck. 2017. “Presenting Geco: An Eyetracking Corpus of Monolingual and Bilingual Sentence Reading.” Behavior Research Methods 49 (2). Springer: 602–15.\nCulotta, Aron, Andrew McCallum, and Jonathan Betz. 2006. “Integrating Probabilistic Extraction Models and Data Mining to Discover Relations and Patterns in Text.” In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference, 296–303. New York City, USA: Association for Computational Linguistics. https://www.aclweb.org/anthology/N06-1038.\nDay, Matthew. 2004. “Religion, Off-Line Cognition and the Extended Mind.” Journal of Cognition and Culture 4 (1). Brill: 101–21.\nDemberg, Vera, and Frank Keller. 2008. “Data from Eye-Tracking Corpora as Evidence for Theories of Syntactic Processing Complexity.” Cognition 109 (2). Elsevier: 193–210.\nEdmonds, Bruce M. 1999. “Syntactic Measures of Complexity.” PhD thesis, University of Manchester Manchester, UK.\nFine, Alex B, T Florian Jaeger, Thomas A Farmer, and Ting Qian. 2013. “Rapid Expectation Adaptation During Syntactic Comprehension.” PloS One 8 (10). Public Library of Science: e77661.\nFrazier, Lyn. 1978. “On Comprehending Sentences: Syntactic Parsing Strategies.” PhD thesis, University of Connecticut.\nFrazier, Lyn, and Janet Dean Fodor. 1978. “The Sausage Machine: A New Two-Stage Parsing Model.” Cognition 6 (4). Elsevier: 291–325.\nFutrell, Richard, Edward Gibson, and Roger P Levy. 2020. “Lossy-Context Surprisal: An Information-Theoretic Model of Memory Effects in Sentence Processing.” Cognitive Science 44 (3). Wiley Online Library: e12814.\nFutrell, Richard, Ethan Wilcox, Takashi Morita, Peng Qian, Miguel Ballesteros, and Roger Levy. 2019. “Neural Language Models as Psycholinguistic Subjects: Representations of Syntactic State.” In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), 32–42. Minneapolis, Minnesota: Association for Computational Linguistics. https://doi.org/10.18653/v1/N19-1004.\nGibson, Edward. 1991. “A Computational Theory of Human Linguistic Processing: Memory Limitations and Processing Breakdown.” PhD thesis, Pittsburgh, PA: Carnegie Mellon University.\nGibson, Edward. 1998. “Linguistic Complexity: Locality of Syntactic Dependencies.” Cognition 68 (1). Elsevier: 1–76.\nGibson, Edward. 2000. “The Dependency Locality Theory: A Distance-Based Theory of Linguistic Complexity.” Image, Language, Brain 2000: 95–126.\nGonzález-Garduño, Ana Valeria, and Anders Søgaard. 2018. “Learning to Predict Readability Using Eye-Movement Data from Natives and Learners.” AAAI Conference on Artificial Intelligence.\nGoodman, Joshua. 2001. “A Bit of Progress in Language Modeling.” arXiv Preprint Cs/0108005.\nGrodner, Daniel, Edward Gibson, Vered Argaman, and Maria Babyonyshev. 2003. “Against Repair-Based Reanalysis in Sentence Comprehension.” Journal of Psycholinguistic Research 32 (2). Springer: 141–66.\nHale, John. 2001. “A Probabilistic Earley Parser as a Psycholinguistic Model.” In Second Meeting of the North American Chapter of the Association for Computational Linguistics.\nHale, John. 2016. “Information-Theoretical Complexity Metrics.” Language and Linguistics Compass 10 (9). Wiley Online Library: 397–412.\nHauser, Marc D, Noam Chomsky, and W Tecumseh Fitch. 2002. “The Faculty of Language: What Is It, Who Has It, and How Did It Evolve?” Science 298 (5598). American Association for the Advancement of Science: 1569–79.\nHollenstein, Nora, Jonathan Rotsztejn, Marius Troendle, Andreas Pedroni, Ce Zhang, and Nicolas Langer. 2018. “ZuCo, a Simultaneous Eeg and Eye-Tracking Resource for Natural Sentence Reading.” Scientific Data 5 (1). Nature Publishing Group: 1–13.\nHollenstein, Nora, Marius Troendle, Ce Zhang, and Nicolas Langer. 2020. “ZuCo 2.0: A Dataset of Physiological Recordings During Natural Reading and Annotation.” In Proceedings of the 12th Language Resources and Evaluation Conference, 138–46. Marseille, France: European Language Resources Association. https://www.aclweb.org/anthology/2020.lrec-1.18.\nHollenstein, Nora, and Ce Zhang. 2019. “Entity Recognition at First Sight: Improving NER with Eye Movement Information.” In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), 1–10. Minneapolis, Minnesota: Association for Computational Linguistics. https://doi.org/10.18653/v1/N19-1001.\nIverson, Jana M, and Esther Thelen. 1999. “Hand, Mouth and Brain. The Dynamic Emergence of Speech and Gesture.” Journal of Consciousness Studies 6 (11-12). Imprint Academic: 19–40.\nJurafsky, Daniel. 1996. “A Probabilistic Model of Lexical and Syntactic Access and Disambiguation.” Cognitive Science 20 (2). Wiley Online Library: 137–94.\nKennedy, Alan, Robin Hill, and Joël Pynte. 2003. “The Dundee Corpus.” In Proceedings of the 12th European Conference on Eye Movement.\nKusters, Wouter. 2003. “Linguistic Complexity.” PhD thesis, Netherlands Graduate School of Linguistics.\nKusters, Wouter. 2008. “Complexity in Linguistic Theory, Language Learning and Language Change.” In Language Complexity: Typology, Contact, Change, 3–22. John Benjamins Amsterdam, The Netherlands.\nLevy, Roger. 2008. “Expectation-Based Syntactic Comprehension.” Cognition 106 (3). Elsevier: 1126–77.\nMcDonald, Ryan, Joakim Nivre, Yvonne Quirmbach-Brundage, Yoav Goldberg, Dipanjan Das, Kuzman Ganchev, Keith Hall, et al. 2013. “Universal Dependency Annotation for Multilingual Parsing.” In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), 92–97. Sofia, Bulgaria: Association for Computational Linguistics. https://www.aclweb.org/anthology/P13-2017.\nMcWhorter, John H. 2001. “The Worlds Simplest Grammars Are Creole Grammars.” Linguistic Typology 5 (2-3). De Gruyter Mouton: 125–66.\nMeyer, Bonnie JF, and G Elizabeth Rice. 1992. “12 Prose Processing in Adulthood: The Text, the Reader, and the Task.” Everyday Cognition in Adulthood and Late Life. Cambridge Univ Pr, 157.\nMiestamo, Matti. 2004. “On the Feasibility of Complexity Metrics.” In FinEst Linguistics, Proceedings of the Annual Finnish and Estonian Conference of Linguistics, 11–26. Tallin, Finland.\nMiestamo, Matti. 2008. “Grammatical Complexity in a Cross-Linguistic Perspective.” In Language Complexity: Typology, Contact, Change, 41. John Benjamins Amsterdam, The Netherlands.\nMishra, Abhijit, Kuntal Dey, and Pushpak Bhattacharyya. 2017. “Learning Cognitive Features from Gaze Data for Sentiment and Sarcasm Classification Using Convolutional Neural Network.” In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 377–87. Vancouver, Canada: Association for Computational Linguistics. https://doi.org/10.18653/v1/P17-1035.\nMitchell, Don C. 1984. “An Evaluation of Subject-Paced Reading Tasks and Other Methods for Investigating Immediate Processes in Reading.” New Methods in Reading Comprehension Research, 69–89.\nMunro, Robert, Steven Bethard, Victor Kuperman, Vicky Tzuyin Lai, Robin Melnick, Christopher Potts, Tyler Schnoebelen, and Harry Tily. 2010. “Crowdsourcing and Language Studies: The New Generation of Linguistic Data.” In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk, 122–30. Los Angeles: Association for Computational Linguistics. https://www.aclweb.org/anthology/W10-0719.\nNivre, Joakim, Marie-Catherine de Marneffe, Filip Ginter, Yoav Goldberg, Jan Hajič, Christopher D. Manning, Ryan McDonald, et al. 2016. “Universal Dependencies V1: A Multilingual Treebank Collection.” In Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC’16), 1659–66. Portorož, Slovenia: European Language Resources Association (ELRA). https://www.aclweb.org/anthology/L16-1262.\nPrasad, Grusha, and Tal Linzen. 2019a. “Do Self-Paced Reading Studies Provide Evidence for Rapid Syntactic Adaptation?” PsyArXiv Pre-Print. https://tallinzen.net/media/papers/prasad_linzen_2019_adaptation.pdf.\nPrasad, Grusha, and Tal Linzen. 2019b. “How Much Harder Are Hard Garden-Path Sentences Than Easy Ones?” OSF Preprint syh3j. https://osf.io/syh3j/.\nRuder, Sebastian. 2017. “An Overview of Multi-Task Learning in Deep Neural Networks.” ArXiv Pre-Print 1706.05098. https://arxiv.org/abs/1706.05098.\nSchijndel, Marten van, and Tal Linzen. 2020. “Single-Stage Prediction Models Do Not Explain the Magnitude of Syntactic Disambiguation Difficulty.” PsyArXiv Pre-Print sgbqy. https://psyarxiv.com/sgbqy/.\nSchotter, Elizabeth R. 2018. “Reading Ahead by Hedging Our Bets on Seeing the Future: Eye Tracking and Electrophysiology Evidence for Parafoveal Lexical Processing and Saccadic Control by Partial Word Recognition.” In Psychology of Learning and Motivation, 68:263–98. Elsevier.\nSchotter, Elizabeth R. 2020. Eye Tracking for Cognitive Science. SISSA Course.\nSchotter, Elizabeth R, Bernhard Angele, and Keith Rayner. 2012. “Parafoveal Processing in Reading.” Attention, Perception, \u0026amp; Psychophysics 74 (1). Springer: 5–35.\nSimi, Maria, Cristina Bosco, and Simonetta Montemagni. 2014. “Less Is More? Towards a Reduced Inventory of Categories for Training a Parser for the Italian Stanford Dependencies.” In Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC’14), 83–90. Reykjavik, Iceland: European Language Resources Association (ELRA). http://www.lrec-conf.org/proceedings/lrec2014/pdf/818_Paper.pdf.\nSingh, Abhinav Deep, Poojan Mehta, Samar Husain, and Rajkumar Rajakrishnan. 2016. “Quantifying Sentence Complexity Based on Eye-Tracking Measures.” In Proceedings of the Workshop on Computational Linguistics for Linguistic Complexity (CL4LC), 202–12. Osaka, Japan: The COLING 2016 Organizing Committee. https://www.aclweb.org/anthology/W16-4123.\nSinnemäki, Kaius. 2011. “Language Universals and Linguistic Complexity: Three Case Studies in Core Argument Marking.” PhD thesis, University of Helsinki.\nSmith, Nathaniel J, and Roger Levy. 2013. “The Effect of Word Predictability on Reading Time Is Logarithmic.” Cognition 128 (3). Elsevier: 302–19.\nSocher, Richard, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. 2013. “Recursive Deep Models for Semantic Compositionality over a Sentiment Treebank.” In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, 1631–42. Seattle, Washington, USA: Association for Computational Linguistics. https://www.aclweb.org/anthology/D13-1170.\nStraka, Milan, Jan Hajič, and Jana Straková. 2016. “UDPipe: Trainable Pipeline for Processing CoNLL-U Files Performing Tokenization, Morphological Analysis, POS Tagging and Parsing.” In Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC’16), 4290–7. Portorož, Slovenia: European Language Resources Association (ELRA). https://www.aclweb.org/anthology/L16-1680.\nStrzyz, Michalina, David Vilares, and Carlos Gómez-Rodríguez. 2019. “Towards Making a Dependency Parser See.” In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (Emnlp-Ijcnlp), 1500–1506. Hong Kong, China: Association for Computational Linguistics. https://doi.org/10.18653/v1/D19-1160.\nSturt, Patrick, Martin J Pickering, and Matthew W Crocker. 1999. “Structural Change and Reanalysis Difficulty in Language Comprehension.” Journal of Memory and Language 40 (1). Elsevier: 136–50.\nTaylor, Wilson L. 1953. “‘Cloze Procedure’: A New Tool for Measuring Readability.” Journalism Quarterly 30 (4). SAGE Publications Sage CA: Los Angeles, CA: 415–33.\nVajjala, Sowmya, and Ivana Lucic. 2019. “On Understanding the Relation Between Expert Annotations of Text Readability and Target Reader Comprehension.” In Proceedings of the Fourteenth Workshop on Innovative Use of Nlp for Building Educational Applications, 349–59. Florence, Italy: Association for Computational Linguistics. https://doi.org/10.18653/v1/W19-4437.\nVajjala, Sowmya, and Ivana Lučić. 2018. “OneStopEnglish Corpus: A New Corpus for Automatic Readability Assessment and Text Simplification.” In Proceedings of the Thirteenth Workshop on Innovative Use of NLP for Building Educational Applications, 297–304. New Orleans, Louisiana: Association for Computational Linguistics. https://doi.org/10.18653/v1/W18-0535.\nVan Schijndel, Marten, and Tal Linzen. 2018. “Modeling Garden Path Effects Without Explicit Hierarchical Syntax.” In Proceedings of the 40th Annual Conference of the Cognitive Science Society, 2600–2605.\nVasishth, Shravan, Titus von der Malsburg, and Felix Engelmann. 2013. “What Eye Movements Can Tell Us About Sentence Comprehension.” Cognitive Science 4 2. Wiley interdisciplinary reviews: 125–34. https://onlinelibrary.wiley.com/doi/full/10.1002/wcs.1209.\nVoghera, Miriam. 2001. “Riflessioni Su Semplificazione, Complessità E Modalità Di Trasmissione: Sintassi E Semantica.” Scritto E Parlato. Metodi, Testi E Contesti. Aracne, 65–78.\nXu, Wei, Chris Callison-Burch, and Courtney Napoles. 2015. “Problems in Current Text Simplification Research: New Data Can Help.” Transactions of the Association for Computational Linguistics 3: 283–97. https://doi.org/10.1162/tacl_a_00139.\nAvailable at http://linguistic-profiling.italianlp.it↩\nThis motivates the previous claim about the interdependence of intrinsic and extrinsic approaches. See Section 2.1 of Martinc, Pollak, and Robnik-Sikonja (2019) for an overview of the most popular metrics for English.↩\nThis is an admittedly simplistic reduction, given the importance of parafoveal processing in reading (Schotter, Angele, and Rayner 2012; Schotter 2018)↩\nSee Collins-Thompson (2014) for a thorough review of ARA approaches.↩\nSee Rayner (1998) for a comprehensive survey on findings related to eye-tracking research.↩\nSee Hollenstein, Barrett, and Beinborn (2020) for an exhaustive overview of current approaches and best practices.↩\nAppendix B contains information about deriving metric values for all corpora.↩\nhttps://twitter.com/drswissmiss/status/1304856856649756673↩\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"5214d2a6a0f3dc5ede5a441d0880af14","permalink":"https://gsarti.com/msc-thesis/chap-ling-comp/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/msc-thesis/chap-ling-comp/","section":"msc-thesis","summary":"\u003c!DOCTYPE html\u003e 1 Linguistic Complexity | Interpreting Neural Language Models for Linguistic Complexity Assessment Introduction 1 Linguistic Complexity 1.1 Categorizing Linguistic Complexity Measures 1.2 Intrinsic Perspective 1.2.1 Structural Linguistic Complexity 1.2.2 Language Modeling Surprisal 1.3 Extrinsic Perspective 1.3.1 Automatic Readability Assessment 1.3.2 Perceived Complexity Prediction 1.3.3 Gaze Metrics Prediction 1.4 Garden-path Sentences 2 Models of Linguistic Complexity 2.1 Desiderata for Models of Linguistic Complexity 2.2 Neural Language Models: Unsupervised Multitask Learners 2.","tags":null,"title":"","type":"msc-thesis"},{"authors":null,"categories":null,"content":"\u003c!DOCTYPE html\u003e 2 Models of Linguistic Complexity | Interpreting Neural Language Models for Linguistic Complexity Assessment Introduction 1 Linguistic Complexity 1.1 Categorizing Linguistic Complexity Measures 1.2 Intrinsic Perspective 1.2.1 Structural Linguistic Complexity 1.2.2 Language Modeling Surprisal 1.3 Extrinsic Perspective 1.3.1 Automatic Readability Assessment 1.3.2 Perceived Complexity Prediction 1.3.3 Gaze Metrics Prediction 1.4 Garden-path Sentences 2 Models of Linguistic Complexity 2.1 Desiderata for Models of Linguistic Complexity 2.2 Neural Language Models: Unsupervised Multitask Learners 2.2.1 Emergent Linguistic Structures in Neural Language Models 2.3 Analyzing Neural Models of Complexity 2.3.1 Probing classifiers 2.3.2 Representational Similarity Analysis 2.3.3 Projection-Weighted Canonical Correlation Analysis 3 Complexity Phenomena in Linguistic Annotations and Language Models 3.1 Data and Preprocessing 3.2 Analysis of Linguistic Phenomena 3.2.1 Linguistic Phenomena in Length-controlled Bins 3.3 Modeling Online and Offline Linguistic Complexity 3.3.1 Modeling Complexity in Length-controlled Bins 3.4 Probing Linguistic Phenomena in ALBERT Representations 3.5 Summary 4 Representational Similarity in Models of Complexity 4.1 Knowledge-driven Requirements for Learning Models 4.2 Experimentsl Evaluation 4.2.1 Data 4.2.2 Inter-model Representational Similarity 4.2.3 Intra-model Representational Similarity 4.3 Summary 5 Gaze-informed Models for Cognitive Processing Prediction 5.1 Experimental Setup 5.2 Experimental Evaluation 5.2.1 Estimating Magnitudes of Garden-path Delays 5.2.2 Predicting Delays with Surprisal and Gaze Metrics 5.3 Summary Conclusion Broader Impact and Ethical Perspectives Future Directions Appendix A Linguistic Features A.1 Raw Text Properties and Lexical Variety A.2 Morpho-syntacting Information A.3 Verbal Predicate Structure A.4 Global and Local Parsed Tree Structures A.5 Syntactic Relations A.6 Subordination Phenomena B Precisions on Eye-tracking Metrics and Preprocessing C Multi-task Token-level Regression for Gaze Metrics Prediction D Intra-model Similarity for All Models E Gaze Metrics Predictions for Garden Path Sentences F Reproducibility and Environmental Impact References Back to my website Interpreting Neural Language Models\nfor Linguistic Complexity Assessment 2 Models of Linguistic Complexity Standard linguistic complexity studies analyze complexity annotations produced by human subjects to evaluate how specific language structures influence our perception of complexity under various viewpoints. For example, one can derive insights about early cognitive processing by looking at early gaze metrics, like first pass duration and first fixation duration, or study language comprehension by evaluating perceived complexity annotations. These approaches rely on a single implicit assumption: that complexity annotations contain enough information to reflect the input’s underlying complexity properties appropriately. Without this premise, there would be a complete disconnect between human subjective perception, as reflected by annotations and linguistic structures. Given the ever-growing compelling evidence derived from carefully-planned complexity research, I argue that this is a relatively safe assumption to be made.\nThis work instead adopts a modeling-driven approach for the study of linguistic complexity. Annotations produced by human subjects still play a fundamental role in this context. However, instead of acting as the main subject of analysis, they are used as a source of distant supervision to create computational models of linguistic complexity. More specifically, machine learning models are trained to predict complexity annotation from raw input text by minimizing a task-specific loss function. The learning step here is fundamental, given the connection mentioned above between linguistic complexity and knowledge acquisition. After the training process, human annotations are put aside, and the model itself is studied as a complexity-sensitive subject: in particular, this study focuses on how the information encoded in the parameters of complexity-trained models is related to structural linguistic properties (Chapter 3), how this information differs when models are exposed to different complexity perspectives during training (Chapter 4) and finally how the encoded knowledge affects models’ generalization capabilities over unseen constructs (Chapter 5).\nWhile this approach still relies on the annotation pertinence assumption stated above, it requires making a second, stronger hypothesis: that models employed can grasp a significant portion of the relations subsisting between language structures and complexity perspectives. This assumption can be further declined in two requirements. First, from a conceptual point-of-view, we must ensure that the model architecture is endowed with meaningful inductive biases concerning what is currently known about linguistic complexity. This includes having sufficient approximation capabilities to capture linguistic complexity phenomena, which are likely to be highly-nonlinear functions of the input. From a functional perspective, then, we should confirm that the quality of model predictions is sufficiently close to human-produced annotations to make their production mechanisms worth investigating.\nThis chapter justifies the selected modeling approach and introduces models later employed in complexity assessment experiments. Section 2.1 discusses the conceptual requirements for linguistic complexity modeling and motivates the choice of pretrained neural language models as primary subjects of this thesis work. Section 2.2 presents the architectures used in experimental sections and their desirable properties regarding the encoding of linguistic structures in latent representations. Finally, Section 2.3 presents the challenge of interpreting NLM’s representations and behaviors and introduces various interpretability approaches used throughout this study.\n2.1 Desiderata for Models of Linguistic Complexity From the in-depth analysis of Chapter 1, we can distill some general desiderata for an idealized LCA model \\(M^*\\). From a linguistic perspective:\n\\(M^*\\) should distinguish between lexical forms and be informed about their probability of occurrence. This is a basic (although fundamental) step given the importance of words’ variety and frequency in determining our perception of complexity.\n\\(M^*\\) should be aware of syntactic structures and sensitive to their properties. As we saw with garden-path sentences, atypical or ambiguous syntax constructs are among the most prominent factors for determining the magnitude of processing difficulties. An ideal model should map complex syntactic constructs to higher complexity scores and discriminate potentially ambiguous or problematic structures from regular ones, even when changes in the form are minimal (e.g., when a single comma is missing).\n\\(M^*\\) should capture semantic information and relations between entities. Ideally, this means the ability to frame agents, patients, and actions in a semantic context and evaluate how likely or typical the latter is. For example, semantically unrelated entities occurring together in a sentence should produce an increase in processing difficulties. This includes the ability to disambiguate polysemic terms (e.g., “fly” verb vs. noun) given the surrounding context.\nThen, from a technical standpoint:\n\\(M^*\\) should not rely on hand-crafted features to represent language. This is an implicit requirement since this study aims to analyze how the model autonomously learns to represent language in its parameters while simultaneously encoding information about its complexity. Chapter 3 presents how complexity models with hand-crafted features compare to those selected for the study.\n\\(M^*\\) should not rely too heavily on labeled data. Complexity datasets presented in Chapter 1 are usually composed of a few thousand labeled examples. While this may seem a lot to our eyes, a language model may require a lot more information to achieve sufficient generalization capabilities. A viable option in this context, as we will see with NLMs, is to prime models with general linguistic knowledge through an unsupervised pretraining procedure before training them on complexity-related tasks.\n\\(M^*\\) should be sufficiently interpretable. Ideally, we would like to draw direct causal relations from input properties to complexity prediction in a consistent way across complexity perspectives. More realistically, we need at least to find coherent patterns between the model’s inputs and its predictive behaviors.\nMost standard modeling approaches fail to encompass even a small subset of those non-trivial requirements. For example, one can consider modeling complexity properties with static word representations (Turian, Ratinov, and Bengio 2010) such as Word2Vec or GloVe embeddings (Mikolov et al. 2013; Pennington, Socher, and Manning 2014). In these approaches, feature vectors representing words are learned by a neural network through a pretraining procedure to model word co-occurrences. While these approaches were shown to capture a significant amount of semantic information while reducing the dependence on labeled data thanks to pretraining, static word embeddings generally yield modest results when employed for syntactic predictions (Andreas and Klein 2014). Moreover, since the model learns a direct mapping \\(f: t_i \\rightarrow \\textbf{v}_i\\) from lexical forms to vectorial representations, polysemic terms are reduced to single context-independent representation, and contextual information that often plays a crucial role in determining complexity is mixed and diluted.\nAmong more sophisticated modeling approaches for representing language, I argue that modern neural language models (NLMs) are the approaches that yield a better match for the requirements stated above. These models consist of multi-layer neural networks (Goodfellow et al. 2016) pretrained using standard language modeling or masked language modeling training objectives to produce contextualized word embeddings, which were shown to be very effective in downstream syntactic and semantic tasks (Peters et al. 2018) even with relatively few labeled examples. Moreover, being language models, NLMs predict a probability distribution over their vocabulary at each step, enabling us to compute information-theoretic metrics such as surprisal that we saw being conceptually close to one-stage cognitive processing accounts. Finally, their high parameter counts and the presence of self-attention mechanisms (Bahdanau, Cho, and Bengio 2015; Vaswani et al. 2017) as learned weighting functions suggests that NLMs might be capable of learning to approximate highly nonlinear functions effectively.\nThe most significant downside of NLMs in the context of our analysis is their opaqueness. As for most neural networks, the nonlinear multi-layer structure that characterizes NLMs makes them incredibly valid function approximators. At the same time, though, it hinders our efforts in interpreting their behaviors (Samek et al. 2019). Because of this fact, in recent years, we witnessed a surge in approaches trying to “open the black box” of neural networks by using various techniques borrowed from information theory (Shwartz-Ziv and Tishby 2017) and cognitive science (Kriegeskorte, Mur, and Bandettini 2008). Given the wide availability of these approaches, this work joins the choir of interpretability researchers and argues that studying how such performant models encode their knowledge about language complexity is still a matter of interest and worth exploring. In the next section, the architecture and training process of NLMs will be formalized, and their properties will be described in detail.\n2.2 Neural Language Models: Unsupervised Multitask Learners The objective of natural language processing applications such as summarization, machine translation, and dialogue generation is to produce text that is both fluent and contextually accurate. As we saw in Chapter 1, a text’s fluency can also be used as a significant factor in determining its complexity from a linguistic viewpoint. A possible approach to establishing a sentence’s fluency is to rely on relative frequency estimates for words in large corpora. Consider a sentence \\(s\\) and a large corpus \\(\\mathcal{C}\\). We can estimate its probability of occurrence in natural language as:\n\\[\\begin{equation} P(s) = \\frac{\\text{count}(s)}{|\\mathcal{C}|} \\end{equation}\\]\nWhile this is an unbiased estimator since it converges to the actual frequency value when the corpus size is sufficiently large, it is both very data-reliant and highly unreliable. If a sentence happens to be absent in \\(\\mathcal{C}\\), it will be assigned probability equal to zero. Therefore, we need to rely on other approaches, such as language models, to obtain reliable estimates from limited training datasets.\nAs we saw in Chapter 1.2.2, language models assign probabilities to sequences of tokens. Formally, this can be framed as learning words’ conditional probability distributions given their context, either preceding or bidirectional depending on the language modeling approach. I will here refer to sequential language models unless otherwise mentioned.\nLanguage models are trained on sequences \\(\\textbf{x} = \\langle x_1, \\dots, x_n \\rangle\\) composed by \\(n\\) tokens taken from a predefined vocabulary \\(\\mathcal{V}\\). Each token \\(x_i\\) can be represented as a one-hot encoded vector \\(x_i \\in \\{0,1\\}^{|\\mathcal{V}|}\\), and the probability of sequence \\(\\textbf{x}\\) is factored using the chain rule:\n\\[\\begin{equation} P(\\textbf{x}) = \\prod_{t=1}^{n}\\,P(x_t\\,|\\,x_1,\\dots,x_{t-1}) \\end{equation}\\]\nAfter the training process, we can use the likelihood that the model assigns to held-out data \\(\\textbf{y}\\) treated as a single stream of \\(m\\) tokens as an intrinsic evaluation metric for the quality of its predictions:\n\\[\\begin{equation} \\ell(\\textbf{y}) = \\sum_{t=1}^m \\log P(x_t|x_1,\\dots,x_{t-1}) \\end{equation}\\]\n\\(\\ell(\\textbf{y})\\) can be rephrased in terms of perplexity, an information-theoretic metric independent from the size of the held-out set:\n\\[\\begin{equation} \\text{PPL}(\\textbf{y}) = 2^{-\\ell(\\textbf{y})/m} \\end{equation}\\]\n\\(\\text{PPL}\\) is equal to 1 if the language model is perfect (i.e., predicts all tokens in the held-out corpus with probability 1) and matches the vocabulary size \\(|\\mathcal{V}|\\) when the model assign a uniform probability to all tokens in the vocabulary (a “random” language model):\n\\[\\begin{align} \\log_2(\\textbf{y}) = \\sum_{t=1}^m \\log_2 \\frac{1}{|\\mathcal{V}|} = - \\sum_{t=1}^m \\log_2 |\\mathcal{V}| = -m \\log_2 |\\mathcal{V}| \\\\ \\text{PPL}(\\textbf{y}) = 2^{\\frac{1}{m}m\\log_2 |\\mathcal{V}|} = 2^{\\log_2 |\\mathcal{V}|} = |\\mathcal{V}| \\end{align}\\]\nPerplexity represents the number of bits required to encode the average word in the corpora. For example, reporting a perplexity score of 10 over a held-out corpus means that the language model will predict on average words with the same accuracy as if it had to choose uniformly and independently across ten possibilities for each word.\nWhile tokens used by language models generally correspond to words in most NLP pipelines, recent language modeling work highlighted the effectiveness of using subword tokens (Sennrich, Haddow, and Birch 2016; Wu et al. 2016; Kudo and Richardson 2018) or even single characters to further improve LM’s generalization performances. In particular, models used in this work rely on SentencePiece and Byte-Pair Encoding (BPE) subword tokenization (Sennrich, Haddow, and Birch 2016; Kudo and Richardson 2018). The SentencePiece algorithm derives a fixed-size vocabulary from word co-occurrences in a large corpus and treats whitespace as a normal symbol by converting it to “_”, while BPE does the same using the “Ġ” character. For example:\nInput sentence: Heteroscedasticity is hard to model!\nSentencePiece tokenization: _Hetero s ced astic ity _is _hard _to _model !\nBPE tokenization: H eter os ced astic ity Ġis Ġhard Ġto Ġmodel !\nwhere whitespaces correspond to separators after tokenization. From the example, we can observe that frequent words like hard, to and model are treated similarly by both tokenizers, while rare words like heteroscedasticity are split into subwords depending on their observed frequency inside the tokenizer’s training corpus.\nIn recent years n-gram language models, which were the most common approach to estimate probabilities from relative frequencies, have been largely supplanted by neural networks. A significant advantage of neural approaches is the overcoming of context restrictions: relevant information can be incorporated from arbitrarily distant contexts while preserving the tractability of the problem from both a statistical and a computational viewpoint.\nNeural language models treat language modeling as a discriminative learning task aimed at maximizing the log conditional probability of a corpus. Formally, the probability distribution \\(p(x|c)\\) is reparametrized as the dot product of two dense numeric vectors \\(\\boldsymbol\\theta_x, \\boldsymbol h_c \\in \\mathbb{R}^H\\) under a softmax transformation:\n\\[\\begin{equation} P(x|c) = \\frac{\\exp(\\boldsymbol\\theta_x \\cdot \\boldsymbol h_c)}{\\sum_{x\u0026#39;\\in\\mathcal{V}} \\exp(\\boldsymbol\\theta_{x\u0026#39;} \\cdot \\boldsymbol h_c)} \\tag{2.1} \\end{equation}\\]\nIn (2.1), the denominator is present to ensure that the probability distribution is properly normalized over vocabulary \\(\\mathcal{V}\\). \\(\\boldsymbol\\theta_x\\) represent model parameters that can be learned through an iterative procedure, while \\(\\boldsymbol h_c\\) is the contextual information that can be computed in different ways depending on the model. For example, a neural language model based on the recurrent neural network architecture (RNN; Mikolov et al. (2010)) recurrently updates context vectors initialized at random with relevant information that needs to be preserved while moving through the sequence.10\nThis work leverages models belonging to the most recent and influential family of neural language models at the time of writing, that is, the one based on the Transformer architecture (Vaswani et al. 2017). Transformers are deep learning models designed to handle sequential data and were conceived to compensate for a significant downside of recurrent models: the need to process data in an orderly manner to perform backpropagation through time. By replacing recurrent computations with attention mechanisms to maintain contextual information throughout the model, Transformers’ operations are entirely parallelizable on dedicated hardware and therefore lead to reduced training times. This fact is especially relevant considering the massive corpora size used to pretrain neural language models to obtain contextual representations. Self-attention was also shown to behave better than other approaches at learning long-range dependencies, avoiding the vanishing gradient problem that plagued non-gated recurrent NLMs altogether (Pascanu, Mikolov, and Bengio 2013).\nFigure 2.1: The original Transformer model architecture by Vaswani et al. (2017). The original Transformer architecture comprises an encoder and a decoder, each composed of a stacked sequence of identical layers that transform input embeddings in outputs with the same dimension (hence the name). First, the encoder maps the sequence \\((x_1, \\dots, x_n)\\) to a sequence of embeddings \\(\\boldsymbol z = (z_1, \\dots, z_n)\\). Given \\(\\boldsymbol z\\), the decoder then autoregressively produces an output token sequence \\((y_1, \\dots, y_m)\\). Each layer of the Transformer encoder comprises two sublayers, a multi-head self-attention mechanism and a feed-forward network, surrounded by residual connections and followed by layer normalization. The decoder includes a third layer that performs multi-head self-attention over the encoder output and modifies the original self-attention sublayer to prevent attending to future context, as required by the language modeling objective. Figure 2.1 presents the original architecture for a \\(N\\)-layer Transformer. I will now proceed to describe the main components of the Transformer model.\nPositional Encodings The original Transformer relies on two sets of embeddings to represent the input sequence: learned word embeddings, used as vector representations for each token in the vocabulary, and fixed positional encodings (PEs) used to inject information about the position of tokens in the sequence. Those are needed since no information about the sequential nature of the input would otherwise be preserved. For position \\(pos\\) and dimension \\(i\\), PEs correspond to sinusoidal periodic functions that were empirically shown to perform on par with learned embeddings, and were chosen to enable extrapolation for longer sequences:\n\\[\\begin{align} PE_{pos, 2i} = \\sin(\\text{pos}/10000^{2i/|h|}) \\\\ PE_{pos, 2i + 1} = \\cos(\\text{pos}/10000^{2i/|h|}) \\end{align}\\]\nwhere \\(|h|\\) is the model’s hidden layer size. Embeddings and PEs are summed and passed to the attention layer.\nSelf-Attention The scaled dot-product self-attention mechanisms is the driving force of the Transformer architecture. Given an input embedding matrix \\(X\\), we multiply it by three weight matrices \\(W^Q, W^K, W^V\\) obtaining the projections \\(Q\\) (queries), \\(K\\) (keys) and \\(V\\) (values). Those are then combined by the self-attention function as follows:\n\\[\\begin{equation} \\text{Attention(Q,K,V)} = \\text{softmax}\\Big ( \\frac{QK^T}{\\sqrt{d_k}}\\Big)V \\end{equation}\\]\nwhere \\(d_k\\) is the size of individual query and key vectors. The output of this operation is a matrix \\(Z\\) which will be passed to the feed-forward layer. The self attention mechanism is further extended to multi-head self-attention in Transformer architectures. In the multi-head variant, the attention function is applied in parallel to \\(n\\) version of queries, keys and values projected with learned parameter matrices, and outputs are finally concatenated and projected again to obtain final values:\n\\[\\begin{align} \\text{MultiHead}(Q,K,V) = \\text{Concat}(\\text{head}_1,\\dots, \\text{head}_n)W^O \\\\ \\text{where } \\text{head}_i = \\text{Attention}(QW_i^Q,KW_i^K,VW_i^V) \\end{align}\\]\nWhere \\(W_i^Q \\in \\mathbb{R}^{|h| \\times d_k}\\), \\(W_i^K \\in \\mathbb{R}^{|h| \\times d_k}\\), \\(W_i^V \\in \\mathbb{R}^{|h| \\times d_v}\\) and \\(W^O \\in \\mathbb{R}^{nd_v \\times |h|}\\). In multi-head attention layers of Figure 2.1, each position can attend to all position from the previous layer, while in the masked multi-head attention layer only previous positions in the sequence can be attended by applying a triangular mask to attention matrices. This additional step is needed to preserve the autoregressive property during decoding.\nFeed-forward Layer Each block in the encoder and the decoder contains an independent fully connected 2-layer feed-forward network with a ReLU nonlinearity applied separately to each position of the sequence:\n\\[\\begin{equation} \\text{FFN}(Z) = \\max(0,Z\\,\\Theta_1 + b_1)\\Theta_2 + b_2 \\end{equation}\\]\nwhere \\(Z\\) are the representations passed forward from the attention sublayer, \\(\\Theta_1, \\Theta_2\\) are two learned independent parameter matrices for each layer and \\(b_1, b_2\\) are their respective bias vectors.\nNow that the main concepts regarding the Transformer architecture have been introduced, the two Transformer-based models used in this study will be presented.\nGPT-2 GPT-2 (Radford et al. 2019) is a transformer model built using only the decoder blocks with masked self-attention, alongside BPE tokenization. The latter’s autoregressive capabilities, i.e. being able to iteratively add a newly predicted token to the existing sequence in the next steps, make it especially suitable for text generation and related tasks. The learning of model parameters is performed in two stages. First, an unsupervised pretraining is carried out to learn a high capacity language model on a large corpus: in particular, here the model is trained to maximize the likelihood of sequential language modeling over WebText, a corpus containing roughly 8 million documents (40GB of text), by adapting its parameters using stochastic gradient descent. The purpose of this step is to learn contextual word embeddings encoding both low and high-level information that can be recycled in downstream tasks, following the transfer learning approach inspired by the field of computer vision and initially proposed by Howard and Ruder (2018) for NLP. The second step is a supervised fine-tuning, where the language modeling softmax layer is replaced by a task-specific layer (called head) with parameters \\(W_y\\) receiving final transformer activations \\(h_l\\) and predicting a label \\(y\\) (e.g. in a classification task) as:\n\\[\\begin{equation} P(y|x_1,\\dots, x_m) = \\text{softmax}(h^{sent}_lW_y) \\end{equation}\\]\nwhere \\(h_l^{sent}\\) is the sentence-level representation for \\((x_1, \\dots, x_m)\\). The parameters of the whole model, including transformer blocks and task-specific heads, can then be tuned by minimizing the loss \\(\\mathcal{L}\\) over the whole supervised corpus \\(\\mathcal{C}\\):\n\\[\\begin{equation} \\mathcal{L}(\\mathcal{C}) = - \\sum_{(x,y)} \\log P(y|x_1, \\dots, x_m) \\end{equation}\\]\nFigure 2.2 visualizes the forward pass through the GPT-2 architecture. We see from the figure that attention patterns learned during pre-trained are often interpretable. Here, the token it is correctly identified as the pronoun referring to the subject a robot. Authors show how large NLMs such as GPT-2 become strong unsupervised multitask learners when trained on sufficiently large corpora, providing the initial motivation for choosing pretrained Transformer models for experiments throughout this study. GPT-2 will be specifically be employed in the experiments of Chapter 5, where its autoregressive nature is ideal for replicating human surprisal estimates during sequential reading on garden-path sentences.\nFigure 2.2: An overview of the forward pass in GPT-2. Adapted from Alammar (2018b). ALBERT ALBERT (Lan et al. 2020) is an efficient variant of the Bidirectional Encoder Representations from Transformers (BERT) approach by Devlin et al. (2019). BERT was built following the intuition that many sentence-level tasks would greatly benefit from an approach capable of incorporating bidirectional context inside language representations. This is not the case for decoder-based approaches like GPT-2 that, being aimed at generation-oriented tasks, could only leverage the previous context using masked self-attention. BERT tackles the unidirectional constraint by introducing masked language modeling (MLM, see Equation (1.2)) and using a stack of transformer encoder layers with GELU nonlinearities (Hendrycks and Gimpel 2016).\nAs for GPT-2, the pretraining and fine-tuning steps are taken to provide the model with general language knowledge and subsequently adapt it to specific downstream tasks. At each pretraining step, a fixed portion of input tokens get masked, and the model predicts the original vocabulary id of masked tokens. Moreover, a sentence-level task is used to improve discourse coherence. For BERT, the next sentence prediction (NSP) task is adopted, i.e. determining whether, given two sentences, they are consecutive or not in the original text using both positive and negative pairs. NSP was found unreliable by subsequent studies and was replaced in ALBERT by a sentence ordering prediction loss that is more challenging for the model. A third set of segment embeddings is added to initial representations to distinguish input sentences in multi-sentence tasks. Special tokens [CLS] and [SEP] are added as sentence-level representations.\nALBERT introduces two main contributions aimed at reducing the final number of model parameters inside BERT:\nFactorized embedding parametrization: a projection layer is introduced between the embedding matrix \\(E\\) and the hidden layer \\(H\\) of the model so that the dimensions of the two are untied. This approach modifies embedding parameter count from \\(O(|V| \\times |E|)\\) to \\(O(|\\mathcal{V}| \\times |E| + |E| \\times |h|)\\), with \\(|\\mathcal{V}|, |E|, |h|\\) being respectively the sizes of vocabulary, embedding vectors and hidden layers. A significant reduction in model parameters is therefore produce when \\(|h| \\gg |E|\\), which is desirable since \\(H\\) contains context-dependent representations that encode more information than the context-independent ones of \\(E\\).\nCross-layer parameter sharing: All layers of ALBERT share the same set of feed-forward and self-attention parameters. Therefore, we can see ALBERT as an iterated function \\(f_A^n: h \\rightarrow h\u0026#39;\\), where \\(n\\) is the number of encoder layers present in the model (in this study \\(n=12\\)), with parameters trained using end-to-end stochastic gradient descent.\nBoth factors significantly contribute to reducing the computational complexity of the model without affecting too much its performances: the ALBERT base used in all experimental chapters of this study have 9x fewer parameters than a regular BERT base model (12M vs. 108M) while performing comparably well on many natural language understanding benchmarks such as GLUE (Wang et al. 2018) and SQuAD (Rajpurkar et al. 2016).\nFigure 2.3 presents how a pretrained ALBERT model can be leveraged for sentence classification, using the ARA task as an example. We note that the procedure is the same as for GPT-2: a task-specific classification head is initialized with random weights, and the whole model-head architecture is fine-tuned on the target task end-to-end. The figure also shows how the common choice for BERT-based models is to use their [CLS] token \\(h_{12}^{1}\\) as the full-sentence representation equivalent \\(h_{12}^{sent}\\).\nFigure 2.3: Using a pretrained ALBERT model for the ARA task. Adapted from Alammar (2018a). To conclude, the fine-tuning approach relying on a pretrained model “body” and a task-specific head adopted in both GPT-2 and ALBERT can be extended out-of-the-box to a multitask learning scenario. A multitask approach can prove useful when considering parallel annotations on the same corpus that provide similar but complementary information about a studied phenomenon’s nature. We can interpret this as an inductive bias that encourages finding knowledge representations to explain multiple sets of annotations at once.11 More specifically, multitask learning with hard parameter sharing (Caruana 1997) is performed in all experimental sections over eye-tracking scores to produce representations encompassing the whole set of phenomena related to natural reading. For doing so, each metric was associated with a task-specific head, and the whole set of heads was trained while sharing the same underlying model.\n2.2.1 Emergent Linguistic Structures in Neural Language Models This section presents evidence in support of the ability of pretrained language models to effectively encode language-related properties in their learned representations.12\nLin, Tan, and Frank (2019) were among the first to highlight how BERT representations encode hierarchical structures akin to syntax trees, despite the absence of syntactic information or recurrent biases during pretraining. Liu et al. (2019) and Tenney, Das, and Pavlick (2019) further showed that contextualized embeddings produced by BERT encode information about part-of-speech, entity roles, and partial syntactic structures.\nHewitt and Manning (2019) formulate the syntax distance hypothesis, assuming that there exists a linear transformation \\(B\\) of the word representation space under which vector distance encodes parse trees. They proceed to test this assumption equating L2 distance in the 2-dimensional space of representations projected by \\(B \\in \\mathbb{R}^{2 \\times |h|}\\) and tree distances in parse trees, finding a close match between BERT representational space and Penn Treebank formalisms. The approach is visualized in Figure 2.4. Jawahar, Sagot, and Seddah (2019) work support these findings, highlighting a close match between BERT representation and dependency trees after testing multiple decomposition schemes. The syntax distance hypothesis’s validity is especially relevant to this work, given the aforementioned importance of syntactic properties in driving human subjects’ perception of complexity.\nFigure 2.4: The mapping from 2D representation space to syntax tree distances adopted in Hewitt and Manning (2019). Despite the evidence of syntactic knowledge in contextual word representations, recent results suggest that the model may not leverage this for its predictions. Ettinger (2020) highlights the insensitivity of BERT to negation and malformed inputs using psycholinguistic diagnostics commonly used with human subjects, while Wallace et al. (2019) show that nonsensical inputs do not affect the prediction quality of BERT, despite having a clear input on underlying syntactic structures. These results are coherent with the experimental findings of this study and will be further discussed in later sections.\n2.3 Analyzing Neural Models of Complexity Having introduced the model architectures that will be used throughout this study, we will now focus on the interpretability approaches allowing us to analyze and compare neural network representations.\nWhen training deep neural networks, we would like to go beyond predictive performance and understand how different design choices and training objectives affect learned representations from a qualitative viewpoint. This fact is especially crucial in the model-driven approach adopted in this work, as stated at the end of Section 2.1. While for linear models, the direct correspondence between the magnitude of feature coefficients and feature importance provides us with some out-of-the-box insights about decision boundaries and feature importance, the hierarchical and nonlinear structure that characterizes neural networks produce model weights that are relatively uninformative when taken in isolation.\nThis work focuses on two interpretability perspectives: highlighting linguistic knowledge encoded in model representations (Chapter 3) and comparing representations across models trained on different complexity-related tasks (Chapter 4). For the first objective, probing classifiers, which have become the de-facto standard in the interpretability literature, are used to evaluate the amount of information encoded in each layer of the model.13 In the second case, two multivariate statistical analysis methods, namely representational similarity analysis and canonical correlation analysis, are leveraged to quantify the relation between model embeddings by evaluating their second-order similarity and learning a mapping to a shared low-dimensional space, respectively. The following sections conclude the chapter by presenting the three approaches in detail.\n2.3.1 Probing classifiers The probing task approach is a natural way to estimate the mutual information shared by a neural network’s parameters and some latent property that the model could have implicitly learned during training. During probing experiments, a supervised model (probe) is trained to predict the latent information from the network’s learned representations. If the probe does well, we may conclude that the network effectively encodes some knowledge related to the selected property.\nFormally speaking, let \\(f: x_i \\rightarrow y_i\\) be a neural network model mapping a corpus of input sentences \\(X = (x_1, \\dots, x_n)\\) to a set of outputs \\(Y = (y_1, \\dots, y_n)\\). Assume that each sentence \\(x_i\\) is also labeled with some linguistic annotations \\(z_i\\), reflecting the underlying properties we aim to detect. Let also \\(h_l(x_i)\\) be the network’s output at the \\(l\\)-th layer given the sentence \\(x_i\\) as input. To estimate the quality of representations \\(h_l\\) with respect to property \\(z\\), a supervised model \\(g: h_l(x_i) \\rightarrow z_i\\) mapping representations to property values is trained. We take such model’s performances as a proxy of \\(H(h_l(x),z)\\). In information theoretic terms, the probe is trained to minimize entropy \\(H(z|h_l(x))\\), and by doing that it maximizes mutual information between the two quantities.\nThe probe \\(g\\) does not need to be a linear model. While historically simple linear probes were used to minimize the risk of memorization, recent results show that more complex probes produce tighter estimates for the actual underlying information (Pimentel et al. 2020). To account for the probe’s ability to learn the task through sheer memorization, Hewitt and Liang (2019) introduce control tasks using the performances of a probe exposed to random labels as baselines.\nAlain and Bengio (2016) were among the first to use linear probing classifiers as tools to evaluate the presence of task-specific information inside neural networks’ layers. The approach was later extended to the field of NLP by Conneau et al. (2018) and Zhang and Bowman (2018) inter alia, which evaluated the presence of semantic and syntactic information inside sentence embeddings generated by LSTM encoders (Hochreiter and Schmidhuber 1997) pretrained on different objectives using probing task suites. Recently, Miaschi and Dell’Orletta (2020) showed how contextual representations produced by pretrained Transformer models could encode sentence-level properties within single-word embeddings. Moreover, Miaschi et al. (2020) highlighted the tendency of pretrained NLMs to lose general linguistic information during the fine-tuning process and found a positive relation between encoded linguistic information and the downstream performances of the model.\n2.3.2 Representational Similarity Analysis Figure 2.5: The Representational Similarity Analysis (RSA) algorithm applied to the representations of three models. Image taken from Abnar (2020). Representational similarity analysis (RSA, Laakso and Cottrell (2000)) is a technique developed in the field of cognitive science to evaluate the similarity of fMRI responses in selected regions of the brain after a stimulus (Kriegeskorte, Mur, and Bandettini 2008). The technique can be extended to compare the heterogeneous representational spaces formed by a set of computational models \\(m\\) exposed to a shared set of observations. Figure 2.5 visualizes the approach. First, each model is fed with a shared corpus of \\(n\\) sentences to produce a set of matrix embeddings \\((E^1, \\dots, E^m)\\), where \\(E^i_j\\) represents the embedding produced by the last layer of the \\(i\\)-th model on the \\(j\\)-th sentence of the corpus.14 Next, for each matrix \\(E^i\\) a representational distance matrix \\(S^i\\) is produced such that \\(S^i_{j,k} = \\text{sim}(E^i_j, E^i_k),\\;S^i \\in \\mathbb{R}^{n \\times n}\\) where \\(\\text{sim}_1\\) is a similarity function (here, dot product). \\(S_i\\) encodes information on the similarity subsisting between model activations across different observations. Finally, a second-level representational similarity matrix \\(S\u0026#39;\\) is computed, where for each pair of matrices \\((S^i, S^j)\\) the corresponding \\(S\u0026#39;_{i,j}\\) entry has value:\n\\[\\begin{equation} S\u0026#39;_{i,j} = S\u0026#39;_{j,i} = \\frac{1}{n}\\sum_{k=1}^n \\text{sim}_2\\big(\\,\\eta\\,(S^i_k),\\eta\\,(S^j_k)\\big) \\end{equation}\\]\nwhere \\(\\eta\\) is the L1 normalization function and \\(\\text{sim}_2\\) is a similarity function (here, Pearson’s correlation coefficient). Each entry \\(S\u0026#39;_{i,j}\\) corresponds to a similarity score between activity patterns of model \\(i\\) and model \\(j\\) across the entire set of \\(n\\) observations.\nIn the context of NLP, Abnar et al. (2019) recently used RSA to compare the activations of multiple neural language models and evaluated the impact of parameter values on the representations formed by a single model. Interestingly, they also use RSA to compare fMRI imaging data collected from human subjects and NLMs activations. Abdou et al. (2019) use RSA to highlight the connection between processing difficulties (measured by high gaze metrics values) and the representational divergence, both inter and intra-encoder. Abnar, Dehghani, and Zuidema (2020) visualize training paths of various neural network architectures as 2D projections of RSA and show how different inductive biases can be transferred across network categories using knowledge distillation (Hinton, Vinyals, and Dean 2015).\n2.3.3 Projection-Weighted Canonical Correlation Analysis Canonical correlation analysis (CCA, Thompson (1984)) is a statistical technique for relating two sets of observations arising from an underlying unknown process. In the context of this work, the underlying process is represented by NLMs being trained on complexity-related tasks. Given a corpus of sentences \\(X = (x_1, \\dots, x_m)\\) annotated with complexity labels, we have that \\(\\boldsymbol z^l_ = (z_i^l(x_1), \\dots z_i^l(x_m))\\) corresponds to all activations of neuron \\(z_i\\) at layer \\(l\\) stacked to form a vector.15 If we consider all activations of all neurons in a layer \\(L_i = (z^i_1, \\dots, z^i_n)\\) for all inputs, we can represent them as a matrix \\(A_i \\in \\mathbb{R}^{m \\times n}\\), i.e. a set of multidimensional variates where \\(n\\) is the number of neurons in the layer. The CCA algorithm aims to identify the best (i.e. most correlated) linear relationship under mutual orthogonality and norm constraints between two sets of multidimensional variates, which in this case are activation matrices like \\(L_1\\). This approach was used, among other things, to study the coherence between modeled and real brain activations (Sussillo et al. 2015).\nFormally, if we have two activation matrices \\(A_1, A_2 \\in \\mathbb{R}^{m \\times n}\\) we aim to find vectors \\(w, v \\in \\mathbb{R}^m\\) such that the correlation:\n\\[\\begin{equation} \\rho = \\frac{\\langle w^TA_1, v^TA_2 \\rangle}{\\|w^TA_1\\| \\cdot \\| v^T A_2\\|} \\end{equation}\\]\nis maximized. The formula can be solved by changing the basis and recurring to singular value decomposition. The output of CCA is a set of singular pairwise orthogonal vectors \\(u, v\\) and their canonical correlation coefficients \\(\\rho \\in [0,1]\\) representing the correlation of vectors \\(w^TA_1\\) and \\(v^TA_2\\).\nFigure 2.6: Projection-Weighted Canonical Correlation Analysis (PWCCA) applied to last-layer representations of two language models. The SVCCA method (Raghu et al. 2017) extends the CCA approach for deep learning research by pruning neurons through a singular value decomposition step before computing canonical correlation coefficients. As the authors mention, “This is especially important in neural network representations, where as we will show many low variance directions (neurons) are primarily noise”. Then, the similarity between two layers \\(L_1, L_2\\) is computed as the mean correlation coefficient produce by SVCCA, and adapted to a distance measure for evaluation: \\[\\begin{equation} d_{\\text{SVCCA}}(A_1, A_2) = 1 - \\frac{1}{|\\rho|} \\sum_{i=1}^{|\\rho|} \\rho^{(i)} \\end{equation}\\] Morcos, Raghu, and Bengio (2018) suggest that the equal importance given to all the \\(|\\rho|\\) SVCCA vectors during the final averaging step may be problematic since it has been extensively shown that overparametrized neural networks often do not recur to their full dimensionality for representing solutions (Frankle and Carbin 2018). They suggest replacing the mean with a weighted mean: \\[\\begin{equation} d_{\\text{PWCCA}}(A_1, A_2) = 1 - \\sum_{i=1}^{|\\rho|} \\alpha \\rho^{(i)} \\;\\;\\text{with} \\;\\; \\tilde \\alpha_i = \\sum_j |\\langle h_i, x_j \\rangle| \\end{equation}\\] where weights \\(\\alpha\\) corresponds to the portion of inputs \\(x\\) accounted for by CCA vectors \\(h\\) and \\(\\tilde \\alpha_i\\) values are normalized such that \\(\\sum_i \\alpha_i = 1\\). The resulting approach, projection-weighted canonical correlation analysis (PWCCA), is used in this study and was shown to be much more robust than SVCCA to filter noise in activations. Figure 2.6 visualizes the selected approach.\nNotable applications of CCA-related methods in NLP are Saphra and Lopez (2019), where SVCCA is used to study the evolution of LSTM language models’ representations during training, and Voita, Sennrich, and Titov (2019), where PWCCA is used to compare Transformer language models across layers and pretraining objectives.\nReferences Abdou, Mostafa, Artur Kulmizev, Felix Hill, Daniel M. Low, and Anders Søgaard. 2019. “Higher-Order Comparisons of Sentence Encoder Representations.” In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (Emnlp-Ijcnlp), 5838–45. Hong Kong, China: Association for Computational Linguistics. https://doi.org/10.18653/v1/D19-1593.\nAbnar, Samira. 2020. “Visualizing Model Comparison.” Blog Post. https://samiraabnar.github.io/articles/2020-05/vizualization.\nAbnar, Samira, Lisa Beinborn, Rochelle Choenni, and Willem Zuidema. 2019. “Blackbox Meets Blackbox: Representational Similarity \u0026amp; Stability Analysis of Neural Language Models and Brains.” In Proceedings of the 2019 Acl Workshop Blackboxnlp: Analyzing and Interpreting Neural Networks for Nlp, 191–203. Florence, Italy: Association for Computational Linguistics. https://doi.org/10.18653/v1/W19-4820.\nAbnar, Samira, Mostafa Dehghani, and Willem Zuidema. 2020. “Transferring Inductive Biases Through Knowledge Distillation.” ArXiv Pre-Print 2006.00555. https://arxiv.org/abs/2006.00555.\nAlain, Guillaume, and Yoshua Bengio. 2016. “Understanding Intermediate Layers Using Linear Classifier Probes.” ArXiv Pre-Print 1610.01644. https://arxiv.org/abs/1610.01644.\nAlammar, Jay. 2018a. “The Illustrated Bert, Elmo, and Co. (How NLP Cracked Transfer Learning).” Blog Post. https://jalammar.github.io/illustrated-bert/.\nAlammar, Jay. 2018b. “The Illustrated Gpt-2.” Blog Post. https://http://jalammar.github.io/illustrated-gpt2/.\nAndreas, Jacob, and Dan Klein. 2014. “How Much Do Word Embeddings Encode About Syntax?” In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), 822–27. Baltimore, Maryland: Association for Computational Linguistics. https://doi.org/10.3115/v1/P14-2133.\nBahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio. 2015. “Neural Machine Translation by Jointly Learning to Align and Translate.” In Proceeding of the 3rd International Conference on Learning Representations (ICLR’15).\nCaruana, Rich. 1997. “Multitask Learning.” Machine Learning 28: 41–75. https://www.cs.utexas.edu/~kuipers/readings/Caruana-mlj-97.pdf.\nConneau, Alexis, German Kruszewski, Guillaume Lample, Loı̈c Barrault, and Marco Baroni. 2018. “What You Can Cram into a Single $\u0026amp;!#* Vector: Probing Sentence Embeddings for Linguistic Properties.” In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2126–36. Melbourne, Australia: Association for Computational Linguistics. https://doi.org/10.18653/v1/P18-1198.\nDevlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. “BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding.” In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), 4171–86. Minneapolis, Minnesota: Association for Computational Linguistics. https://doi.org/10.18653/v1/N19-1423.\nEttinger, Allyson. 2020. “What BERT Is Not: Lessons from a New Suite of Psycholinguistic Diagnostics for Language Models.” Transactions of the Association for Computational Linguistics 8: 34–48. https://doi.org/10.1162/tacl_a_00298.\nFrankle, Jonathan, and Michael Carbin. 2018. “The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks.” In Proceedings of the 8th International Conference on Learning Representations (Iclr’18).\nGoodfellow, Ian, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. 2016. Deep Learning. MIT Press Cambridge.\nHendrycks, Dan, and Kevin Gimpel. 2016. “Gaussian Error Linear Units (Gelus).” ArXiv Pre-Print 1606.08415. https://arxiv.org/abs/1606.08415.\nHewitt, John, and Percy Liang. 2019. “Designing and Interpreting Probes with Control Tasks.” In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (Emnlp-Ijcnlp), 2733–43. Hong Kong, China: Association for Computational Linguistics. https://doi.org/10.18653/v1/D19-1275.\nHewitt, John, and Christopher D. Manning. 2019. “A Structural Probe for Finding Syntax in Word Representations.” In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), 4129–38. Minneapolis, Minnesota: Association for Computational Linguistics. https://doi.org/10.18653/v1/N19-1419.\nHinton, Geoffrey, Oriol Vinyals, and Jeff Dean. 2015. “Distilling the Knowledge in a Neural Network.” ArXiv Pre-Print 1503.02531. https://arxiv.org/abs/1503.02531.\nHochreiter, Sepp, and Jürgen Schmidhuber. 1997. “Long Short-Term Memory.” Neural Computation 9 (8). MIT Press: 1735–80.\nHoward, Jeremy, and Sebastian Ruder. 2018. “Universal Language Model Fine-Tuning for Text Classification.” In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 328–39. Melbourne, Australia: Association for Computational Linguistics. https://doi.org/10.18653/v1/P18-1031.\nJawahar, Ganesh, Benoit Sagot, and Djamé Seddah. 2019. “What Does BERT Learn About the Structure of Language?” In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, 3651–7. Florence, Italy: Association for Computational Linguistics. https://doi.org/10.18653/v1/P19-1356.\nKriegeskorte, N., M. Mur, and P. Bandettini. 2008. “Representational Similarity Analysis – Connecting the Branches of Systems Neuroscience.” Frontiers in Systems Neuroscience 2. https://doi.org/10.3389/neuro.06.004.2008.\nKudo, Taku, and John Richardson. 2018. “SentencePiece: A Simple and Language Independent Subword Tokenizer and Detokenizer for Neural Text Processing.” In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, 66–71. Brussels, Belgium: Association for Computational Linguistics. https://doi.org/10.18653/v1/D18-2012.\nLaakso, Aarre, and Garrison Cottrell. 2000. “Content and Cluster Analysis: Assessing Representational Similarity in Neural Systems.” Philosophical Psychology 13 (1). Taylor \u0026amp; Francis: 47–76.\nLan, Zhenzhong, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. 2020. “ALBERT: A Lite BERT for Self-Supervised Learning of Language Representations.” In International Conference on Learning Representations. https://openreview.net/forum?id=H1eA7AEtvS.\nLin, Yongjie, Yi Chern Tan, and Robert Frank. 2019. “Open Sesame: Getting Inside BERT’s Linguistic Knowledge.” In Proceedings of the 2019 Acl Workshop Blackboxnlp: Analyzing and Interpreting Neural Networks for Nlp, 241–53. Florence, Italy: Association for Computational Linguistics. https://doi.org/10.18653/v1/W19-4825.\nLiu, Nelson F., Matt Gardner, Yonatan Belinkov, Matthew E. Peters, and Noah A. Smith. 2019. “Linguistic Knowledge and Transferability of Contextual Representations.” In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), 1073–94. Minneapolis, Minnesota: Association for Computational Linguistics. https://doi.org/10.18653/v1/N19-1112.\nMiaschi, Alessio, Dominique Brunato, Felice Dell’Orletta, and Giulia Venturi. 2020. “Linguistic Profiling of a Neural Language Model.” In Proceedings of the 28th Conference on Computational Linguistics (Coling). Online: Association for Computational Linguistics. https://arxiv.org/abs/2010.01869.\nMiaschi, Alessio, and Felice Dell’Orletta. 2020. “Contextual and Non-Contextual Word Embeddings: An in-Depth Linguistic Investigation.” In Proceedings of the 5th Workshop on Representation Learning for Nlp, 110–19. Online: Association for Computational Linguistics. https://www.aclweb.org/anthology/2020.repl4nlp-1.15.\nMikolov, Tomas, Kai Chen, G. S. Corrado, and J. Dean. 2013. “Efficient Estimation of Word Representations in Vector Space.” CoRR abs/1301.3781.\nMikolov, Tomas, M. Karafiát, L. Burget, J. Cernocký, and S. Khudanpur. 2010. “Recurrent Neural Network Based Language Model.” In INTERSPEECH.\nMorcos, Ari, Maithra Raghu, and Samy Bengio. 2018. “Insights on Representational Similarity in Neural Networks with Canonical Correlation.” In Advances in Neural Information Processing Systems 31, edited by S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, 5727–36. Curran Associates, Inc. http://papers.nips.cc/paper/7815-insights-on-representational-similarity-in-neural-networks-with-canonical-correlation.pdf.\nPascanu, R., Tomas Mikolov, and Yoshua Bengio. 2013. “On the Difficulty of Training Recurrent Neural Networks.” In Proceedings of the 30th International Conference on Machine Learning (Icml’13).\nPennington, Jeffrey, Richard Socher, and Christopher Manning. 2014. “GloVe: Global Vectors for Word Representation.” In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), 1532–43. Doha, Qatar: Association for Computational Linguistics. https://doi.org/10.3115/v1/D14-1162.\nPeters, Matthew, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. “Deep Contextualized Word Representations.” In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), 2227–37. New Orleans, Louisiana: Association for Computational Linguistics. https://doi.org/10.18653/v1/N18-1202.\nPimentel, Tiago, Josef Valvoda, Rowan Hall Maudslay, Ran Zmigrod, Adina Williams, and Ryan Cotterell. 2020. “Information-Theoretic Probing for Linguistic Structure.” In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, 4609–22. Online: Association for Computational Linguistics. https://doi.org/10.18653/v1/2020.acl-main.420.\nRadford, A., Jeffrey Wu, R. Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. “Language Models Are Unsupervised Multitask Learners.” OpenAI Blog. OpenAI.\nRaghu, Maithra, Justin Gilmer, Jason Yosinski, and Jascha Sohl-Dickstein. 2017. “SVCCA: Singular Vector Canonical Correlation Analysis for Deep Learning Dynamics and Interpretability.” In Advances in Neural Information Processing Systems 30, edited by I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, 6076–85. Curran Associates, Inc. http://papers.nips.cc/paper/7188-svcca-singular-vector-canonical-correlation-analysis-for-deep-learning-dynamics-and-interpretability.pdf.\nRajpurkar, Pranav, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. “SQuAD: 100,000+ Questions for Machine Comprehension of Text.” In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, 2383–92. Austin, Texas: Association for Computational Linguistics. https://doi.org/10.18653/v1/D16-1264.\nSamek, W., Grégoire Montavon, A. Vedaldi, L. Hansen, and K. Müller. 2019. “Explainable Ai: Interpreting, Explaining and Visualizing Deep Learning.” Explainable AI: Interpreting, Explaining and Visualizing Deep Learning.\nSaphra, Naomi, and Adam Lopez. 2019. “Understanding Learning Dynamics of Language Models with SVCCA.” In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), 3257–67. Minneapolis, Minnesota: Association for Computational Linguistics. https://doi.org/10.18653/v1/N19-1329.\nSennrich, Rico, Barry Haddow, and Alexandra Birch. 2016. “Neural Machine Translation of Rare Words with Subword Units.” In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 1715–25. Berlin, Germany: Association for Computational Linguistics. https://doi.org/10.18653/v1/P16-1162.\nShwartz-Ziv, Ravid, and Naftali Tishby. 2017. “Opening the Black Box of Deep Neural Networks via Information.” ArXiv Pre-Print 1703.00810. https://arxiv.org/abs/1703.00810.\nSussillo, David, Mark M Churchland, Matthew T Kaufman, and Krishna V Shenoy. 2015. “A Neural Network That Finds a Naturalistic Solution for the Production of Muscle Activity.” Nature Neuroscience 18 (7). Nature Publishing Group: 1025–33.\nTenney, Ian, Dipanjan Das, and Ellie Pavlick. 2019. “BERT Rediscovers the Classical NLP Pipeline.” In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, 4593–4601. Florence, Italy: Association for Computational Linguistics. https://doi.org/10.18653/v1/P19-1452.\nThompson, Bruce. 1984. Canonical Correlation Analysis: Uses and Interpretation. 47. Sage.\nTurian, Joseph, Lev-Arie Ratinov, and Yoshua Bengio. 2010. “Word Representations: A Simple and General Method for Semi-Supervised Learning.” In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, 384–94. Uppsala, Sweden: Association for Computational Linguistics. https://www.aclweb.org/anthology/P10-1040.\nVaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. “Attention Is All You Need.” In Advances in Neural Information Processing Systems 30, edited by I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, 5998–6008. Curran Associates, Inc. http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf.\nVoita, Elena, Rico Sennrich, and Ivan Titov. 2019. “The Bottom-up Evolution of Representations in the Transformer: A Study with Machine Translation and Language Modeling Objectives.” In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (Emnlp-Ijcnlp), 4396–4406. Hong Kong, China: Association for Computational Linguistics. https://doi.org/10.18653/v1/D19-1448.\nWallace, Eric, Yizhong Wang, Sujian Li, Sameer Singh, and Matt Gardner. 2019. “Do NLP Models Know Numbers? Probing Numeracy in Embeddings.” In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (Emnlp-Ijcnlp), 5307–15. Hong Kong, China: Association for Computational Linguistics. https://doi.org/10.18653/v1/D19-1534.\nWang, Alex, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2018. “GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding.” In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, 353–55. Brussels, Belgium: Association for Computational Linguistics. https://doi.org/10.18653/v1/W18-5446.\nWu, Y., Mike Schuster, Z. Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey, M. Krikun, et al. 2016. “Google’s Neural Machine Translation System: Bridging the Gap Between Human and Machine Translation.” ArXiv Pre-Print 1609.08144. https://arxiv.org/abs/1609.08144.\nZhang, Kelly, and Samuel Bowman. 2018. “Language Modeling Teaches You More Than Translation Does: Lessons Learned Through Auxiliary Syntactic Task Analysis.” In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, 359–61. Brussels, Belgium: Association for Computational Linguistics. https://doi.org/10.18653/v1/W18-5448.\nRefer to Chapter 6.3 of Eisenstein (2019) for additional details about recurrent language models.↩\nSee Ruder (2017) for a comprehensive overview↩\nRogers, Kovaleva, and Rumshisky (2020) and Linzen and Baroni (2021) are surveys covering this topic.↩\nSee Belinkov and Glass (2019) survey and Belinkov, Gehrmann, and Pavlick (2020) tutorial.↩\nThis can be any layer; embeddings can be produced by different layers of the same model.↩\nDifferent from the activation vector, i.e. all neurons’ activations for a single input \\((z^l_1(x_1),\\dots,z^l_n(x_1))\\)↩\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f3e4c9150d167db5b96935b28f4f2fa8","permalink":"https://gsarti.com/msc-thesis/chap-models/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/msc-thesis/chap-models/","section":"msc-thesis","summary":"\u003c!DOCTYPE html\u003e 2 Models of Linguistic Complexity | Interpreting Neural Language Models for Linguistic Complexity Assessment Introduction 1 Linguistic Complexity 1.1 Categorizing Linguistic Complexity Measures 1.2 Intrinsic Perspective 1.2.1 Structural Linguistic Complexity 1.2.2 Language Modeling Surprisal 1.3 Extrinsic Perspective 1.3.1 Automatic Readability Assessment 1.3.2 Perceived Complexity Prediction 1.3.3 Gaze Metrics Prediction 1.4 Garden-path Sentences 2 Models of Linguistic Complexity 2.1 Desiderata for Models of Linguistic Complexity 2.2 Neural Language Models: Unsupervised Multitask Learners 2.","tags":null,"title":"","type":"msc-thesis"},{"authors":null,"categories":null,"content":"\u003c!DOCTYPE html\u003e Conclusion | Interpreting Neural Language Models for Linguistic Complexity Assessment Introduction 1 Linguistic Complexity 1.1 Categorizing Linguistic Complexity Measures 1.2 Intrinsic Perspective 1.2.1 Structural Linguistic Complexity 1.2.2 Language Modeling Surprisal 1.3 Extrinsic Perspective 1.3.1 Automatic Readability Assessment 1.3.2 Perceived Complexity Prediction 1.3.3 Gaze Metrics Prediction 1.4 Garden-path Sentences 2 Models of Linguistic Complexity 2.1 Desiderata for Models of Linguistic Complexity 2.2 Neural Language Models: Unsupervised Multitask Learners 2.2.1 Emergent Linguistic Structures in Neural Language Models 2.3 Analyzing Neural Models of Complexity 2.3.1 Probing classifiers 2.3.2 Representational Similarity Analysis 2.3.3 Projection-Weighted Canonical Correlation Analysis 3 Complexity Phenomena in Linguistic Annotations and Language Models 3.1 Data and Preprocessing 3.2 Analysis of Linguistic Phenomena 3.2.1 Linguistic Phenomena in Length-controlled Bins 3.3 Modeling Online and Offline Linguistic Complexity 3.3.1 Modeling Complexity in Length-controlled Bins 3.4 Probing Linguistic Phenomena in ALBERT Representations 3.5 Summary 4 Representational Similarity in Models of Complexity 4.1 Knowledge-driven Requirements for Learning Models 4.2 Experimentsl Evaluation 4.2.1 Data 4.2.2 Inter-model Representational Similarity 4.2.3 Intra-model Representational Similarity 4.3 Summary 5 Gaze-informed Models for Cognitive Processing Prediction 5.1 Experimental Setup 5.2 Experimental Evaluation 5.2.1 Estimating Magnitudes of Garden-path Delays 5.2.2 Predicting Delays with Surprisal and Gaze Metrics 5.3 Summary Conclusion Broader Impact and Ethical Perspectives Future Directions Appendix A Linguistic Features A.1 Raw Text Properties and Lexical Variety A.2 Morpho-syntacting Information A.3 Verbal Predicate Structure A.4 Global and Local Parsed Tree Structures A.5 Syntactic Relations A.6 Subordination Phenomena B Precisions on Eye-tracking Metrics and Preprocessing C Multi-task Token-level Regression for Gaze Metrics Prediction D Intra-model Similarity for All Models E Gaze Metrics Predictions for Garden Path Sentences F Reproducibility and Environmental Impact References Back to my website Interpreting Neural Language Models\nfor Linguistic Complexity Assessment Conclusion This thesis work adopted a model-driven approach to investigate the relationship between different linguistic complexity perspectives for the English language and study how those are learned and encoded by deep learning models at various abstraction levels.\nFrom the theoretical viewpoint of connecting different complexity perspectives using empirical annotations, Chapter 3 analysis highlighted the strong connection between online/offline complexity metrics and length-related linguistic properties of sentences. The relation was further investigated in length-controlled settings, obtaining similar results across online gaze measurements but different for offline perceived complexity annotations. The overall results identify syntagmatic complexity as the primary source of variation in both offline and online complexity perception for readers. However, they also show how the variety in parts and hierarchical structures contributes differently across different complexity perspectives when sentence length is controlled. Another theoretical aspect supported by Chapter 5 experimental results is the role played by cognitive mechanisms other than predictability in shaping human processing patterns on ambiguous constructions like garden-path sentences. In this context, a computational model that accurately predicts the presence or garden-path effects was used as a psycholinguistic subject to provide predictability annotations on standard and atypical constructions. A surprisal-to-reading-times conversion coefficient was then estimated from gaze annotations and surprisal scores on standard constructions. The resulting reading times were used to highlight how the model widely overestimated the magnitude of garden-path effects, following the methodology of Schijndel and Linzen (2020). While results differ significantly from the latter study due to a much larger conversion coefficient, the presence of different accounts for cognitive processing is supported when considering how proportions in predicted magnitudes on different types of constructions do not match the ones reported in recent psycholinguistics literature.\nDespite interesting theoretical findings, this work is mostly devoted to interpreting complexity phenomena from a modeling standpoint. Chapter 3 evaluates the encoding of linguistic properties inside neural language models’ representations using probing tasks performed before and after model fine-tuning on complexity-related tasks. Results highlighted the emergence of task-related linguistic properties within the model’s representations after the fine-tuning process, providing evidence for the relation between models’ linguistic skills during training and their performances on morphosyntactically-related tasks. In light of these findings, it can be conjectured that linguistic probes may provide a reasonable estimate of the task-oriented quality of representations for those highly-syntactic tasks. In Chapter 4, the representations learned by neural language models were compared across layers and fine-tuning tasks using representational similarity approaches. The absence of higher similarity scores between complexity-trained models compared to the pre-trained one suggests that training objectives are learned by overfitting annotations and that learned parameters hardly capture information that could be relevant for multiple complexity-related tasks.\nMoreover, task framing and the annotation modalities were observed to play a much larger role in defining representational similarity scores rather than the conceptual similarity between tasks. This fact supports the claim that standard optimization procedures used in deep learning are not suitable for this type of concept-driven learning. Finally, Chapter 5 highlighted the inability of standard neural language models in leveraging syntactic cues to improve prediction in the context of garden-path effects. Models fine-tuned on gaze annotations were tested on garden-path test suites to evaluate whether reading time predictions can perform as well as surprisal in identifying garden-path triggers. Results highlight how models heavily overfit gaze annotation and cannot predict the increase in reading times observed in human subjects despite being exposed to the temporary syntactic ambiguity that characterizes garden-path constructions.\nRecent trends in transfer learning have profoundly shaped the last few years of research in NLP, leading to astonishing improvements in almost all language-related tasks, including linguistic complexity prediction. Despite all the hype, the fundamental problem behind all computational linguistics research remains: even the most powerful deep learning models do not “understand” language, and their learned representations are “potentially useful, but incomplete, reflections of the actual meaning” they derive from structural training procedures (Bender and Koller 2020). In support of this affirmation, all models leveraged in this study by following closely standard procedures were found lacking in generalization capabilities and hierarchical abstraction, despite their excellent performances on predicting in-domain observations. To conclude with a somewhat cliché affirmation, much work still needs to be done to drive generalizable, hierarchical, and compositional representation learning in language models, enabling proper human-level natural language understanding.\nBroader Impact and Ethical Perspectives The findings described in this thesis work are mostly meta-analytical, and as such, mostly intended to distill theoretical insights and evaluate recent efforts in the natural language processing community. This said, some of the models and procedures described in this work can be clearly beneficial to society. For example, using models trained to predict reading patterns may be used in educational settings to identify difficult passages that can be simplified, improving reading comprehension for students in a fully-personalizable way. This type of technology can also be applied to domain-specific documents such as juridical or medical reports to identify critical areas that can be adapted to improve layman’s understanding. However, it is essential to recognize the potentially malicious usage of such systems. The integration of eye-tracking systems in mobile devices, paired with predictive models presented in this work, could be used to build harmful surveillance systems and advertisement platforms using gaze predictions for extreme behavioral manipulation. Moreover, multiple individuals’ gaze data could be leveraged by autonomous systems to enforce discriminatory practices towards neurodiverse subjects in hardly-detectable ways. In terms of research impact, the experiments presented in this work may provide useful insights into the behavior of neural language models for researchers working in the fields of interpretability in NLP and computational psycholinguistics.\nFuture Directions In conclusion, multiple paths to improve and extend the scope of this work were identified during the experimental process, and will be left here as a final note for my future self and for anyone interested in pushing forward research in fields related to this thesis’ topics.\nSelf-training has recently proven to be very effective for compensating the lack of large labeled datasets in the context of acceptability and complexity prediction (Sarti 2020). In light of these results, it would be interesting to evaluate whether self-training could also improve the performances and generalization of models used for gaze metrics prediction.\nEvaluate whether gaze-trained neural language models having undergone a cloze distillation process (Eisape, Zaslavsky, and Levy 2020), combining intuitions from masked language modeling and knowledge distillation (Hinton, Vinyals, and Dean 2015), would produce better results for modeling out-of-distribution garden-path phenomena compared to the somewhat naive approach adopted in this study.\nIncorporating gaze metrics prediction in the training objectives of learning models can be interesting to account for human cognitive biases during reading. The crucial aspect is how to get a sufficient amount of annotated data to make this idea scalable for modern language models’ pre-training needs. In this regard, it could be interesting to test the approach by Hollenstein and Zhang (2019) where mean gaze scores are averaged for each type across annotators, effectively providing a way to label input sentences with robust gaze information in an unsupervised manner.\nSince eye-tracking metrics are complexity signals with free human supervision, it could be possible to leverage those for simplification and other related tasks in an iterative learning-from-human-feedback paradigm similar to the one described in Stiennon et al. (2020).\nIt should in principle be possible to use human processing data as a replacement for the self-attention computation. The dot product critically bounds the computational efficiency of attention-based models, and fixed attention has been shown to have a limited negative impact on final results while making inference much faster (Tay et al. 2020). Fixing attention weights using human attention, as measured by eye-tracking metrics, can be an exciting perspective to explore in this context. This idea can be thought of as an application of human attention regularization of LSTM attentional networks for various tasks proposed in Barrett et al. (2018) to Transformers networks.\nWould explicitly embedding complexity in the learning process of language models favor hierarchical abstraction? In this perspective, it would be exciting to evaluate whether a model trained on easy-to-hard sentences following language acquisition insights would encode different knowledge in terms of linguistic structures, concept abstraction, and allowances.\nFinding better ways to instill useful inductive biases into learning models, especially for syntax-heavy downstream tasks. Concrete examples following this direction may use parsing as a complementary task to keep top-level representations sensible to syntactic changes, as tested in Glavas and Vulic (2020) for natural language understanding, or use hybrid symbolic-neural models to represent syntax as in Zanzotto et al. (2020).\nReferences Barrett, Maria, Joachim Bingel, Nora Hollenstein, Marek Rei, and Anders Søgaard. 2018. “Sequence Classification with Human Attention.” In Proceedings of the 22nd Conference on Computational Natural Language Learning, 302–12. Brussels, Belgium: Association for Computational Linguistics. https://doi.org/10.18653/v1/K18-1030.\nBender, Emily M., and Alexander Koller. 2020. “Climbing Towards NLU: On Meaning, Form, and Understanding in the Age of Data.” In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, 5185–98. Online: Association for Computational Linguistics. https://doi.org/10.18653/v1/2020.acl-main.463.\nEisape, Tiwalayo, Noga Zaslavsky, and Roger Levy. 2020. “Cloze Distillation Improves Psychometric Predictive Power.” In Proceedings of the 24th Conference on Computational Natural Language Learning, 609–19. Online: Association for Computational Linguistics. https://www.aclweb.org/anthology/2020.conll-1.49.\nGlavas, Goran, and Ivan Vulic. 2020. “Is Supervised Syntactic Parsing Beneficial for Language Understanding? An Empirical Investigation.” ArXiv Pre-Print 2008.06788. https://arxiv.org/abs/2008.06788.\nHinton, Geoffrey, Oriol Vinyals, and Jeff Dean. 2015. “Distilling the Knowledge in a Neural Network.” ArXiv Pre-Print 1503.02531. https://arxiv.org/abs/1503.02531.\nHollenstein, Nora, and Ce Zhang. 2019. “Entity Recognition at First Sight: Improving NER with Eye Movement Information.” In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), 1–10. Minneapolis, Minnesota: Association for Computational Linguistics. https://doi.org/10.18653/v1/N19-1001.\nSarti, Gabriele. 2020. “UmBERTo-MTSA @ AcCompl-It: Improving Complexity and Acceptability Prediction with Multi-Task Learning on Self-Supervised Annotations.” ArXiv Pre-Print 2011.05197. https://arxiv.org/abs/2011.05197.\nSchijndel, Marten van, and Tal Linzen. 2020. “Single-Stage Prediction Models Do Not Explain the Magnitude of Syntactic Disambiguation Difficulty.” PsyArXiv Pre-Print sgbqy. https://psyarxiv.com/sgbqy/.\nStiennon, Nisan, Long Ouyang, Jeff Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul Christiano. 2020. “Learning to Summarize from Human Feedback.” ArXiv Pre-Print 2009.01325. https://arxiv.org/abs/2009.01325.\nTay, Yi, Dara Bahri, Donald Metzler, D. Juan, Zhe Zhao, and Che Zheng. 2020. “Synthesizer: Rethinking Self-Attention in Transformer Models.” ArXiv Pre-Print 2005.00743. https://arxiv.org/abs/2005.00743.\nZanzotto, Fabio Massimo, Andrea Santilli, Leonardo Ranaldi, Dario Onorati, Pierfrancesco Tommasino, and Francesca Fallucchi. 2020. “KERMIT: Complementing Transformer Architectures with Encoders of Explicit Syntactic Interpretations.” In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (Emnlp), 256–67. Online: Association for Computational Linguistics. https://www.aclweb.org/anthology/2020.emnlp-main.18.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"e14df1f3348a3cd9cdb2e016f54a4861","permalink":"https://gsarti.com/msc-thesis/conclusion/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/msc-thesis/conclusion/","section":"msc-thesis","summary":"\u003c!DOCTYPE html\u003e Conclusion | Interpreting Neural Language Models for Linguistic Complexity Assessment Introduction 1 Linguistic Complexity 1.1 Categorizing Linguistic Complexity Measures 1.2 Intrinsic Perspective 1.2.1 Structural Linguistic Complexity 1.2.2 Language Modeling Surprisal 1.3 Extrinsic Perspective 1.3.1 Automatic Readability Assessment 1.3.2 Perceived Complexity Prediction 1.3.3 Gaze Metrics Prediction 1.4 Garden-path Sentences 2 Models of Linguistic Complexity 2.1 Desiderata for Models of Linguistic Complexity 2.2 Neural Language Models: Unsupervised Multitask Learners 2.2.1 Emergent Linguistic Structures in Neural Language Models 2.","tags":null,"title":"","type":"msc-thesis"},{"authors":null,"categories":null,"content":"\u003c!DOCTYPE html\u003e Interpreting Neural Language Models for Linguistic Complexity Assessment Introduction 1 Linguistic Complexity 1.1 Categorizing Linguistic Complexity Measures 1.2 Intrinsic Perspective 1.2.1 Structural Linguistic Complexity 1.2.2 Language Modeling Surprisal 1.3 Extrinsic Perspective 1.3.1 Automatic Readability Assessment 1.3.2 Perceived Complexity Prediction 1.3.3 Gaze Metrics Prediction 1.4 Garden-path Sentences 2 Models of Linguistic Complexity 2.1 Desiderata for Models of Linguistic Complexity 2.2 Neural Language Models: Unsupervised Multitask Learners 2.2.1 Emergent Linguistic Structures in Neural Language Models 2.3 Analyzing Neural Models of Complexity 2.3.1 Probing classifiers 2.3.2 Representational Similarity Analysis 2.3.3 Projection-Weighted Canonical Correlation Analysis 3 Complexity Phenomena in Linguistic Annotations and Language Models 3.1 Data and Preprocessing 3.2 Analysis of Linguistic Phenomena 3.2.1 Linguistic Phenomena in Length-controlled Bins 3.3 Modeling Online and Offline Linguistic Complexity 3.3.1 Modeling Complexity in Length-controlled Bins 3.4 Probing Linguistic Phenomena in ALBERT Representations 3.5 Summary 4 Representational Similarity in Models of Complexity 4.1 Knowledge-driven Requirements for Learning Models 4.2 Experimentsl Evaluation 4.2.1 Data 4.2.2 Inter-model Representational Similarity 4.2.3 Intra-model Representational Similarity 4.3 Summary 5 Gaze-informed Models for Cognitive Processing Prediction 5.1 Experimental Setup 5.2 Experimental Evaluation 5.2.1 Estimating Magnitudes of Garden-path Delays 5.2.2 Predicting Delays with Surprisal and Gaze Metrics 5.3 Summary Conclusion Broader Impact and Ethical Perspectives Future Directions Appendix A Linguistic Features A.1 Raw Text Properties and Lexical Variety A.2 Morpho-syntacting Information A.3 Verbal Predicate Structure A.4 Global and Local Parsed Tree Structures A.5 Syntactic Relations A.6 Subordination Phenomena B Precisions on Eye-tracking Metrics and Preprocessing C Multi-task Token-level Regression for Gaze Metrics Prediction D Intra-model Similarity for All Models E Gaze Metrics Predictions for Garden Path Sentences F Reproducibility and Environmental Impact References Back to my website Interpreting Neural Language Models\nfor Linguistic Complexity Assessment Interpreting Neural Language Models\nfor Linguistic Complexity Assessment Gabriele Sarti\nAbstract\nLo studio della complessità linguistica è un ambito profondamente multidisciplinare, che spazia dallo studio dell’elaborazione cognitiva in lettori umani alla classificazione della complessità strutturale caratterizzante espressioni in linguaggio naturale. In tempi recenti, l’utilizzo di metodi computazionali per il trattamento e l’analisi del linguaggio ha prodotto importanti sviluppi nella comprensione di molteplici fenomeni associati alla complessità linguistica. In linea con lo stato dell’arte del settore, questa tesi presenta uno studio model-driven di molteplici fenomeni associati alla complessità linguistica. In primo luogo, vengono esplorate empiricamente le relazioni che sussistono tra varie metriche estrinseche di complessità – percezione di complessità linguistica, leggibilità, elaborazione cognitiva e prevedibilità – evidenziando similitudini e differenze da una prospettiva linguisticamente e cognitivamente motivata. In seguito, viene studiato come l’informazione alla base delle diverse metriche di complessità possa essere acquisita da modelli del linguaggio basati su reti neurali, a vari livelli di astrazione e granularità, applicando tecniche di interpretabilità derivate dalla letteratura sull’elaborazione del linguaggio naturale. In conclusione, viene valutata la capacità di vari modelli computazionali di complessità nel prevedere difficoltà di elaborazione cognitiva associate a costrutti sintattici atipici, quali le garden-path sentences. I risultati sperimentali di questo studio forniscono prove convergenti riguardo alle limitate capacità di astrazione e generalizzazione dei modelli di linguaggio neurali allo stato dell’arte per la previsione della complessità linguistica, e incoraggiano all’adozione di linee di ricerca che integrino informazione simbolica e interpretabile in questo settore. In un’ottica di riproducibilità, il codice utilizzato per gli esperimenti viene reso disponibile al seguente indirizzo: https://github.com/gsarti/interpreting-complexity Introduction The study of complexity in language production and comprehension is a multidisciplinary field encompassing approaches that range from the analysis of cognitive processing phenomena in human subjects to the classification of structural complexity in natural language utterances. Because of its inherently faceted nature, linguistic complexity still defies a univocal definition and depends heavily on the point of view adopted during experimental inquiries. In recent years, as a consequence of the astounding expansion in human technological capabilities, the scientific community witnessed a proliferation of studies leveraging computational methods to investigate different complexity perspectives and develop automatic systems for linguistic complexity assessment. The introduction of neural network models able to automatically learn hierarchical representations of language spurred new lines of research in the field of Natural Language Processing, with researchers aiming to reverse-engineer theoretical intuitions by interpreting results and learning mechanics of those models. Nowadays, deep computational models are routinely adopted to study and evaluate linguistic complexity in applicative settings such as readability assessment, simplification, and first/second language learning.\nThis thesis fits into this current line of research by pursuing a two-fold aim. On the one hand, it investigates the connection between multiple human-centric perspectives of linguistic complexity – perception of complexity, readability, cognitive processing, and predictability – highlighting similarities and differences between them from a linguistically and cognitively-motivated viewpoint. On the other hand, it studies how those perspectives are learned by deep learning models at various levels of granularity. This work’s primary focus concerns the analysis of learned representations using multiple interpretability techniques derived from the natural language processing (NLP) literature and the study of abstraction and generalization capabilities of modern computational models of language. A model-driven approach is adopted throughout this study, following the intuition that learned representations can be leveraged as proxies of the informational content required to perform linguistic complexity assessment. The modeling of linguistic complexity is studied on multiple extensively-used corpora spanning three complexity-related tasks – perceived complexity prediction, automatic readability assessment, and gaze metrics prediction – and further validated on ad-hoc psycholinguistic test suites. To further validate the impact of structural factors for complexity assessment, neural network-based annotation pipelines are notably employed alongside neural language models as black-box feature extraction systems.\nChapter 1 marks the beginning of this work by introducing the reader to the multiple facets of linguistic complexity. It starts with a broad categorization of complexity measurements into a spectrum taking into account both the perspective of analysis (intrinsic or extrinsic) and the processing modalities (online or offline). Relevant intrinsic perspectives related to linguistic complexity are then briefly presented, focusing on the extraction and use of morphosyntactic structures in complexity studies and the use of information-theoretic surprisal from language models as a structural measure of complexity. The three extrinsic complexity tasks representing this study’s focus and their respective corpora are introduced in detail, focusing on their differences both from a conceptual and a data collection perspective. The chapter ends with an introduction to garden-path sentences, peculiar syntactic constructs associated with cognitive processing difficulties, later employed in the experiments of Chapter 5.\nChapter 2 motivates the choice of NLMs as the critical component in our experimental analysis: their ability to encode both semantic and structural properties of language makes them especially suitable in the context of linguistic complexity modeling. After a summary of the ascent of NLMs in the field of NLP, the two neural language models used in experimental sections are presented in detail. To conclude, three interpretability approaches are used to leverage learned representations to study complexity learning across tasks, and abstraction layers are presented.\nChapter 3 is the first experimental section, in which perceived complexity annotations and eye-tracking metrics collected at sentence level are linked to various linguistic phenomena extracted by a linguistic parser. The same analysis is also performed by controlling sentence length to limit the disproportionate influence of length-related features on complexity measures. The predictive performances of NLMs are then evaluated on perceived complexity and various eye-tracking metrics for both length-controlled and unconditional settings. The chapter ends with probing task experiments highlighting how complexity-related linguistic properties become implicitly encoded in model representations after complexity learning, suggesting interesting perspectives in priming models with syntactic information to improve their performances on complexity-related tasks.\nChapter 4 builds upon previous chapters’ intuitions to compare the contextual embeddings generated from a single corpus by multiple models trained on the different complexity-related tasks. First, a set of assumptions is formulated to guide the empirical evaluation of how models encode complexity properties after fine-tuning. Similarity scores are then computed layer-wise across language models using two interpretability approaches to evaluate whether the information shared across different complexity perspectives is encoded by models with different fine-tuning objectives. Finally, learned representations are compared across model layers and fine-tuning tasks to highlight whether and how fine-tuning objectives influence the abstraction hierarchy learned by language models.\nChapter 5 concludes the experimental portion of this work by studying the connection between eye-tracking metrics and language modeling surprisal and investigating whether gaze metrics fine-tuning can enable language models to individuate cognitive processing triggers like garden-path sentences. A data-driven strategy is first adopted to establish a conversion coefficient between surprisal units and reading times. This coefficient is then used to evaluate whether a model that correctly highlights increased cognitive processing in specific constructions can also predict the magnitude of such phenomena. Autoregressive and masked language models are fine-tuned on eye-tracking measurements and then leveraged in a zero-shot setting to evaluate their ability in replicating garden-path effects in a controlled setting. Finally, models’ performances are evaluated on a set of psycholinguistic benchmarks using surprisal and gaze recordings predictions to estimate the presence and magnitude of garden-path effects.\nWhile studies on natural language complexity usually adopt a cross-lingual perspective, either by performing typological comparisons across language families or studying the impact of interlingual contacts on complexity changes, this work focuses solely on analyzing complexity annotations produced by native speakers of English. The English language was selected due to the broad availability of open-source corpora and resources, and no other languages were included in the study to keep it as self-contained as possible. Readers should be aware that English is widely considered morphologically and inflectionally poor despite its ubiquity in language studies, even compared to its Indo-European siblings. It should thus be avoided to generalize the results of this thesis work to other language families and typologies.1 Moreover, this study focuses on the written language paradigm, but the importance of phonological phenomena in spoken language in evaluating language complexity is acknowledged (McWhorter 2001).\nThis thesis work should be regarded as a broad, high-level exploration of multiple linguistic complexity perspectives employing modern computational approaches. In this sense, both introductory and experimental chapters are not intended to be exhaustive in providing a complete overview of the discussed topics. Instead, they aim to provide the minimal context needed to interpret experimental results correctly. Introductory chapters include pointers to additional resources discussing linguistic complexity for curious readers, and future studies on these topics will likely encompass any other perspective that was not covered by the present work.\nReferences McWhorter, John H. 2001. “The Worlds Simplest Grammars Are Creole Grammars.” Linguistic Typology 5 (2-3). De Gruyter Mouton: 125–66.\nSee Ruder (2020) for the importance of multilingual studies in NLP.↩\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"d9ca9611a6ab477827897b1f97828f8d","permalink":"https://gsarti.com/msc-thesis/introduction/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/msc-thesis/introduction/","section":"msc-thesis","summary":"\u003c!DOCTYPE html\u003e Interpreting Neural Language Models for Linguistic Complexity Assessment Introduction 1 Linguistic Complexity 1.1 Categorizing Linguistic Complexity Measures 1.2 Intrinsic Perspective 1.2.1 Structural Linguistic Complexity 1.2.2 Language Modeling Surprisal 1.3 Extrinsic Perspective 1.3.1 Automatic Readability Assessment 1.3.2 Perceived Complexity Prediction 1.3.3 Gaze Metrics Prediction 1.4 Garden-path Sentences 2 Models of Linguistic Complexity 2.1 Desiderata for Models of Linguistic Complexity 2.2 Neural Language Models: Unsupervised Multitask Learners 2.2.1 Emergent Linguistic Structures in Neural Language Models 2.","tags":null,"title":"","type":"msc-thesis"},{"authors":null,"categories":null,"content":"\u003c!DOCTYPE html\u003e References | Interpreting Neural Language Models for Linguistic Complexity Assessment Introduction 1 Linguistic Complexity 1.1 Categorizing Linguistic Complexity Measures 1.2 Intrinsic Perspective 1.2.1 Structural Linguistic Complexity 1.2.2 Language Modeling Surprisal 1.3 Extrinsic Perspective 1.3.1 Automatic Readability Assessment 1.3.2 Perceived Complexity Prediction 1.3.3 Gaze Metrics Prediction 1.4 Garden-path Sentences 2 Models of Linguistic Complexity 2.1 Desiderata for Models of Linguistic Complexity 2.2 Neural Language Models: Unsupervised Multitask Learners 2.2.1 Emergent Linguistic Structures in Neural Language Models 2.3 Analyzing Neural Models of Complexity 2.3.1 Probing classifiers 2.3.2 Representational Similarity Analysis 2.3.3 Projection-Weighted Canonical Correlation Analysis 3 Complexity Phenomena in Linguistic Annotations and Language Models 3.1 Data and Preprocessing 3.2 Analysis of Linguistic Phenomena 3.2.1 Linguistic Phenomena in Length-controlled Bins 3.3 Modeling Online and Offline Linguistic Complexity 3.3.1 Modeling Complexity in Length-controlled Bins 3.4 Probing Linguistic Phenomena in ALBERT Representations 3.5 Summary 4 Representational Similarity in Models of Complexity 4.1 Knowledge-driven Requirements for Learning Models 4.2 Experimentsl Evaluation 4.2.1 Data 4.2.2 Inter-model Representational Similarity 4.2.3 Intra-model Representational Similarity 4.3 Summary 5 Gaze-informed Models for Cognitive Processing Prediction 5.1 Experimental Setup 5.2 Experimental Evaluation 5.2.1 Estimating Magnitudes of Garden-path Delays 5.2.2 Predicting Delays with Surprisal and Gaze Metrics 5.3 Summary Conclusion Broader Impact and Ethical Perspectives Future Directions Appendix A Linguistic Features A.1 Raw Text Properties and Lexical Variety A.2 Morpho-syntacting Information A.3 Verbal Predicate Structure A.4 Global and Local Parsed Tree Structures A.5 Syntactic Relations A.6 Subordination Phenomena B Precisions on Eye-tracking Metrics and Preprocessing C Multi-task Token-level Regression for Gaze Metrics Prediction D Intra-model Similarity for All Models E Gaze Metrics Predictions for Garden Path Sentences F Reproducibility and Environmental Impact References Back to my website Interpreting Neural Language Models\nfor Linguistic Complexity Assessment References Abdou, Mostafa, Artur Kulmizev, Felix Hill, Daniel M. Low, and Anders Søgaard. 2019. “Higher-Order Comparisons of Sentence Encoder Representations.” In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (Emnlp-Ijcnlp), 5838–45. Hong Kong, China: Association for Computational Linguistics. https://doi.org/10.18653/v1/D19-1593.\nAbnar, Samira. 2020. “Visualizing Model Comparison.” Blog Post. https://samiraabnar.github.io/articles/2020-05/vizualization.\nAbnar, Samira, Lisa Beinborn, Rochelle Choenni, and Willem Zuidema. 2019. “Blackbox Meets Blackbox: Representational Similarity \u0026amp; Stability Analysis of Neural Language Models and Brains.” In Proceedings of the 2019 Acl Workshop Blackboxnlp: Analyzing and Interpreting Neural Networks for Nlp, 191–203. Florence, Italy: Association for Computational Linguistics. https://doi.org/10.18653/v1/W19-4820.\nAbnar, Samira, Mostafa Dehghani, and Willem Zuidema. 2020. “Transferring Inductive Biases Through Knowledge Distillation.” ArXiv Pre-Print 2006.00555. https://arxiv.org/abs/2006.00555.\nAlain, Guillaume, and Yoshua Bengio. 2016. “Understanding Intermediate Layers Using Linear Classifier Probes.” ArXiv Pre-Print 1610.01644. https://arxiv.org/abs/1610.01644.\nAlammar, Jay. 2018a. “The Illustrated Bert, Elmo, and Co. (How NLP Cracked Transfer Learning).” Blog Post. https://jalammar.github.io/illustrated-bert/.\n———. 2018b. “The Illustrated Gpt-2.” Blog Post. https://http://jalammar.github.io/illustrated-gpt2/.\nAmbati, Bharat Ram, Siva Reddy, and Mark Steedman. 2016. “Assessing Relative Sentence Complexity Using an Incremental CCG Parser.” In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 1051–7. San Diego, California: Association for Computational Linguistics. https://doi.org/10.18653/v1/N16-1120.\nAndreas, Jacob, and Dan Klein. 2014. “How Much Do Word Embeddings Encode About Syntax?” In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), 822–27. Baltimore, Maryland: Association for Computational Linguistics. https://doi.org/10.3115/v1/P14-2133.\nBa, Jimmy, J. Kiros, and Geoffrey E. Hinton. 2016. “Layer Normalization.” ArXiv Pre-Print 1607.06450. https://arxiv.org/abs/1607.06450.\nBahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio. 2015. “Neural Machine Translation by Jointly Learning to Align and Translate.” In Proceeding of the 3rd International Conference on Learning Representations (ICLR’15).\nBarrett, Maria, Joachim Bingel, Nora Hollenstein, Marek Rei, and Anders Søgaard. 2018. “Sequence Classification with Human Attention.” In Proceedings of the 22nd Conference on Computational Natural Language Learning, 302–12. Brussels, Belgium: Association for Computational Linguistics. https://doi.org/10.18653/v1/K18-1030.\nBarrett, Maria, Joachim Bingel, Frank Keller, and Anders Søgaard. 2016. “Weakly Supervised Part-of-Speech Tagging Using Eye-Tracking Data.” In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), 579–84. Berlin, Germany: Association for Computational Linguistics. https://doi.org/10.18653/v1/P16-2094.\nBelinkov, Yonatan, Sebastian Gehrmann, and Ellie Pavlick. 2020. “Interpretability and Analysis in Neural NLP.” In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts, 1–5. Online: Association for Computational Linguistics. https://doi.org/10.18653/v1/2020.acl-tutorials.1.\nBelinkov, Yonatan, and James Glass. 2019. “Analysis Methods in Neural Language Processing: A Survey.” Transactions of the Association for Computational Linguistics (TACL) 7: 49–72. https://doi.org/10.1162/tacl\\_a\\_00254.\nBender, Emily M., and Alexander Koller. 2020. “Climbing Towards NLU: On Meaning, Form, and Understanding in the Age of Data.” In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, 5185–98. Online: Association for Computational Linguistics. https://doi.org/10.18653/v1/2020.acl-main.463.\nBerruto, Gaetano, and Massimo Simone Cerruti. 2011. La Linguistica. Un Corso Introduttivo. De Agostini.\nBerzak, Yevgeni, Boris Katz, and Roger Levy. 2018. “Assessing Language Proficiency from Eye Movements in Reading.” In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), 1986–96. New Orleans, Louisiana: Association for Computational Linguistics. https://doi.org/10.18653/v1/N18-1180.\nBever, Thomas G. 1970. “The Cognitive Basis for Linguistic Structures.” Cognition and the Development of Language. Wiley.\nBox, George EP. 1976. “Science and Statistics.” Journal of the American Statistical Association 71 (356). Taylor \u0026amp; Francis: 791–99.\nBrunato, Dominique, Andrea Cimino, Felice Dell’Orletta, Giulia Venturi, and Simonetta Montemagni. 2020. “Profiling-UD: A Tool for Linguistic Profiling of Texts.” In Proceedings of the 12th Language Resources and Evaluation Conference, 7145–51. Marseille, France: European Language Resources Association. https://www.aclweb.org/anthology/2020.lrec-1.883.\nBrunato, Dominique, Lorenzo De Mattei, Felice Dell’Orletta, Benedetta Iavarone, and Giulia Venturi. 2018. “Is This Sentence Difficult? Do You Agree?” In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, 2690–9. Brussels, Belgium: Association for Computational Linguistics. https://doi.org/10.18653/v1/D18-1289.\nCangelosi, Angelo, and Huck Turner. 2002. “L’emergere Del Linguaggio.” Scienze Della Mente.\nCarr, Jon W, Valentina N Pescuma, Michele Furlan, Maria Ktori, and Davide Crepaldi. 2020. “Algorithms for the Automated Correction of Vertical Drift in Eye Tracking Data.” OSF Preprints, June. osf.io/jg3nc.\nCaruana, Rich. 1997. “Multitask Learning.” Machine Learning 28: 41–75. https://www.cs.utexas.edu/~kuipers/readings/Caruana-mlj-97.pdf.\nChristie, Agatha. 2003. The Mysterious Affair at Styles: A Detective Story. Modern Library.\nCollins-Thompson, Kevyn. 2014. “Computational Assessment of Text Readability: A Survey of Current and Future Research.” ITL-International Journal of Applied Linguistics 165 (2). John Benjamins: 97–135. http://www-personal.umich.edu/~kevynct/pubs/ITL-readability-invited-article-v10-camera.pdf.\nConneau, Alexis, German Kruszewski, Guillaume Lample, Loı̈c Barrault, and Marco Baroni. 2018. “What You Can Cram into a Single $\u0026amp;!#* Vector: Probing Sentence Embeddings for Linguistic Properties.” In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2126–36. Melbourne, Australia: Association for Computational Linguistics. https://doi.org/10.18653/v1/P18-1198.\nCop, Uschi, Nicolas Dirix, Denis Drieghe, and Wouter Duyck. 2017. “Presenting Geco: An Eyetracking Corpus of Monolingual and Bilingual Sentence Reading.” Behavior Research Methods 49 (2). Springer: 602–15.\nCulotta, Aron, Andrew McCallum, and Jonathan Betz. 2006. “Integrating Probabilistic Extraction Models and Data Mining to Discover Relations and Patterns in Text.” In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference, 296–303. New York City, USA: Association for Computational Linguistics. https://www.aclweb.org/anthology/N06-1038.\nDay, Matthew. 2004. “Religion, Off-Line Cognition and the Extended Mind.” Journal of Cognition and Culture 4 (1). Brill: 101–21.\nDemberg, Vera, and Frank Keller. 2008. “Data from Eye-Tracking Corpora as Evidence for Theories of Syntactic Processing Complexity.” Cognition 109 (2). Elsevier: 193–210.\nDevlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. “BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding.” In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), 4171–86. Minneapolis, Minnesota: Association for Computational Linguistics. https://doi.org/10.18653/v1/N19-1423.\nEdmonds, Bruce M. 1999. “Syntactic Measures of Complexity.” PhD thesis, University of Manchester Manchester, UK.\nEisape, Tiwalayo, Noga Zaslavsky, and Roger Levy. 2020. “Cloze Distillation Improves Psychometric Predictive Power.” In Proceedings of the 24th Conference on Computational Natural Language Learning, 609–19. Online: Association for Computational Linguistics. https://www.aclweb.org/anthology/2020.conll-1.49.\nEisenstein, Jacob. 2019. Introduction to Natural Language Processing. MIT press.\nElman, Jeffrey L. 1991. “Distributed Representations, Simple Recurrent Networks, and Grammatical Structure.” Machine Learning 7 (2-3). Springer: 195–225.\nEttinger, Allyson. 2020. “What BERT Is Not: Lessons from a New Suite of Psycholinguistic Diagnostics for Language Models.” Transactions of the Association for Computational Linguistics 8: 34–48. https://doi.org/10.1162/tacl_a_00298.\nFine, Alex B, T Florian Jaeger, Thomas A Farmer, and Ting Qian. 2013. “Rapid Expectation Adaptation During Syntactic Comprehension.” PloS One 8 (10). Public Library of Science: e77661.\nFrankle, Jonathan, and Michael Carbin. 2018. “The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks.” In Proceedings of the 8th International Conference on Learning Representations (Iclr’18).\nFrazier, Lyn. 1978. “On Comprehending Sentences: Syntactic Parsing Strategies.” PhD thesis, University of Connecticut.\nFrazier, Lyn, and Janet Dean Fodor. 1978. “The Sausage Machine: A New Two-Stage Parsing Model.” Cognition 6 (4). Elsevier: 291–325.\nFutrell, Richard, Edward Gibson, and Roger P Levy. 2020. “Lossy-Context Surprisal: An Information-Theoretic Model of Memory Effects in Sentence Processing.” Cognitive Science 44 (3). Wiley Online Library: e12814.\nFutrell, Richard, Ethan Wilcox, Takashi Morita, Peng Qian, Miguel Ballesteros, and Roger Levy. 2019. “Neural Language Models as Psycholinguistic Subjects: Representations of Syntactic State.” In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), 32–42. Minneapolis, Minnesota: Association for Computational Linguistics. https://doi.org/10.18653/v1/N19-1004.\nGauthier, Jon, Jennifer Hu, Ethan Wilcox, Peng Qian, and Roger Levy. 2020. “SyntaxGym: An Online Platform for Targeted Evaluation of Language Models.” In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, 70–76. Online: Association for Computational Linguistics. https://doi.org/10.18653/v1/2020.acl-demos.10.\nGauthier, Jon, and Roger Levy. 2019. “Linking Artificial and Human Neural Representations of Language.” In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (Emnlp-Ijcnlp), 529–39. Hong Kong, China: Association for Computational Linguistics. https://doi.org/10.18653/v1/D19-1050.\nGibson, Edward. 1991. “A Computational Theory of Human Linguistic Processing: Memory Limitations and Processing Breakdown.” PhD thesis, Pittsburgh, PA: Carnegie Mellon University.\n———. 1998. “Linguistic Complexity: Locality of Syntactic Dependencies.” Cognition 68 (1). Elsevier: 1–76.\n———. 2000. “The Dependency Locality Theory: A Distance-Based Theory of Linguistic Complexity.” Image, Language, Brain 2000: 95–126.\nGlavas, Goran, and Ivan Vulic. 2020. “Is Supervised Syntactic Parsing Beneficial for Language Understanding? An Empirical Investigation.” ArXiv Pre-Print 2008.06788. https://arxiv.org/abs/2008.06788.\nGonzález-Garduño, Ana Valeria, and Anders Søgaard. 2018. “Learning to Predict Readability Using Eye-Movement Data from Natives and Learners.” AAAI Conference on Artificial Intelligence.\nGoodfellow, Ian, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. 2016. Deep Learning. MIT Press Cambridge.\nGoodman, Joshua. 2001. “A Bit of Progress in Language Modeling.” arXiv Preprint Cs/0108005.\nGrodner, Daniel, Edward Gibson, Vered Argaman, and Maria Babyonyshev. 2003. “Against Repair-Based Reanalysis in Sentence Comprehension.” Journal of Psycholinguistic Research 32 (2). Springer: 141–66.\nGulordava, Kristina, Piotr Bojanowski, Edouard Grave, Tal Linzen, and Marco Baroni. 2018. “Colorless Green Recurrent Networks Dream Hierarchically.” In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), 1195–1205. New Orleans, Louisiana: Association for Computational Linguistics. https://doi.org/10.18653/v1/N18-1108.\nHale, John. 2001. “A Probabilistic Earley Parser as a Psycholinguistic Model.” In Second Meeting of the North American Chapter of the Association for Computational Linguistics.\n———. 2016. “Information-Theoretical Complexity Metrics.” Language and Linguistics Compass 10 (9). Wiley Online Library: 397–412.\nHauser, Marc D, Noam Chomsky, and W Tecumseh Fitch. 2002. “The Faculty of Language: What Is It, Who Has It, and How Did It Evolve?” Science 298 (5598). American Association for the Advancement of Science: 1569–79.\nHendrycks, Dan, and Kevin Gimpel. 2016. “Gaussian Error Linear Units (Gelus).” ArXiv Pre-Print 1606.08415. https://arxiv.org/abs/1606.08415.\nHewitt, John, and Percy Liang. 2019. “Designing and Interpreting Probes with Control Tasks.” In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (Emnlp-Ijcnlp), 2733–43. Hong Kong, China: Association for Computational Linguistics. https://doi.org/10.18653/v1/D19-1275.\nHewitt, John, and Christopher D. Manning. 2019. “A Structural Probe for Finding Syntax in Word Representations.” In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), 4129–38. Minneapolis, Minnesota: Association for Computational Linguistics. https://doi.org/10.18653/v1/N19-1419.\nHinton, Geoffrey, Oriol Vinyals, and Jeff Dean. 2015. “Distilling the Knowledge in a Neural Network.” ArXiv Pre-Print 1503.02531. https://arxiv.org/abs/1503.02531.\nHochreiter, Sepp, and Jürgen Schmidhuber. 1997. “Long Short-Term Memory.” Neural Computation 9 (8). MIT Press: 1735–80.\nHollenstein, Nora, Maria Barrett, and Lisa Beinborn. 2020. “Towards Best Practices for Leveraging Human Language Processing Signals for Natural Language Processing.” In Proceedings of the Second Workshop on Linguistic and Neurocognitive Resources, 15–27. Marseille, France: European Language Resources Association. https://www.aclweb.org/anthology/2020.lincr-1.3.\nHollenstein, Nora, Jonathan Rotsztejn, Marius Troendle, Andreas Pedroni, Ce Zhang, and Nicolas Langer. 2018. “ZuCo, a Simultaneous Eeg and Eye-Tracking Resource for Natural Sentence Reading.” Scientific Data 5 (1). Nature Publishing Group: 1–13.\nHollenstein, Nora, Antonio de la Torre, Nicolas Langer, and Ce Zhang. 2019. “CogniVal: A Framework for Cognitive Word Embedding Evaluation.” In Proceedings of the 23rd Conference on Computational Natural Language Learning (Conll), 538–49. Hong Kong, China: Association for Computational Linguistics. https://doi.org/10.18653/v1/K19-1050.\nHollenstein, Nora, Marius Troendle, Ce Zhang, and Nicolas Langer. 2020. “ZuCo 2.0: A Dataset of Physiological Recordings During Natural Reading and Annotation.” In Proceedings of the 12th Language Resources and Evaluation Conference, 138–46. Marseille, France: European Language Resources Association. https://www.aclweb.org/anthology/2020.lrec-1.18.\nHollenstein, Nora, and Ce Zhang. 2019. “Entity Recognition at First Sight: Improving NER with Eye Movement Information.” In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), 1–10. Minneapolis, Minnesota: Association for Computational Linguistics. https://doi.org/10.18653/v1/N19-1001.\nHoward, Jeremy, and Sebastian Ruder. 2018. “Universal Language Model Fine-Tuning for Text Classification.” In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 328–39. Melbourne, Australia: Association for Computational Linguistics. https://doi.org/10.18653/v1/P18-1031.\nHu, Jennifer, Jon Gauthier, Peng Qian, Ethan Wilcox, and Roger Levy. 2020. “A Systematic Assessment of Syntactic Generalization in Neural Language Models.” In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, 1725–44. Online: Association for Computational Linguistics. https://doi.org/10.18653/v1/2020.acl-main.158.\nIverson, Jana M, and Esther Thelen. 1999. “Hand, Mouth and Brain. The Dynamic Emergence of Speech and Gesture.” Journal of Consciousness Studies 6 (11-12). Imprint Academic: 19–40.\nJawahar, Ganesh, Benoit Sagot, and Djamé Seddah. 2019. “What Does BERT Learn About the Structure of Language?” In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, 3651–7. Florence, Italy: Association for Computational Linguistics. https://doi.org/10.18653/v1/P19-1356.\nJurafsky, Daniel. 1996. “A Probabilistic Model of Lexical and Syntactic Access and Disambiguation.” Cognitive Science 20 (2). Wiley Online Library: 137–94.\nKennedy, Alan, Robin Hill, and Joël Pynte. 2003. “The Dundee Corpus.” In Proceedings of the 12th European Conference on Eye Movement.\nKriegeskorte, N., M. Mur, and P. Bandettini. 2008. “Representational Similarity Analysis – Connecting the Branches of Systems Neuroscience.” Frontiers in Systems Neuroscience 2. https://doi.org/10.3389/neuro.06.004.2008.\nKudo, Taku, and John Richardson. 2018. “SentencePiece: A Simple and Language Independent Subword Tokenizer and Detokenizer for Neural Text Processing.” In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, 66–71. Brussels, Belgium: Association for Computational Linguistics. https://doi.org/10.18653/v1/D18-2012.\nKusters, Wouter. 2003. “Linguistic Complexity.” PhD thesis, Netherlands Graduate School of Linguistics.\n———. 2008. “Complexity in Linguistic Theory, Language Learning and Language Change.” In Language Complexity: Typology, Contact, Change, 3–22. John Benjamins Amsterdam, The Netherlands.\nLaakso, Aarre, and Garrison Cottrell. 2000. “Content and Cluster Analysis: Assessing Representational Similarity in Neural Systems.” Philosophical Psychology 13 (1). Taylor \u0026amp; Francis: 47–76.\nLacoste, Alexandre, Alexandra Luccioni, Victor Schmidt, and Thomas Dandres. 2019. “Quantifying the Carbon Emissions of Machine Learning.” ArXiv Pre-Print 1910.09700.\nLan, Zhenzhong, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. 2020. “ALBERT: A Lite BERT for Self-Supervised Learning of Language Representations.” In International Conference on Learning Representations. https://openreview.net/forum?id=H1eA7AEtvS.\nLevy, Roger. 2008. “Expectation-Based Syntactic Comprehension.” Cognition 106 (3). Elsevier: 1126–77.\nLin, Yongjie, Yi Chern Tan, and Robert Frank. 2019. “Open Sesame: Getting Inside BERT’s Linguistic Knowledge.” In Proceedings of the 2019 Acl Workshop Blackboxnlp: Analyzing and Interpreting Neural Networks for Nlp, 241–53. Florence, Italy: Association for Computational Linguistics. https://doi.org/10.18653/v1/W19-4825.\nLinzen, Tal, and Marco Baroni. 2021. “Syntactic Structure from Deep Learning.” Annual Review of Linguistics 7 (1): null. https://doi.org/10.1146/annurev-linguistics-032020-051035.\nLinzen, Tal, Emmanuel Dupoux, and Yoav Goldberg. 2016. “Assessing the Ability of Lstms to Learn Syntax-Sensitive Dependencies.” Transactions of the Association for Computational Linguistics 4. MIT Press: 521–35.\nLiu, Nelson F., Matt Gardner, Yonatan Belinkov, Matthew E. Peters, and Noah A. Smith. 2019. “Linguistic Knowledge and Transferability of Contextual Representations.” In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), 1073–94. Minneapolis, Minnesota: Association for Computational Linguistics. https://doi.org/10.18653/v1/N19-1112.\nLoshchilov, I., and F. Hutter. 2019. “Decoupled Weight Decay Regularization.” In Proceeding of the 7th International Conference on Learning Representations (Iclr’19).\nMartinc, Matej, S. Pollak, and M. Robnik-Sikonja. 2019. “Supervised and Unsupervised Neural Approaches to Text Readability.” ArXiv Pre-Print 1907.11779. https://arxiv.org/abs/1907.11779.\nMcDonald, Ryan, Joakim Nivre, Yvonne Quirmbach-Brundage, Yoav Goldberg, Dipanjan Das, Kuzman Ganchev, Keith Hall, et al. 2013. “Universal Dependency Annotation for Multilingual Parsing.” In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), 92–97. Sofia, Bulgaria: Association for Computational Linguistics. https://www.aclweb.org/anthology/P13-2017.\nMcWhorter, John H. 2001. “The Worlds Simplest Grammars Are Creole Grammars.” Linguistic Typology 5 (2-3). De Gruyter Mouton: 125–66.\nMeyer, Bonnie JF, and G Elizabeth Rice. 1992. “12 Prose Processing in Adulthood: The Text, the Reader, and the Task.” Everyday Cognition in Adulthood and Late Life. Cambridge Univ Pr, 157.\nMiaschi, Alessio, Dominique Brunato, Felice Dell’Orletta, and Giulia Venturi. 2020. “Linguistic Profiling of a Neural Language Model.” In Proceedings of the 28th Conference on Computational Linguistics (Coling). Online: Association for Computational Linguistics. https://arxiv.org/abs/2010.01869.\nMiaschi, Alessio, and Felice Dell’Orletta. 2020. “Contextual and Non-Contextual Word Embeddings: An in-Depth Linguistic Investigation.” In Proceedings of the 5th Workshop on Representation Learning for Nlp, 110–19. Online: Association for Computational Linguistics. https://www.aclweb.org/anthology/2020.repl4nlp-1.15.\nMiestamo, Matti. 2004. “On the Feasibility of Complexity Metrics.” In FinEst Linguistics, Proceedings of the Annual Finnish and Estonian Conference of Linguistics, 11–26. Tallin, Finland.\n———. 2008. “Grammatical Complexity in a Cross-Linguistic Perspective.” In Language Complexity: Typology, Contact, Change, 41. John Benjamins Amsterdam, The Netherlands.\nMikolov, Tomas, Kai Chen, G. S. Corrado, and J. Dean. 2013. “Efficient Estimation of Word Representations in Vector Space.” CoRR abs/1301.3781.\nMikolov, Tomas, M. Karafiát, L. Burget, J. Cernocký, and S. Khudanpur. 2010. “Recurrent Neural Network Based Language Model.” In INTERSPEECH.\nMishra, Abhijit, Kuntal Dey, and Pushpak Bhattacharyya. 2017. “Learning Cognitive Features from Gaze Data for Sentiment and Sarcasm Classification Using Convolutional Neural Network.” In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 377–87. Vancouver, Canada: Association for Computational Linguistics. https://doi.org/10.18653/v1/P17-1035.\nMitchell, Don C. 1984. “An Evaluation of Subject-Paced Reading Tasks and Other Methods for Investigating Immediate Processes in Reading.” New Methods in Reading Comprehension Research, 69–89.\nMorcos, Ari, Maithra Raghu, and Samy Bengio. 2018. “Insights on Representational Similarity in Neural Networks with Canonical Correlation.” In Advances in Neural Information Processing Systems 31, edited by S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, 5727–36. Curran Associates, Inc. http://papers.nips.cc/paper/7815-insights-on-representational-similarity-in-neural-networks-with-canonical-correlation.pdf.\nMoro, Alberto, and Laura Lonza. 2018. “Electricity Carbon Intensity in European Member States: Impacts on Ghg Emissions of Electric Vehicles.” Transportation Research Part D: Transport and Environment 64. Elsevier: 5–14.\nMunro, Robert, Steven Bethard, Victor Kuperman, Vicky Tzuyin Lai, Robin Melnick, Christopher Potts, Tyler Schnoebelen, and Harry Tily. 2010. “Crowdsourcing and Language Studies: The New Generation of Linguistic Data.” In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk, 122–30. Los Angeles: Association for Computational Linguistics. https://www.aclweb.org/anthology/W10-0719.\nNivre, Joakim, Marie-Catherine de Marneffe, Filip Ginter, Yoav Goldberg, Jan Hajič, Christopher D. Manning, Ryan McDonald, et al. 2016. “Universal Dependencies V1: A Multilingual Treebank Collection.” In Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC’16), 1659–66. Portorož, Slovenia: European Language Resources Association (ELRA). https://www.aclweb.org/anthology/L16-1262.\nPascanu, R., Tomas Mikolov, and Yoshua Bengio. 2013. “On the Difficulty of Training Recurrent Neural Networks.” In Proceedings of the 30th International Conference on Machine Learning (Icml’13).\nPennington, Jeffrey, Richard Socher, and Christopher Manning. 2014. “GloVe: Global Vectors for Word Representation.” In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), 1532–43. Doha, Qatar: Association for Computational Linguistics. https://doi.org/10.3115/v1/D14-1162.\nPeters, Matthew, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. “Deep Contextualized Word Representations.” In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), 2227–37. New Orleans, Louisiana: Association for Computational Linguistics. https://doi.org/10.18653/v1/N18-1202.\nPimentel, Tiago, Josef Valvoda, Rowan Hall Maudslay, Ran Zmigrod, Adina Williams, and Ryan Cotterell. 2020. “Information-Theoretic Probing for Linguistic Structure.” In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, 4609–22. Online: Association for Computational Linguistics. https://doi.org/10.18653/v1/2020.acl-main.420.\nPrasad, Grusha, and Tal Linzen. 2019a. “Do Self-Paced Reading Studies Provide Evidence for Rapid Syntactic Adaptation?” PsyArXiv Pre-Print. https://tallinzen.net/media/papers/prasad_linzen_2019_adaptation.pdf.\n———. 2019b. “How Much Harder Are Hard Garden-Path Sentences Than Easy Ones?” OSF Preprint syh3j. https://osf.io/syh3j/.\nRadford, A., Jeffrey Wu, R. Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. “Language Models Are Unsupervised Multitask Learners.” OpenAI Blog. OpenAI.\nRaghu, Maithra, Justin Gilmer, Jason Yosinski, and Jascha Sohl-Dickstein. 2017. “SVCCA: Singular Vector Canonical Correlation Analysis for Deep Learning Dynamics and Interpretability.” In Advances in Neural Information Processing Systems 30, edited by I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, 6076–85. Curran Associates, Inc. http://papers.nips.cc/paper/7188-svcca-singular-vector-canonical-correlation-analysis-for-deep-learning-dynamics-and-interpretability.pdf.\nRajpurkar, Pranav, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. “SQuAD: 100,000+ Questions for Machine Comprehension of Text.” In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, 2383–92. Austin, Texas: Association for Computational Linguistics. https://doi.org/10.18653/v1/D16-1264.\nRayner, Keith. 1998. “Eye Movements in Reading and Information Processing: 20 Years of Research.” Psychological Bulletin 124 (3). American Psychological Association: 372.\nRello, Luz, Susana Bautista, Ricardo Baeza-Yates, Pablo Gervás, Raquel Hervás, and Horacio Saggion. 2013. “One Half or 50%? An Eye-Tracking Study of Number Representation Readability.” In Human-Computer Interaction – Interact 2013, edited by Paula Kotzé, Gary Marsden, Gitte Lindgaard, Janet Wesson, and Marco Winckler, 229–45. Berlin, Heidelberg: Springer Berlin Heidelberg.\nRogers, Anna, O. Kovaleva, and Anna Rumshisky. 2020. “A Primer in Bertology: What We Know About How Bert Works.” ArXiv Pre-Print 2002.12327. https://arxiv.org/abs/2002.12327.\nRuder, Sebastian. 2017. “An Overview of Multi-Task Learning in Deep Neural Networks.” ArXiv Pre-Print 1706.05098. https://arxiv.org/abs/1706.05098.\n———. 2020. “Why You Should Do NLP Beyond English.” Blog Post. http://ruder.io/nlp-beyond-english.\nSamek, W., Grégoire Montavon, A. Vedaldi, L. Hansen, and K. Müller. 2019. “Explainable Ai: Interpreting, Explaining and Visualizing Deep Learning.” Explainable AI: Interpreting, Explaining and Visualizing Deep Learning.\nSanguinetti, Manuela, and Cristina Bosco. 2015. “PartTUT: The Turin University Parallel Treebank.” In Harmonization and Development of Resources and Tools for Italian Natural Language Processing Within the Parli Project, edited by Roberto Basili, Cristina Bosco, Rodolfo Delmonte, Alessandro Moschitti, and Maria Simi, 51–69. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-319-14206-7\\_3.\nSaphra, Naomi, and Adam Lopez. 2019. “Understanding Learning Dynamics of Language Models with SVCCA.” In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), 3257–67. Minneapolis, Minnesota: Association for Computational Linguistics. https://doi.org/10.18653/v1/N19-1329.\nSapir, Edward. 1921. “Language.” London: Harcourt, Brace \u0026amp; World, Inc.\nSarti, Gabriele. 2020. “UmBERTo-MTSA @ AcCompl-It: Improving Complexity and Acceptability Prediction with Multi-Task Learning on Self-Supervised Annotations.” ArXiv Pre-Print 2011.05197. https://arxiv.org/abs/2011.05197.\nSchijndel, Marten van, and Tal Linzen. 2020. “Single-Stage Prediction Models Do Not Explain the Magnitude of Syntactic Disambiguation Difficulty.” PsyArXiv Pre-Print sgbqy. https://psyarxiv.com/sgbqy/.\nSchotter, Elizabeth R. 2018. “Reading Ahead by Hedging Our Bets on Seeing the Future: Eye Tracking and Electrophysiology Evidence for Parafoveal Lexical Processing and Saccadic Control by Partial Word Recognition.” In Psychology of Learning and Motivation, 68:263–98. Elsevier.\n———. 2020. Eye Tracking for Cognitive Science. SISSA Course.\nSchotter, Elizabeth R, Bernhard Angele, and Keith Rayner. 2012. “Parafoveal Processing in Reading.” Attention, Perception, \u0026amp; Psychophysics 74 (1). Springer: 5–35.\nSennrich, Rico, Barry Haddow, and Alexandra Birch. 2016. “Neural Machine Translation of Rare Words with Subword Units.” In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 1715–25. Berlin, Germany: Association for Computational Linguistics. https://doi.org/10.18653/v1/P16-1162.\nShwartz-Ziv, Ravid, and Naftali Tishby. 2017. “Opening the Black Box of Deep Neural Networks via Information.” ArXiv Pre-Print 1703.00810. https://arxiv.org/abs/1703.00810.\nSilveira, Natalia, Timothy Dozat, Marie-Catherine de Marneffe, Samuel Bowman, Miriam Connor, John Bauer, and Chris Manning. 2014. “A Gold Standard Dependency Corpus for English.” In Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC’14), 2897–2904. Reykjavik, Iceland: European Language Resources Association (ELRA). http://www.lrec-conf.org/proceedings/lrec2014/pdf/1089_Paper.pdf.\nSimi, Maria, Cristina Bosco, and Simonetta Montemagni. 2014. “Less Is More? Towards a Reduced Inventory of Categories for Training a Parser for the Italian Stanford Dependencies.” In Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC’14), 83–90. Reykjavik, Iceland: European Language Resources Association (ELRA). http://www.lrec-conf.org/proceedings/lrec2014/pdf/818_Paper.pdf.\nSingh, Abhinav Deep, Poojan Mehta, Samar Husain, and Rajkumar Rajakrishnan. 2016. “Quantifying Sentence Complexity Based on Eye-Tracking Measures.” In Proceedings of the Workshop on Computational Linguistics for Linguistic Complexity (CL4LC), 202–12. Osaka, Japan: The COLING 2016 Organizing Committee. https://www.aclweb.org/anthology/W16-4123.\nSinnemäki, Kaius. 2011. “Language Universals and Linguistic Complexity: Three Case Studies in Core Argument Marking.” PhD thesis, University of Helsinki.\nSmith, Nathaniel J, and Roger Levy. 2013. “The Effect of Word Predictability on Reading Time Is Logarithmic.” Cognition 128 (3). Elsevier: 302–19.\nSocher, Richard, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. 2013. “Recursive Deep Models for Semantic Compositionality over a Sentiment Treebank.” In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, 1631–42. Seattle, Washington, USA: Association for Computational Linguistics. https://www.aclweb.org/anthology/D13-1170.\nStiennon, Nisan, Long Ouyang, Jeff Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul Christiano. 2020. “Learning to Summarize from Human Feedback.” ArXiv Pre-Print 2009.01325. https://arxiv.org/abs/2009.01325.\nStraka, Milan, Jan Hajič, and Jana Straková. 2016. “UDPipe: Trainable Pipeline for Processing CoNLL-U Files Performing Tokenization, Morphological Analysis, POS Tagging and Parsing.” In Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC’16), 4290–7. Portorož, Slovenia: European Language Resources Association (ELRA). https://www.aclweb.org/anthology/L16-1680.\nStrzyz, Michalina, David Vilares, and Carlos Gómez-Rodríguez. 2019. “Towards Making a Dependency Parser See.” In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (Emnlp-Ijcnlp), 1500–1506. Hong Kong, China: Association for Computational Linguistics. https://doi.org/10.18653/v1/D19-1160.\nSturt, Patrick, Martin J Pickering, and Matthew W Crocker. 1999. “Structural Change and Reanalysis Difficulty in Language Comprehension.” Journal of Memory and Language 40 (1). Elsevier: 136–50.\nSussillo, David, Mark M Churchland, Matthew T Kaufman, and Krishna V Shenoy. 2015. “A Neural Network That Finds a Naturalistic Solution for the Production of Muscle Activity.” Nature Neuroscience 18 (7). Nature Publishing Group: 1025–33.\nTay, Yi, Dara Bahri, Donald Metzler, D. Juan, Zhe Zhao, and Che Zheng. 2020. “Synthesizer: Rethinking Self-Attention in Transformer Models.” ArXiv Pre-Print 2005.00743. https://arxiv.org/abs/2005.00743.\nTaylor, Wilson L. 1953. “‘Cloze Procedure’: A New Tool for Measuring Readability.” Journalism Quarterly 30 (4). SAGE Publications Sage CA: Los Angeles, CA: 415–33.\nTenney, Ian, Dipanjan Das, and Ellie Pavlick. 2019. “BERT Rediscovers the Classical NLP Pipeline.” In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, 4593–4601. Florence, Italy: Association for Computational Linguistics. https://doi.org/10.18653/v1/P19-1452.\nThompson, Bruce. 1984. Canonical Correlation Analysis: Uses and Interpretation. 47. Sage.\nTurian, Joseph, Lev-Arie Ratinov, and Yoshua Bengio. 2010. “Word Representations: A Simple and General Method for Semi-Supervised Learning.” In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, 384–94. Uppsala, Sweden: Association for Computational Linguistics. https://www.aclweb.org/anthology/P10-1040.\nVajjala, Sowmya, and Ivana Lucic. 2019. “On Understanding the Relation Between Expert Annotations of Text Readability and Target Reader Comprehension.” In Proceedings of the Fourteenth Workshop on Innovative Use of Nlp for Building Educational Applications, 349–59. Florence, Italy: Association for Computational Linguistics. https://doi.org/10.18653/v1/W19-4437.\nVajjala, Sowmya, and Ivana Lučić. 2018. “OneStopEnglish Corpus: A New Corpus for Automatic Readability Assessment and Text Simplification.” In Proceedings of the Thirteenth Workshop on Innovative Use of NLP for Building Educational Applications, 297–304. New Orleans, Louisiana: Association for Computational Linguistics. https://doi.org/10.18653/v1/W18-0535.\nVan Schijndel, Marten, and Tal Linzen. 2018. “Modeling Garden Path Effects Without Explicit Hierarchical Syntax.” In Proceedings of the 40th Annual Conference of the Cognitive Science Society, 2600–2605.\nVasishth, Shravan, Titus von der Malsburg, and Felix Engelmann. 2013. “What Eye Movements Can Tell Us About Sentence Comprehension.” Cognitive Science 4 2. Wiley interdisciplinary reviews: 125–34. https://onlinelibrary.wiley.com/doi/full/10.1002/wcs.1209.\nVaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. “Attention Is All You Need.” In Advances in Neural Information Processing Systems 30, edited by I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, 5998–6008. Curran Associates, Inc. http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf.\nVoghera, Miriam. 2001. “Riflessioni Su Semplificazione, Complessità E Modalità Di Trasmissione: Sintassi E Semantica.” Scritto E Parlato. Metodi, Testi E Contesti. Aracne, 65–78.\nVoita, Elena, Rico Sennrich, and Ivan Titov. 2019. “The Bottom-up Evolution of Representations in the Transformer: A Study with Machine Translation and Language Modeling Objectives.” In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (Emnlp-Ijcnlp), 4396–4406. Hong Kong, China: Association for Computational Linguistics. https://doi.org/10.18653/v1/D19-1448.\nWallace, Eric, Yizhong Wang, Sujian Li, Sameer Singh, and Matt Gardner. 2019. “Do NLP Models Know Numbers? Probing Numeracy in Embeddings.” In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (Emnlp-Ijcnlp), 5307–15. Hong Kong, China: Association for Computational Linguistics. https://doi.org/10.18653/v1/D19-1534.\nWang, Alex, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2018. “GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding.” In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, 353–55. Brussels, Belgium: Association for Computational Linguistics. https://doi.org/10.18653/v1/W18-5446.\nWolf, Thomas, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, et al. 2020. “Transformers: State-of-the-Art Natural Language Processing.” In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, 38–45. Online: Association for Computational Linguistics. https://www.aclweb.org/anthology/2020.emnlp-demos.6.\nWu, Y., Mike Schuster, Z. Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey, M. Krikun, et al. 2016. “Google’s Neural Machine Translation System: Bridging the Gap Between Human and Machine Translation.” ArXiv Pre-Print 1609.08144. https://arxiv.org/abs/1609.08144.\nXu, Wei, Chris Callison-Burch, and Courtney Napoles. 2015. “Problems in Current Text Simplification Research: New Data Can Help.” Transactions of the Association for Computational Linguistics 3: 283–97. https://doi.org/10.1162/tacl_a_00139.\nZanzotto, Fabio Massimo, Andrea Santilli, Leonardo Ranaldi, Dario Onorati, Pierfrancesco Tommasino, and Francesca Fallucchi. 2020. “KERMIT: Complementing Transformer Architectures with Encoders of Explicit Syntactic Interpretations.” In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (Emnlp), 256–67. Online: Association for Computational Linguistics. https://www.aclweb.org/anthology/2020.emnlp-main.18.\nZeldes, Amir. 2017. “The GUM Corpus: Creating Multilayer Resources in the Classroom.” Language Resources and Evaluation 51: 581–612.\nZhang, Kelly, and Samuel Bowman. 2018. “Language Modeling Teaches You More Than Translation Does: Lessons Learned Through Auxiliary Syntactic Task Analysis.” In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, 359–61. Brussels, Belgium: Association for Computational Linguistics. https://doi.org/10.18653/v1/W18-5448.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"0eafcc1175f8b0ffc27b90997a60be2a","permalink":"https://gsarti.com/msc-thesis/references/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/msc-thesis/references/","section":"msc-thesis","summary":"\u003c!DOCTYPE html\u003e References | Interpreting Neural Language Models for Linguistic Complexity Assessment Introduction 1 Linguistic Complexity 1.1 Categorizing Linguistic Complexity Measures 1.2 Intrinsic Perspective 1.2.1 Structural Linguistic Complexity 1.2.2 Language Modeling Surprisal 1.3 Extrinsic Perspective 1.3.1 Automatic Readability Assessment 1.3.2 Perceived Complexity Prediction 1.3.3 Gaze Metrics Prediction 1.4 Garden-path Sentences 2 Models of Linguistic Complexity 2.1 Desiderata for Models of Linguistic Complexity 2.2 Neural Language Models: Unsupervised Multitask Learners 2.2.1 Emergent Linguistic Structures in Neural Language Models 2.","tags":null,"title":"","type":"msc-thesis"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"37d819b881416f5acc6779e8981ab193","permalink":"https://gsarti.com/talk/aniti-2025/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/talk/aniti-2025/","section":"talk","summary":"","tags":null,"title":"","type":"talk"}]