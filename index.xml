<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Gabriele Sarti</title>
    <link>http://localhost:1313/</link>
      <atom:link href="http://localhost:1313/index.xml" rel="self" type="application/rss+xml" />
    <description>Gabriele Sarti</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© Gabriele Sarti 2025</copyright><lastBuildDate>Tue, 11 Mar 2025 14:00:00 +0100</lastBuildDate>
    <image>
      <url>http://localhost:1313/img/avatar.jpg</url>
      <title>Gabriele Sarti</title>
      <link>http://localhost:1313/</link>
    </image>
    
    <item>
      <title>Interpreting Context Usage in Generative Language Models</title>
      <link>http://localhost:1313/talk/aniti-2025/</link>
      <pubDate>Tue, 11 Mar 2025 14:00:00 +0100</pubDate>
      <guid>http://localhost:1313/talk/aniti-2025/</guid>
      <description></description>
    </item>
    
    <item>
      <title>QE4PE: Word-level Quality Estimation for Human Post-Editing</title>
      <link>http://localhost:1313/publication/qe4pe/</link>
      <pubDate>Thu, 06 Mar 2025 01:00:00 +0200</pubDate>
      <guid>http://localhost:1313/publication/qe4pe/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Aprire la scatola nera dei modelli del linguaggio: rischi e opportunità</title>
      <link>http://localhost:1313/talk/ai2s-talk-2024/</link>
      <pubDate>Mon, 09 Dec 2024 20:00:00 +0100</pubDate>
      <guid>http://localhost:1313/talk/ai2s-talk-2024/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Non Verbis, Sed Rebus: Large Language Models are Weak Solvers of Italian Rebuses</title>
      <link>http://localhost:1313/talk/verbalized-rebus-clic2024/</link>
      <pubDate>Thu, 05 Dec 2024 15:00:00 +0100</pubDate>
      <guid>http://localhost:1313/talk/verbalized-rebus-clic2024/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Interpretability for Language Models: Current Trends and Applications</title>
      <link>http://localhost:1313/talk/sapienza-data-science-phd-course-2024/</link>
      <pubDate>Tue, 05 Nov 2024 11:00:00 +0100</pubDate>
      <guid>http://localhost:1313/talk/sapienza-data-science-phd-course-2024/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Non Verbis, Sed Rebus: Large Language Models are Weak Solvers of Italian Rebuses</title>
      <link>http://localhost:1313/publication/verbalized-rebus/</link>
      <pubDate>Fri, 02 Aug 2024 01:00:00 +0200</pubDate>
      <guid>http://localhost:1313/publication/verbalized-rebus/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Interpreting Context Usage in Generative Language Models with Inseq, PECoRe and MIRAGE</title>
      <link>http://localhost:1313/talk/cis-lmu-inseq-pecore-2024/</link>
      <pubDate>Tue, 16 Jul 2024 09:00:00 +0100</pubDate>
      <guid>http://localhost:1313/talk/cis-lmu-inseq-pecore-2024/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Multi-property Steering of Large Language Models with Dynamic Activation Composition</title>
      <link>http://localhost:1313/publication/dynamic-activation-composition/</link>
      <pubDate>Wed, 26 Jun 2024 00:00:00 +0200</pubDate>
      <guid>http://localhost:1313/publication/dynamic-activation-composition/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Model Internals-based Answer Attribution for Trustworthy Retrieval-Augmented Generation</title>
      <link>http://localhost:1313/publication/mirage/</link>
      <pubDate>Sat, 15 Jun 2024 00:00:00 +0200</pubDate>
      <guid>http://localhost:1313/publication/mirage/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Interpreting Context Usage in Generative Language Models with Inseq and PECoRe</title>
      <link>http://localhost:1313/talk/polito-inseq-pecore-2024/</link>
      <pubDate>Mon, 20 May 2024 09:00:00 +0100</pubDate>
      <guid>http://localhost:1313/talk/polito-inseq-pecore-2024/</guid>
      <description></description>
    </item>
    
    <item>
      <title>IT5: Text-to-text Pretraining for Italian Language Understanding and Generation</title>
      <link>http://localhost:1313/publication/it5/</link>
      <pubDate>Mon, 20 May 2024 01:00:00 +0200</pubDate>
      <guid>http://localhost:1313/publication/it5/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Quantifying the Plausibility of Context Reliance in Neural Machine Translation</title>
      <link>http://localhost:1313/talk/area-pecore-2024/</link>
      <pubDate>Fri, 17 May 2024 14:00:00 +0100</pubDate>
      <guid>http://localhost:1313/talk/area-pecore-2024/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A Primer on the Inner Workings of Transformer-based Language Models</title>
      <link>http://localhost:1313/publication/transformer-lm-inner-workings/</link>
      <pubDate>Wed, 01 May 2024 00:00:00 +0200</pubDate>
      <guid>http://localhost:1313/publication/transformer-lm-inner-workings/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Quantifying the Plausibility of Context Reliance in Neural Machine Translation</title>
      <link>http://localhost:1313/talk/gronlp-rg-pecore/</link>
      <pubDate>Fri, 26 Apr 2024 13:00:00 +0100</pubDate>
      <guid>http://localhost:1313/talk/gronlp-rg-pecore/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Post-hoc Interpretability for Generative Language Models: Explaining Context Usage in Transformers</title>
      <link>http://localhost:1313/talk/sheffield-seminar-2024/</link>
      <pubDate>Fri, 01 Mar 2024 17:00:00 +0100</pubDate>
      <guid>http://localhost:1313/talk/sheffield-seminar-2024/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Explaining Language Models with Inseq</title>
      <link>http://localhost:1313/talk/indeep-masterclass-nov23/</link>
      <pubDate>Thu, 02 Nov 2023 10:30:00 +0200</pubDate>
      <guid>http://localhost:1313/talk/indeep-masterclass-nov23/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Post-hoc Interpretability for Language Models</title>
      <link>http://localhost:1313/talk/escience-signlp-seminar-oct23/</link>
      <pubDate>Thu, 26 Oct 2023 13:30:00 +0200</pubDate>
      <guid>http://localhost:1313/talk/escience-signlp-seminar-oct23/</guid>
      <description></description>
    </item>
    
    <item>
      <title>DecoderLens: Layerwise Interpretation of Encoder-Decoder Transformers</title>
      <link>http://localhost:1313/publication/decoderlens/</link>
      <pubDate>Thu, 05 Oct 2023 00:00:00 +0200</pubDate>
      <guid>http://localhost:1313/publication/decoderlens/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Quantifying the Plausibility of Context Reliance in Neural Machine Translation</title>
      <link>http://localhost:1313/publication/pecore/</link>
      <pubDate>Mon, 02 Oct 2023 00:00:00 +0200</pubDate>
      <guid>http://localhost:1313/publication/pecore/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Post-hoc Interpretability for NLG &amp; Inseq: an Interpretability Toolkit for Sequence Generation Models</title>
      <link>http://localhost:1313/talk/restcl-2023/</link>
      <pubDate>Sun, 02 Jul 2023 15:00:00 +0100</pubDate>
      <guid>http://localhost:1313/talk/restcl-2023/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Post-hoc Interpretability for Neural Language Models</title>
      <link>http://localhost:1313/talk/cosmo-units-2023/</link>
      <pubDate>Thu, 01 Jun 2023 14:30:00 +0100</pubDate>
      <guid>http://localhost:1313/talk/cosmo-units-2023/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Explaining Neural Language Models from Internal Representations to Model Predictions</title>
      <link>http://localhost:1313/talk/ailc-lcl-2023/</link>
      <pubDate>Wed, 31 May 2023 14:00:00 +0100</pubDate>
      <guid>http://localhost:1313/talk/ailc-lcl-2023/</guid>
      <description></description>
    </item>
    
    <item>
      <title>RAMP: Retrieval and Attribute-Marking Enhanced Prompting for Attribute-Controlled Translation</title>
      <link>http://localhost:1313/publication/ramp/</link>
      <pubDate>Mon, 29 May 2023 09:47:38 +0200</pubDate>
      <guid>http://localhost:1313/publication/ramp/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Post-hoc Interpretability for Neural Language Models</title>
      <link>http://localhost:1313/talk/ailo-xai-2023/</link>
      <pubDate>Tue, 23 May 2023 16:00:00 +0100</pubDate>
      <guid>http://localhost:1313/talk/ailo-xai-2023/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Inseq: An Interpretability Toolkit for Sequence Generation Models</title>
      <link>http://localhost:1313/talk/sapienzanlp-seminar-2023/</link>
      <pubDate>Thu, 06 Apr 2023 15:00:00 +0100</pubDate>
      <guid>http://localhost:1313/talk/sapienzanlp-seminar-2023/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Advanced XAI Techniques and Inseq: An Interpretability Toolkit for Sequence Generation Models</title>
      <link>http://localhost:1313/talk/indeep-meeting-mar23/</link>
      <pubDate>Thu, 23 Mar 2023 14:00:00 +0100</pubDate>
      <guid>http://localhost:1313/talk/indeep-meeting-mar23/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Introducing Inseq: An Interpretability Toolkit for Sequence Generation Models</title>
      <link>http://localhost:1313/talk/gronlp-rg-inseq/</link>
      <pubDate>Fri, 10 Mar 2023 13:00:00 +0100</pubDate>
      <guid>http://localhost:1313/talk/gronlp-rg-inseq/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Are Character-level Translations Worth the Wait? Comparing ByT5 and mT5 for Machine Translation</title>
      <link>http://localhost:1313/publication/char-mt-analysis/</link>
      <pubDate>Tue, 28 Feb 2023 09:47:38 +0200</pubDate>
      <guid>http://localhost:1313/publication/char-mt-analysis/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Inseq: An Interpretability Toolkit for Sequence Generation Models</title>
      <link>http://localhost:1313/publication/inseq/</link>
      <pubDate>Mon, 27 Feb 2023 09:47:38 +0200</pubDate>
      <guid>http://localhost:1313/publication/inseq/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Attributing Context Usage in Language Models</title>
      <link>http://localhost:1313/project/pecore/</link>
      <pubDate>Tue, 13 Dec 2022 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/project/pecore/</guid>
      <description>&lt;p&gt;PECoRe is a framework using the internal properties of generative language models to identify and attribute context usage in their generations. In particular, the framework is composed by two steps: Context-sensitive Token Identification (CTI), where generated tokens are classified as context-sensitive by contrastively comparing their probabilities with and without context, and Contextual Cues Imputation (CCI), where the dependence of token selected in the CTI step is highlighted by using contrastive attribution. The framework is integrated in the &lt;a href=&#34;https://github.com/inseq-team/inseq&#34;&gt;Inseq interpretability library&lt;/a&gt; and can be easily used thanks to the &lt;code&gt;inseq attribute-context&lt;/code&gt; command. The framework is described in detail in the paper &lt;a href=&#34;http://localhost:1313/publication/pecore/&#34;&gt;Quantifying the Plausibility of Context Reliance in Neural Machine Translation&lt;/a&gt;, published at ICLR 2024, and its extension MIRAGE was created to support answer attribution in RAG applications &lt;a href=&#34;http://localhost:1313/publication/mirage/&#34;&gt;Model Internals-based Answer Attribution for Trustworthy Retrieval-Augmented Generation&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Inseq: An Interpretability Toolkit for Sequence Generation Models</title>
      <link>http://localhost:1313/project/inseq/</link>
      <pubDate>Tue, 13 Dec 2022 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/project/inseq/</guid>
      <description>&lt;p&gt;Inseq is a Pytorch-based hackable toolkit to democratize the study of interpretability for sequence generation models. Inseq supports a wide set of models from the 🤗 Transformers library and an ever-growing set of feature attribution methods, leveraging in part the widely-used Captum library. For a quick introduction to common use cases, see the &lt;a href=&#34;https://inseq.readthedocs.io/examples/quickstart.html&#34;&gt;Getting started with Inseq&lt;/a&gt; page.&lt;/p&gt;
&lt;p&gt;Using Inseq, feature attribution maps that can be saved, reloaded, aggregated and visualized either as HTMLs (with Jupyter notebook support) or directly in the console using rich. Besides simple attribution, Inseq also supports features like step score extraction, attribution aggregation and attributed functions customization for more advanced use cases.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Towards User-centric Interpretability of Machine Translation Models</title>
      <link>http://localhost:1313/talk/linguistics-lunch/</link>
      <pubDate>Thu, 20 Oct 2022 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/talk/linguistics-lunch/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Probing Linguistic Knowledge in Italian Neural Language Models across Language Varieties</title>
      <link>http://localhost:1313/publication/italian-transformers/</link>
      <pubDate>Fri, 01 Jul 2022 01:00:00 +0200</pubDate>
      <guid>http://localhost:1313/publication/italian-transformers/</guid>
      <description></description>
    </item>
    
    <item>
      <title>DivEMT: Neural Machine Translation Post-Editing Effort Across Typologically Diverse Languages</title>
      <link>http://localhost:1313/publication/divemt/</link>
      <pubDate>Tue, 24 May 2022 01:00:00 +0200</pubDate>
      <guid>http://localhost:1313/publication/divemt/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Towards User-centric Interpretability of NLP Models</title>
      <link>http://localhost:1313/talk/tech-talk-translated-2022/</link>
      <pubDate>Wed, 18 May 2022 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/talk/tech-talk-translated-2022/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Empowering Human Translators via Interpretable Interactive Neural Machine Translation</title>
      <link>http://localhost:1313/talk/xai4debugging21/</link>
      <pubDate>Tue, 14 Dec 2021 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/talk/xai4debugging21/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Characterizing Linguistic Complexity in Humans and Language Models</title>
      <link>http://localhost:1313/talk/aperitivo-bocconi/</link>
      <pubDate>Fri, 05 Nov 2021 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/talk/aperitivo-bocconi/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Contrastive Language-Image Pre-training for the Italian Language</title>
      <link>http://localhost:1313/publication/clip-italian/</link>
      <pubDate>Thu, 19 Aug 2021 09:47:38 +0200</pubDate>
      <guid>http://localhost:1313/publication/clip-italian/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Contrastive Image-Text Pretraining for Italian</title>
      <link>http://localhost:1313/project/clip-italian/</link>
      <pubDate>Fri, 23 Jul 2021 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/project/clip-italian/</guid>
      <description>&lt;p&gt;CLIP is a multimodel model that can learn to represent images and text jointly in the same space. In this project, we aim to propose the first CLIP model trained on Italian data, that in this context can be considered a low resource language. Using a few techniques, we have been able to fine-tune a SOTA Italian CLIP model with only 1.4 million training samples.&lt;/p&gt;
&lt;p&gt;For more information, refer to our &lt;a href=&#34;https://huggingface.co/spaces/clip-italian/clip-italian-demo&#34;&gt;demo&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Teaching NLP with Bracelets and Restaurant Menus: An Interactive Workshop for Italian Students</title>
      <link>http://localhost:1313/publication/teaching-nlp-bracelets-menus/</link>
      <pubDate>Sun, 06 Jun 2021 09:47:38 +0200</pubDate>
      <guid>http://localhost:1313/publication/teaching-nlp-bracelets-menus/</guid>
      <description></description>
    </item>
    
    <item>
      <title>That Looks Hard: Characterizing Linguistic Complexity in Humans and Language Models</title>
      <link>http://localhost:1313/publication/that-looks-hard/</link>
      <pubDate>Sun, 06 Jun 2021 09:47:38 +0200</pubDate>
      <guid>http://localhost:1313/publication/that-looks-hard/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Activities</title>
      <link>http://localhost:1313/activities/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/activities/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Interpreting Neural Language Models for Linguistic Complexity Assessment</title>
      <link>http://localhost:1313/publication/interpreting-nlms-for-lca/</link>
      <pubDate>Sat, 19 Dec 2020 09:47:38 +0200</pubDate>
      <guid>http://localhost:1313/publication/interpreting-nlms-for-lca/</guid>
      <description></description>
    </item>
    
    <item>
      <title>UmBERTo-MTSA@ AcCompl-It: Improving Complexity and Acceptability Prediction with Multi-task Learning on Self-Supervised Annotations</title>
      <link>http://localhost:1313/publication/umberto-mtsa/</link>
      <pubDate>Sat, 19 Dec 2020 09:47:38 +0200</pubDate>
      <guid>http://localhost:1313/publication/umberto-mtsa/</guid>
      <description></description>
    </item>
    
    <item>
      <title>ETC-NLG: End-to-end Topic-Conditioned Natural Language Generation</title>
      <link>http://localhost:1313/publication/etc-nlg/</link>
      <pubDate>Tue, 01 Dec 2020 09:47:38 +0200</pubDate>
      <guid>http://localhost:1313/publication/etc-nlg/</guid>
      <description></description>
    </item>
    
    <item>
      <title>ICLR 2020 Trends: Better &amp; Faster Transformers for Natural Language Processing</title>
      <link>http://localhost:1313/post/iclr2020-transformers/</link>
      <pubDate>Sun, 03 May 2020 19:27:36 +0200</pubDate>
      <guid>http://localhost:1313/post/iclr2020-transformers/</guid>
      <description>&lt;p&gt;&lt;em&gt;(For an overview of the Transformer, see &lt;a href=&#34;https://jalammar.github.io/illustrated-transformer/&#34;&gt;The Illustrated Transformer&lt;/a&gt; by Jay Alammar )&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The Transformer architecture was first proposed in &lt;a href=&#34;https://arxiv.org/abs/1706.03762&#34;&gt;Attention is All you Need&lt;/a&gt; as a valid alternative to sequential language modeling approaches like &lt;a href=&#34;https://www.researchgate.net/publication/13853244_Long_Short-term_Memory&#34;&gt;LSTMs&lt;/a&gt; and has since then become ubiquitous in the field of Natural Language Processing, pushing the state-of-the-art in most downstream language-related tasks.&lt;/p&gt;
&lt;p&gt;This year&amp;rsquo;s edition of the &lt;a href=&#34;https://iclr.cc/&#34;&gt;International Conference on Learning Representation (ICLR)&lt;/a&gt; brought a lot of promising revisions to the original Transformer and its more recent variants &lt;a href=&#34;https://www.aclweb.org/anthology/N19-1423/&#34;&gt;BERT&lt;/a&gt; and &lt;a href=&#34;https://www.aclweb.org/anthology/P19-1285/&#34;&gt;Transformer-XL&lt;/a&gt;. Proposed improvements address the well-known weaknesses of Transformers, namely:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Optimizing the self-attention computation.&lt;/li&gt;
&lt;li&gt;Injecting linguistically-motivated inductive biases in the model architecture.&lt;/li&gt;
&lt;li&gt;Making the model more parameter and data-efficient.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This post wants to summarize and provide a high-level overview of those contributions, highlighting current trends in the development of better and faster models for Natural Language Processing. All image credits go to their respective paper authors.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;index&#34;&gt;Index&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://localhost:1313/post/iclr2020-transformers/#self-attention-variants&#34;&gt;Self-Attention Variants&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://localhost:1313/post/iclr2020-transformers/#long-short-range-attention&#34;&gt;Long-Short Range Attention&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://localhost:1313/post/iclr2020-transformers/#tree-structured-attention-with-subtree-masking&#34;&gt;Tree-Structured Attention with Subtree Masking&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://localhost:1313/post/iclr2020-transformers/#hashed-attention&#34;&gt;Hashed Attention&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://localhost:1313/post/iclr2020-transformers/#extra-hop-attention&#34;&gt;eXtra Hop Attention&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://localhost:1313/post/iclr2020-transformers/#training-objectives&#34;&gt;Training Objectives&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://localhost:1313/post/iclr2020-transformers/#discriminative-replacement-task&#34;&gt;Discriminative Replacement Task&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://localhost:1313/post/iclr2020-transformers/#word-and-sentence-structural-tasks&#34;&gt;Word and Sentence Structural Tasks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://localhost:1313/post/iclr2020-transformers/#type-constrained-entity-replacement&#34;&gt;Type-Constrained Entity Replacement&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://localhost:1313/post/iclr2020-transformers/#embeddings&#34;&gt;Embeddings&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://localhost:1313/post/iclr2020-transformers/#position-aware-complex-word-embeddings&#34;&gt;Position-Aware Complex Word Embeddings&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://localhost:1313/post/iclr2020-transformers/#hierarchical-embeddings&#34;&gt;Hierarchical Embeddings&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://localhost:1313/post/iclr2020-transformers/#factorized-embedding-parametrization&#34;&gt;Factorized Embedding Parametrization&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://localhost:1313/post/iclr2020-transformers/#model-architecture&#34;&gt;Model Architecture&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://localhost:1313/post/iclr2020-transformers/#compressive-memory&#34;&gt;Compressive Memory&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://localhost:1313/post/iclr2020-transformers/#reversible-layers&#34;&gt;Reversible Layers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://localhost:1313/post/iclr2020-transformers/#cross-layer-parameter-sharing&#34;&gt;Cross-Layer Parameter Sharing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://localhost:1313/post/iclr2020-transformers/#adaptive-depth-estimation&#34;&gt;Adaptive Depth Estimation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://localhost:1313/post/iclr2020-transformers/#conclusion&#34;&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h1 id=&#34;self-attention-variants&#34;&gt;Self-Attention Variants&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Scaled dot-product self-attention&lt;/strong&gt; is one of the main components in the standard Transformer layer, enabling the modelling of dependencies regardless of their distance in the input. The self-attention operation projects an input activation tensor $\bf A$ to queries $Q$ of dimension $d_k$, keys $K$ of dimension $d_k$ and values $V$ of dimension $d_v$, returning a weighted version of $V$:&lt;/p&gt;
&lt;p&gt;$$\tag{1} \text{Attention}(Q,K,V) = \text{softmax}\Big(\frac{QK^T}{\sqrt d_k}\Big)V$$&lt;/p&gt;
&lt;p&gt;In the &lt;strong&gt;multi-head self-attention&lt;/strong&gt; variant, the attention function is applied in parallel to $h$ version of queries, keys and values projected with learned projections $W$, and outputs are finally concatenated and projected again to obtain final values:&lt;/p&gt;
&lt;p&gt;$$\text{MultiHead}(Q,K,V) = \text{Concat}(\text{head}_1,\dots, \text{head}_h)W^O$$&lt;/p&gt;
&lt;p&gt;$$\tag{2} \text{where } \text{head}_i = \text{Attention}(QW_i^Q,KW_i^K,VW_i^V)$$&lt;/p&gt;
&lt;p&gt;This section presents some variants of the self-attention component that make it more efficient and effective in the context of language applications.&lt;/p&gt;
&lt;h3 id=&#34;long-short-range-attention&#34;&gt;Long-Short Range Attention&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Introduced in&lt;/em&gt;: &lt;a href=&#34;https://iclr.cc/virtual_2020/poster_ByeMPlHKPH.html&#34;&gt;Lite Transformer with Long-Short Range Attention&lt;/a&gt; by Wu, Liu et al.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:1313/post/iclr2020-transformers/assets/lsra.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Conventional self-attention is deemed as redundant since it was empirically shown to put excessive emphasis on local relations inside a sentence, which can be modeled more efficiently by a standard convolution, as shown also in &lt;a href=&#34;https://iclr.cc/virtual_2020/poster_HJlnC1rKPB.html&#34;&gt;On the Relationship between Self-Attention and Convolutional Layers&lt;/a&gt;. While the redundancy may help model performances in some cases, it is not suitable for lighter applications.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Long-Short Range Attention (LSRA)&lt;/strong&gt; makes the computation more efficient by splitting the input into two parts along channel dimensions and feeding each to two modules: a &lt;strong&gt;global extractor&lt;/strong&gt; using standard self-attention and a &lt;strong&gt;local extractor&lt;/strong&gt; using light depth-wise convolutions. Authors report a $2\times$ reduced overall computation for the model, making it suitable for mobile settings.&lt;/p&gt;
&lt;h3 id=&#34;tree-structured-attention-with-subtree-masking&#34;&gt;Tree-Structured Attention with Subtree Masking&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Introduced in&lt;/em&gt;: &lt;a href=&#34;https://iclr.cc/virtual_2020/poster_HJxK5pEYvr.html&#34;&gt;Tree-Structured Attention with Hierarchical Accumulation&lt;/a&gt; by Nguyen et al.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:1313/post/iclr2020-transformers/assets/hierarchical_accumulation.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;A weakness of the standard Transformer is the absence of inductive biases to account for the hierarchical structure of language. This is due in part to the difficulty in operating with tree-like structures that are usually modeled by recurrent or recursive mechanisms while maintaining the constant parallel time complexity of self-attention.&lt;/p&gt;
&lt;p&gt;The proposed solution leverages constituency parses of input text to build a tree of hidden states, using &lt;strong&gt;hierarchical accumulation&lt;/strong&gt; to build the value of non-terminals as the aggregation of lower representations in the tree. The final output representation is built by performing a &lt;strong&gt;weighted aggregation&lt;/strong&gt; of branch-level representations.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:1313/post/iclr2020-transformers/assets/subtree_masking.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;An interesting addition is the use of &lt;strong&gt;subtree masking&lt;/strong&gt; to filter out superfluous noise by constraining the attention of each node query only on its subtree descendants. The cost for this inductive bias is an increased computational and memory cost, which is then mitigated using &lt;a href=&#34;http://localhost:1313/post/iclr2020-transformers/#cross-layer-parameter-sharing&#34;&gt;parameter sharing&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;hashed-attention&#34;&gt;Hashed Attention&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Introduced in&lt;/em&gt;: &lt;a href=&#34;https://iclr.cc/virtual_2020/poster_rkgNKkHtvB.html&#34;&gt;Reformer: The Efficient Transformer&lt;/a&gt; by Kitaev et al.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:1313/post/iclr2020-transformers/assets/lsh.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;In the self-attention equation the factor $QK^T$ represents a bottleneck, taking $\mathcal{O}(L^2)$ for input sequences of length $L$ both in computational and memory complexity. This effectively hinders the possibility of modeling long sequences.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Reformer&lt;/strong&gt; proposes to restrict the pool of candidates attended by each query to a small set of neighbors found through &lt;strong&gt;locally-sensitive hashing&lt;/strong&gt;. Since LSH bucketing employs random projections, similar vectors may sometimes fall in different neighborhoods; an approach using multiple parallel rounds of hashing is suggested to mitigate this issue. Using LSH attention reduces the computational cost of the self-attention operation to $\mathcal{O}(L \log L)$, allowing the model to operate on longer sequences.&lt;/p&gt;
&lt;h3 id=&#34;extra-hop-attention&#34;&gt;eXtra Hop Attention&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Introduced in&lt;/em&gt;: &lt;a href=&#34;https://iclr.cc/virtual_2020/poster_r1eIiCNYwS.html&#34;&gt;Transformer-XH: Multi-Evidence Reasoning with eXtra Hop Attention&lt;/a&gt; by Zhao et al.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:1313/post/iclr2020-transformers/assets/extra_hop.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;While Transformers were optimized to operate on single sequences or pairs of sequences, they can hardly generalize to settings where evidence is scattered in multiple pieces of text, as in the challenging task of &lt;strong&gt;multi-hop question answering&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Transformer-XH&lt;/strong&gt; introduces a new variant of attention, &lt;strong&gt;eXtra Hop Attention&lt;/strong&gt;, that can be applied to a graph of text sequences connected by edges (e.g. hyperlinks). This new attention mechanism uses the special &lt;code&gt;[CLS]&lt;/code&gt; token at the beginning of each sequence as an &lt;strong&gt;attention hub&lt;/strong&gt; that attends to all other connected sequences in the graph. The resulting representation is then combined to the one obtained by standard self-attention through a linear projection. The resulting model shows significant improvements for tasks requiring reasoning over graphs, at the cost of the extra computations introduced by the new attention mechanism.&lt;/p&gt;
&lt;h1 id=&#34;training-objectives&#34;&gt;Training Objectives&lt;/h1&gt;
&lt;p&gt;The pre-training of Transformer models is usually achieved by the mean of multiple unsupervised objectives, leverage huge quantities of non-annotated texts. The most common tasks used for this purpose are &lt;strong&gt;autoregressive language modeling&lt;/strong&gt;, also known as standard language modeling (LM), and &lt;strong&gt;autoencoding of masked input&lt;/strong&gt;, often referred to as &lt;strong&gt;masked language modeling (MLM)&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The standard Transformer implementation and its &lt;a href=&#34;https://openai.com/blog/better-language-models/&#34;&gt;GPT variants&lt;/a&gt; adopt the autoregressive approach, leveraging a unidirectional context (forward or backward) inside a sequence $\textbf{x} = (x_1, \dots, x_L)$ to estimate next token probability:&lt;/p&gt;
&lt;p&gt;$$p(\textbf{x}) = \prod_{l=1}^L p(x_l|\textbf{x}_{&amp;lt; or &amp;gt;l})$$&lt;/p&gt;
&lt;p&gt;Instead, BERT-like approaches use a bidirectional context to recover a small fraction of the input that was artificially replaced by special &lt;code&gt;[MASK]&lt;/code&gt; tokens. This variant was shown to be especially effective for downstream natural language understanding tasks.&lt;/p&gt;
&lt;p&gt;Besides word-level modeling, a sentence-level classification task like &lt;strong&gt;next sentence prediction (NSP)&lt;/strong&gt; is usually added to the training procedure since many important language applications require an understanding of the relationship between two sequences.&lt;/p&gt;
&lt;p&gt;While those tasks seem to induce meaningful token and sentence-level representation, many of the approaches covered in this section suggest better alternatives that make learning more efficient and grounded in the structure and the content of the input.&lt;/p&gt;
&lt;h3 id=&#34;discriminative-replacement-task&#34;&gt;Discriminative Replacement Task&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Introduced in&lt;/em&gt;: &lt;a href=&#34;https://iclr.cc/virtual_2020/poster_r1xMH1BtvB.html&#34;&gt;ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators&lt;/a&gt; by Clark et al.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:1313/post/iclr2020-transformers/assets/electra.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The masking strategy used in BERT-like models is quite data inefficient, using only ~15% of the input text to complete the MLM task. However, the percentage of masked data can hardly be increased since having too many masked tokens may degrade the overall context information.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;ELECTRA&lt;/strong&gt; proposes a simple yet effective approach to cope with this inefficiency. A small masked language model is trained and then used as a generator to fill the masked tokens in the input with its predictions, as in normal MLM. However, the new task for the main model will be a &lt;strong&gt;discriminative&lt;/strong&gt; one: instead of predicting masked tokens, the model has to detect which tokens have been replaced by the generator. This allows leveraging the entire input sequence for training. As mentioned by the authors, this approach consistently outperforms MLM pre-training given the same compute budget.&lt;/p&gt;
&lt;h3 id=&#34;word-and-sentence-structural-tasks&#34;&gt;Word and Sentence Structural Tasks&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Introduced in&lt;/em&gt;: &lt;a href=&#34;https://iclr.cc/virtual_2020/poster_BJgQ4lSFPH.html&#34;&gt;StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding&lt;/a&gt; by Wang et al.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:1313/post/iclr2020-transformers/assets/structural_objectives.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;As seen previously, Transformers do not explicitly account for structures present in the input. While tree-structured attention injects a heavy hierarchical bias in the model architecture, &lt;strong&gt;StructBERT&lt;/strong&gt; adopts two lighter but effective approaches to make the resulting representations more aware of the underlying sequentiality of language.&lt;/p&gt;
&lt;p&gt;The first is a &lt;strong&gt;word structural objective&lt;/strong&gt; where trigrams inside the inputs are randomly shuffled, and their original position must be reconstructed. This is done in parallel with normal MLM. The &lt;strong&gt;sentence structural objective&lt;/strong&gt; is a lighter variant of the sentence reordering introduced in &lt;a href=&#34;https://arxiv.org/abs/1907.12412&#34;&gt;ERNIE 2.0&lt;/a&gt; and equal to the &lt;strong&gt;sentence ordering prediction&lt;/strong&gt; introduced in &lt;a href=&#34;https://iclr.cc/virtual_2020/poster_H1eA7AEtvS.html&#34;&gt;ALBERT&lt;/a&gt;: given a pair of sentences $(S_1, S_2)$ as input, we ask the model to discriminate whether $S_2$ precedes, follows or is unrelated to $S_1$. This new task extends the standard NSP, which was deemed as too easy for learning meaningful sentence relations. These additions result in significant improvements over standard benchmarks for natural language understanding.&lt;/p&gt;
&lt;h3 id=&#34;type-constrained-entity-replacement&#34;&gt;Type-Constrained Entity Replacement&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Introduced in&lt;/em&gt;: &lt;a href=&#34;https://iclr.cc/virtual_2020/poster_BJlzm64tDH.html&#34;&gt;Pretrained Encyclopedia: Weakly Supervised Knowledge-Pretrained Language Model&lt;/a&gt; by Xiong et al.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:1313/post/iclr2020-transformers/assets/entity_replacement.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;While it was shown that pre-trained Transformer models implicitly capture real-world knowledge, their standard training objectives do not explicitly take into account the entity-centric information needed for robust reasoning over real-world settings.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Type-constrained entity replacement&lt;/strong&gt; is a weakly supervised approach where random entities in the text are replaced with other entities taken from Wikidata that have the same entity type. The model then uses a discriminative objective similar to the one of &lt;a href=&#34;http://localhost:1313/post/iclr2020-transformers/#discriminative-replacement-task&#34;&gt;ELECTRA&lt;/a&gt; to determine which entities were replaced. This is done along with MLM in a multi-task setup, and authors report significant improvements in settings requiring a deeper entity understanding, such as &lt;strong&gt;open-domain QA&lt;/strong&gt; and &lt;strong&gt;entity typing&lt;/strong&gt;.&lt;/p&gt;
&lt;h1 id=&#34;embeddings&#34;&gt;Embeddings&lt;/h1&gt;
&lt;p&gt;The original Transformer relies on two sets of embeddings to represent the input sequence:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Learned &lt;strong&gt;word embeddings&lt;/strong&gt; for each token present in the vocabulary, used as token vector representations for the model.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Fixed &lt;strong&gt;positional embeddings (PE)&lt;/strong&gt;, used to inject information about the position of tokens in the sequence. For position $\text{pos}$ and dimension $i$, those correspond to sinusoidal periodic functions that were empirically shown to perform on par with learned embeddings, and were chosen to enable extrapolation for longer sequences:&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$PE_{pos, 2i} = \sin(\text{pos}/10000^{2i/d_{model}})$$&lt;/p&gt;
&lt;p&gt;$$PE_{pos, 2i + 1} = \cos(\text{pos}/10000^{2i/d_{model}})$$&lt;/p&gt;
&lt;p&gt;For BERT-like models able to operate on multiple input segments, a third set of learned &lt;strong&gt;segment embeddings&lt;/strong&gt; is used to differentiate tokens belonging to different sentences.&lt;/p&gt;
&lt;p&gt;All those embeddings have the same dimensions and get summed together to obtain an input representation. Approaches introduced in this section aim to inject more structure in the embeddings, or to optimize their dimension for better efficiency.&lt;/p&gt;
&lt;h3 id=&#34;position-aware-complex-word-embeddings&#34;&gt;Position-Aware Complex Word Embeddings&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Introduced in&lt;/em&gt;: &lt;a href=&#34;https://iclr.cc/virtual_2020/poster_Hke-WTVtwr.html&#34;&gt;Encoding word order in complex embeddings&lt;/a&gt; by Wang et al.&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;Complex Word Embeddings&#34; src=&#34;http://localhost:1313/post/iclr2020-transformers/assets/complex_embeddings.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;While PE capture different positions in the input, they do not explicitly take into account the relation between those positions, i.e. ordered relationships such as adjacency or precedence. This problem was already addressed in &lt;a href=&#34;https://www.aclweb.org/anthology/P19-1285/&#34;&gt;Transformer-XL&lt;/a&gt; by leveraging relative distances between words instead of raw position indices.&lt;/p&gt;
&lt;p&gt;A proposed improvement is to generalize word embeddings to continuous functions defined over positions, extending the solutions to the complex-valued domain to benefit from richer representations. The resulting &lt;strong&gt;complex-valued embeddings&lt;/strong&gt; introduce new parameters for amplitudes, frequencies and initial phases that determine various properties of the embedding such as position sensitivity. Empirical results show that the complex embeddings with parameter-sharing schemas outperform previous embedding approaches without a significant increase in the number of trainable parameters.&lt;/p&gt;
&lt;h3 id=&#34;hierarchical-embeddings&#34;&gt;Hierarchical Embeddings&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Introduced in&lt;/em&gt;: &lt;a href=&#34;https://iclr.cc/virtual_2020/poster_HJxK5pEYvr.html&#34;&gt;Tree-Structured Attention with Hierarchical Accumulation&lt;/a&gt; by Nguyen et al.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:1313/post/iclr2020-transformers/assets/hierarchical_embeddings.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;In the overview of &lt;a href=&#34;http://localhost:1313/post/iclr2020-transformers/#tree-structured-attention-with-subtree-masking&#34;&gt;tree-structured attention&lt;/a&gt;, we saw how hierarchical accumulation is used to form a representation based on descendants for nonterminal nodes. This procedure, however, has the disadvantage of not taking into account the hierarchical structure of descendants.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Hierarchical embeddings&lt;/strong&gt; are used to inject this structural bias by concatenating &lt;strong&gt;vertical&lt;/strong&gt; and &lt;strong&gt;horizontal embeddings matrices&lt;/strong&gt; representing respectively hierarchical ordering inside branches and relationships between siblings nodes in a subtree. Those embeddings are shared across attention heads, thus accounting only for 0.1% of the total parameters.&lt;/p&gt;
&lt;h3 id=&#34;factorized-embedding-parametrization&#34;&gt;Factorized Embedding Parametrization&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Introduced in&lt;/em&gt;: &lt;a href=&#34;https://iclr.cc/virtual_2020/poster_H1eA7AEtvS.html&#34;&gt;ALBERT: A Lite BERT for Self-supervised Learning of Language Representations&lt;/a&gt; by Lan et al.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:1313/post/iclr2020-transformers/assets/factorized_embeddings.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;In recent models based on BERT and Transformer-XL the input embeddings size $E$ is tied with the hidden layer size $H$, i.e. $E \equiv H$. This is very impractical since, to augment the expressivity of hidden representations used to learn &lt;em&gt;context-dependent representation&lt;/em&gt;, one should also increase the size of the embedding matrix $\textbf{M} = V \times E$, where $V$ is the vocabulary size. Even for relatively small hidden layer dimensions, this results in billions of parameters that are rarely updated during training.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;ALBERT&lt;/strong&gt; authors propose to insert a projection between $E$ and $V$ to make both dimensions independent, an approach that is especially efficient to reduce the parameter count when $H \gg E$. As a result, an ALBERT base with $E = 128$ and $H = 768$ obtains performances comparable with a BERT base with the same configuration on many downstream tasks, using 21M fewer parameters (89M in Table 3 vs 110M for BERT).&lt;/p&gt;
&lt;h1 id=&#34;model-architecture&#34;&gt;Model Architecture&lt;/h1&gt;
&lt;p&gt;The original Transformer architecture is composed of an encoder and a decoder, each composed by a stacked sequence of identical layers that transform input embeddings in outputs having the same dimension (hence the name Transformer).&lt;/p&gt;
&lt;p&gt;Each layer of the Transformer encoder is composed of two sublayers, a multi-head self-attention mechanism and a feed-forward network, surrounded by residual connections and followed by layer normalization. The decoder includes a third layer that performs multi-head self-attention over the encoder output and modifies the original self-attention sublayer to prevent attending to future context, as required by the autoregressive language modeling objective presented above.&lt;/p&gt;
&lt;p&gt;Bidirectional variants of the Transformer drop the decoder structure and focus solely on the encoder to generate the contextual embeddings needed for various tasks, including MLM.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.aclweb.org/anthology/P19-1285/&#34;&gt;Transformer-XL&lt;/a&gt; notably introduces a notion of &lt;strong&gt;memory&lt;/strong&gt; for Transformer networks, where hidden states obtained in previous segments are weighted with attention and reused to better model long-term dependencies, preventing &lt;strong&gt;context fragmentation&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The following approaches try to build on top of current structures to improve long-range modeling, reduce the parameter count, or optimize the computation performed by the model.&lt;/p&gt;
&lt;h3 id=&#34;compressive-memory&#34;&gt;Compressive Memory&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Introduced in&lt;/em&gt;: &lt;a href=&#34;https://iclr.cc/virtual_2020/poster_SylKikSYDH.html&#34;&gt;Compressive Transformers for Long-Range Sequence Modelling&lt;/a&gt; by Rae et al.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:1313/post/iclr2020-transformers/assets/compressive_memory.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;In Transformer-XL&amp;rsquo;s recurrent memory approach, old memories are discarded to enable the storing of new ones in a first-in-first-out fashion. This method accounts only for recency, not taking into account the relevance of information that might get discarded.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Compressive Transformers&lt;/strong&gt; builds upon the memory notion by adding a new &lt;strong&gt;compressed memory&lt;/strong&gt; that stores coarse representations of older memories instead of discarding them. Authors try multiple alternatives for the compression function, finally selecting an &lt;strong&gt;attention-reconstruction loss&lt;/strong&gt; that discards information that is not attended by the network. The use of compressive memory shows large improvements over the modeling of infrequent words, with empirical evidence of the network learning to preserve salient information through the compression mechanism.&lt;/p&gt;
&lt;h3 id=&#34;reversible-layers&#34;&gt;Reversible Layers&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Introduced in&lt;/em&gt;: &lt;a href=&#34;https://iclr.cc/virtual_2020/poster_rkgNKkHtvB.html&#34;&gt;Reformer: The Efficient Transformer&lt;/a&gt; by Kitaev et al.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:1313/post/iclr2020-transformers/assets/reversible.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;The main idea behind &lt;strong&gt;reversibility&lt;/strong&gt; is to enable the recovering of activations in any layer of the network by using only activations of the following layer and model parameters. This feature is especially interesting when applied to Transformer models since they are usually composed of a large pile of stacked layers and their memory complexity grows linearly with the layer count.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Reformer&lt;/strong&gt; introduces reversibility in the Transformer architecture by combining attention and feed-forward sublayers into a single reversible layer. This allows to store activations only for the topmost layer and recover all the other ones by reversing layers during back-propagation, making the model depth irrelevant memory-wise. Further improvements in memory complexity are achieved by &lt;strong&gt;chunking&lt;/strong&gt; independent computations in feed-forward and reversible layers.&lt;/p&gt;
&lt;h3 id=&#34;cross-layer-parameter-sharing&#34;&gt;Cross-Layer Parameter Sharing&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Introduced in&lt;/em&gt;: &lt;a href=&#34;https://iclr.cc/virtual_2020/poster_H1eA7AEtvS.html&#34;&gt;ALBERT: A Lite BERT for Self-supervised Learning of Language Representations&lt;/a&gt; by Lan et al.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:1313/post/iclr2020-transformers/assets/albert_distances.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;A simple yet very effective approach to greatly reduce the parameter count inside deep Transformer models is to share parameters across multiple layers, as it was shown in the &lt;a href=&#34;https://arxiv.org/abs/1807.03819&#34;&gt;Universal Transformer&lt;/a&gt; paper at ICLR 2019.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;ALBERT&lt;/strong&gt; authors experiment cross-layer parameter sharing for both self-attention and feed-forward sublayers, finding that sharing both weight matrices contributes to bringing down the total parameter count of the model by a factor of $7\times$ (for embedding size $E = 128$) while only slightly affecting final performances. The use of parameter sharing leads to smoother transition across layers and effectively stabilizes network parameters.&lt;/p&gt;
&lt;h3 id=&#34;adaptive-depth-estimation&#34;&gt;Adaptive Depth Estimation&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Introduced in&lt;/em&gt;: &lt;a href=&#34;https://iclr.cc/virtual_2020/poster_SJg7KhVKPH.html&#34;&gt;Depth-Adaptive Transformer&lt;/a&gt; by Elbayad et al.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:1313/post/iclr2020-transformers/assets/depth_adaptive.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Current models perform a fixed number of computations for each input, regardless of the underlying complexity specific to each sequence. This problem was already highlighted in the &lt;a href=&#34;https://arxiv.org/abs/1807.03819&#34;&gt;Universal Transformer&lt;/a&gt;, which proposes a repeated application of the same layer with &lt;strong&gt;adaptive computation time (ACT)&lt;/strong&gt;, but the resulting increase in per-layer weights considerably reduce the overall network speed.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Depth-adaptive Transformer&lt;/strong&gt; solves this issue by encoding a sequence with a standard Transformer encoder and decoding it with a variable number of steps. To do so, a classifier is attached to each repeated layer of the decoder and the whole set is then trained with &lt;strong&gt;aligned&lt;/strong&gt; and &lt;strong&gt;mixed training&lt;/strong&gt; (see image) using the &lt;strong&gt;anytime prediction&lt;/strong&gt; approach first introduced in the field of computer vision. Authors explore different mechanisms to adaptively control the amount of computation both on sequence level and on a per-token basis and conclude that an adaptive reduction of more than 75% of decoder layers can be applied without any loss in accuracy on machine translation tasks.&lt;/p&gt;
&lt;h1 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;Many of the approaches introduced at ICLR 2020 offer widely applicable solutions to specific problems that characterize the original Transformer architecture, ranging from the self-attention computation to the model structure itself.&lt;/p&gt;
&lt;p&gt;Many of these approaches seem promising for future developments of the Transformer and, most importantly, are likely to bring complementary improvements once many of them included in a single architecture.&lt;/p&gt;
&lt;p&gt;My hope for ICLR 2021 is to see more incremental work that puts together already-existing strategies to highlight the most effective combinations between them.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;See also:&lt;/em&gt; &lt;a href=&#34;https://towardsdatascience.com/whats-new-for-transformers-at-the-iclr-2020-conference-4285a4294792&#34;&gt;What’s new for Transformers at the ICLR 2020 Conference?&lt;/a&gt; by Sergi Castella&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Covid-19 Semantic Browser</title>
      <link>http://localhost:1313/project/covid-browser/</link>
      <pubDate>Sat, 11 Apr 2020 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/project/covid-browser/</guid>
      <description>&lt;p&gt;In 2020, more than 3,000 scientific studies have been published on the SARS-CoV-2 virus and on the Covid-19 pathology. The total number of articles on the topic of coronaviruses exceeds 40,000 units. Such a volume of scientific production makes it impossible for doctors and researchers to keep up with the latest discoveries without the support of adequate digital platforms that are currently nowhere in sight.&lt;/p&gt;
&lt;p&gt;To make up for this shortcoming, we propose an artificial intelligence system associated with a web application to perform natural language semantic querying inside the &lt;a href=&#34;https://pages.semanticscholar.org/coronavirus-research&#34;&gt;COVID-19 Open Research Dataset&lt;/a&gt; published by the American nonprofit &lt;a href=&#34;https://allenai.org/&#34;&gt;AllenAI&lt;/a&gt;. The system leverages state-of-the-art neural language models trained on scientific publications in the biomedical domain for optimal retrieval performances. The adoption of the system aims to facilitate knowledge sharing across the scientific community and to accelerate the development of adequate drugs and vaccines to counter the ongoing pandemic.&lt;/p&gt;
&lt;p&gt;The project is led by Gabriele Sarti in collaboration with &lt;a href=&#34;en.areasciencepark.it&#34;&gt;Area Science Park&lt;/a&gt; and the &lt;a href=&#34;ai-lc.it&#34;&gt;Italian Association of Computational Linguistics&lt;/a&gt;. The project used to be publicly available at covidbrowser.areasciencepark.it. You can now refer to the code implementation on &lt;a href=&#34;https://github.com/gsarti/covid-papers-browser&#34;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Neural Language Models: the New Frontier of Natural Language Understanding</title>
      <link>http://localhost:1313/talk/neural-lm/</link>
      <pubDate>Fri, 22 Nov 2019 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/talk/neural-lm/</guid>
      <description></description>
    </item>
    
    <item>
      <title>The Literary Ordnance: When the Writer is an AI</title>
      <link>http://localhost:1313/talk/literary-ordnance/</link>
      <pubDate>Thu, 31 Oct 2019 12:00:41 +0100</pubDate>
      <guid>http://localhost:1313/talk/literary-ordnance/</guid>
      <description></description>
    </item>
    
    <item>
      <title>AItalo Svevo: Letters from an Artificial Intelligence</title>
      <link>http://localhost:1313/project/aitalo-svevo/</link>
      <pubDate>Fri, 27 Sep 2019 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/project/aitalo-svevo/</guid>
      <description>&lt;p&gt;We developed an interactive experience putting together NLP and literature to raise awareness on the latest developments in language modeling and natural language generation. Participants received a printed letter written by a neural language model (GPT-2) fine-tuned on the Italian epistolary corpus of Italo Svevo, an italian writer of the 20th century. Participants could choose among several topics discussed by the author in his letters, and were also given some context on author&amp;rsquo;s life and literary production by Cristina Fenu, a digital humanist working at the &lt;a href=&#34;http://www.museosveviano.it/ar/progetto/archivio-digitale/&#34;&gt;Svevian Museum of Trieste&lt;/a&gt;. An open-source implementation will soon be available on Github.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks</title>
      <link>http://localhost:1313/talk/lth/</link>
      <pubDate>Thu, 18 Jul 2019 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/talk/lth/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Histopathologic Cancer Detection with Neural Networks</title>
      <link>http://localhost:1313/project/cancer-detection/</link>
      <pubDate>Fri, 28 Jun 2019 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/project/cancer-detection/</guid>
      <description>&lt;p&gt;As our final project for the course of Statistical Machine Learning held by Prof. Luca Bortolussi we explored different statistical and deep approaches to the problem of detecting tumors in histopathologic scans. We notably tried a random forest on distributional features extracted by pigment segmentation, a state-of-the-art DenseNet and a Capsule Network with Dynamic Routing.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>http://localhost:1313/msc-thesis/app-co2/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/msc-thesis/app-co2/</guid>
      <description>&lt;!DOCTYPE html&gt;
&lt;html lang=&#34;&#34; xml:lang=&#34;&#34;&gt;
&lt;head&gt;

  &lt;meta charset=&#34;utf-8&#34; /&gt;
  &lt;meta http-equiv=&#34;X-UA-Compatible&#34; content=&#34;IE=edge&#34; /&gt;
  &lt;title&gt;F CO2 Emissions Related to Experiments | Cognitively-informed Natural Language Processing Systems for Linguistic Complexity Assessment&lt;/title&gt;
  &lt;meta name=&#34;description&#34; content=&#34;MSc Thesis at the University of Trieste and SISSA under the supervision of Prof. Davide Crepaldi and Dott. Felice dell&#39;Orletta&#34; /&gt;
  &lt;meta name=&#34;generator&#34; content=&#34;bookdown 0.20.6 and GitBook 2.6.7&#34; /&gt;

  &lt;meta property=&#34;og:title&#34; content=&#34;F CO2 Emissions Related to Experiments | Cognitively-informed Natural Language Processing Systems for Linguistic Complexity Assessment&#34; /&gt;
  &lt;meta property=&#34;og:type&#34; content=&#34;book&#34; /&gt;
  &lt;meta property=&#34;og:url&#34; content=&#34;https://gsarti.com/master-thesis&#34; /&gt;
  &lt;meta property=&#34;og:image&#34; content=&#34;https://gsarti.com/master-thesisfigures/cover.png&#34; /&gt;
  &lt;meta property=&#34;og:description&#34; content=&#34;MSc Thesis at the University of Trieste and SISSA under the supervision of Prof. Davide Crepaldi and Dott. Felice dell&#39;Orletta&#34; /&gt;
  &lt;meta name=&#34;github-repo&#34; content=&#34;gsarti/linguistic-complexity&#34; /&gt;

  &lt;meta name=&#34;twitter:card&#34; content=&#34;summary&#34; /&gt;
  &lt;meta name=&#34;twitter:title&#34; content=&#34;F CO2 Emissions Related to Experiments | Cognitively-informed Natural Language Processing Systems for Linguistic Complexity Assessment&#34; /&gt;
  
  &lt;meta name=&#34;twitter:description&#34; content=&#34;MSc Thesis at the University of Trieste and SISSA under the supervision of Prof. Davide Crepaldi and Dott. Felice dell&#39;Orletta&#34; /&gt;
  &lt;meta name=&#34;twitter:image&#34; content=&#34;https://gsarti.com/master-thesisfigures/cover.png&#34; /&gt;

&lt;meta name=&#34;author&#34; content=&#34;Gabriele Sarti&#34; /&gt;



  &lt;meta name=&#34;viewport&#34; content=&#34;width=device-width, initial-scale=1&#34; /&gt;
  &lt;meta name=&#34;apple-mobile-web-app-capable&#34; content=&#34;yes&#34; /&gt;
  &lt;meta name=&#34;apple-mobile-web-app-status-bar-style&#34; content=&#34;black&#34; /&gt;
  &lt;link rel=&#34;apple-touch-icon-precomposed&#34; sizes=&#34;152x152&#34; href=&#34;figures/icons/apple-icon.png&#34; /&gt;
  &lt;link rel=&#34;shortcut icon&#34; href=&#34;figures/icons/favicon.ico&#34; type=&#34;image/x-icon&#34; /&gt;
&lt;link rel=&#34;prev&#34; href=&#34;app-params.html&#34;/&gt;
&lt;link rel=&#34;next&#34; href=&#34;references.html&#34;/&gt;
&lt;style type=&#34;text/css&#34;&gt;
p.abstract{
  text-align: center;
  font-weight: bold;
}
div.abstract{
  margin: auto;
  width: 90%;
}
&lt;/style&gt;
&lt;script src=&#34;libs/jquery-2.2.3/jquery.min.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;libs/gitbook-2.6.7/css/style.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;link href=&#34;libs/gitbook-2.6.7/css/plugin-table.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;link href=&#34;libs/gitbook-2.6.7/css/plugin-bookdown.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;link href=&#34;libs/gitbook-2.6.7/css/plugin-highlight.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;link href=&#34;libs/gitbook-2.6.7/css/plugin-search.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;link href=&#34;libs/gitbook-2.6.7/css/plugin-fontsettings.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;link href=&#34;libs/gitbook-2.6.7/css/plugin-clipboard.css&#34; rel=&#34;stylesheet&#34; /&gt;









&lt;script src=&#34;libs/kePrint-0.0.1/kePrint.js&#34;&gt;&lt;/script&gt;



&lt;link rel=&#34;stylesheet&#34; href=&#34;templates/style.css&#34; type=&#34;text/css&#34; /&gt;
&lt;/head&gt;

&lt;body&gt;



  &lt;div class=&#34;book without-animation with-summary font-size-2 font-family-1&#34; data-basepath=&#34;.&#34;&gt;

    &lt;div class=&#34;book-summary&#34;&gt;
      &lt;nav role=&#34;navigation&#34;&gt;

&lt;ul class=&#34;summary&#34;&gt;
&lt;li&gt;&lt;a href=&#34;introduction.html#introduction&#34;&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1&lt;/b&gt; &lt;strong&gt;Linguistic Complexity&lt;/strong&gt;&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.1&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subchap:categorizing&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.1&lt;/b&gt; Categorizing Linguistic Complexity Measures&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.2&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subchap:intrinsic&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.2&lt;/b&gt; Intrinsic Perspective&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.2.1&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subsubchap:structural&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.2.1&lt;/b&gt; Structural Linguistic Complexity&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.2.2&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subsubchap:lm-surprisal&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.2.2&lt;/b&gt; Language Modeling Surprisal&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.3&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subchap:extrinsic&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.3&lt;/b&gt; Extrinsic Perspective&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.3.1&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subsubchap:readability&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.3.1&lt;/b&gt; Automatic Readability Assessment&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.3.2&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subsubchap:pc&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.3.2&lt;/b&gt; Perceived Complexity Prediction&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.3.3&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subsubchap:eye-tracking&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.3.3&lt;/b&gt; Gaze Metrics Prediction&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.4&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subchap:garden-path&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.4&lt;/b&gt; Garden-path Sentences&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2&lt;/b&gt; &lt;strong&gt;Models of Linguistic Complexity&lt;/strong&gt;&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2.1&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html#subchap:desiderata&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2.1&lt;/b&gt; Desiderata for Models of Linguistic Complexity&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2.2&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html#subchap:nlm&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2.2&lt;/b&gt; Neural Language Models: Unsupervised Multitask Learners&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2.2.1&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html#subsubchap:syntax-nlm&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2.2.1&lt;/b&gt; Emergent Linguistic Structures in Neural Language Models&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2.3&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html#subchap:analyzing-nlm&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2.3&lt;/b&gt; Analyzing Neural Models of Complexity&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2.3.1&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html#subsubchap:probe&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2.3.1&lt;/b&gt; Probing classifiers&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2.3.2&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html#subsubchap:rsa&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2.3.2&lt;/b&gt; Representational Similarity Analysis&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2.3.3&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html#subsubchap:pwcca&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2.3.3&lt;/b&gt; Projection-Weighted Canonical Correlation Analysis&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3&lt;/b&gt; &lt;strong&gt;Complexity Phenomena in Linguistic Annotations and Language Models&lt;/strong&gt;&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3.1&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html#subchap:ex1-data&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3.1&lt;/b&gt; Data and Preprocessing&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3.2&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html#subchap:ex1-analysis&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3.2&lt;/b&gt; Analysis of Linguistic Phenomena&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3.2.1&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html#subsubchap:ex1-analysis-bins&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3.2.1&lt;/b&gt; Linguistic Phenomena in Length-controlled Bins&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3.3&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html#subchap:ex1-modeling&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3.3&lt;/b&gt; Modeling Online and Offline Linguistic Complexity&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3.3.1&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html#subsubchap:ex1-modeling-bins&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3.3.1&lt;/b&gt; Modeling Complexity in Length-controlled Bins&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3.4&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html#subchap:ex1-probing&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3.4&lt;/b&gt; Probing Linguistic Phenomena in ALBERT Representations&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3.5&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html#subchap:ex1-summary&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3.5&lt;/b&gt; Summary&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;4&#34; data-path=&#34;chap-ex2.html&#34;&gt;&lt;a href=&#34;chap-ex2.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;4&lt;/b&gt; &lt;strong&gt;Representational Similarity in Models of Complexity&lt;/strong&gt;&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;4.1&#34; data-path=&#34;chap-ex2.html&#34;&gt;&lt;a href=&#34;chap-ex2.html#knowledge-driven-requirements-for-learning-models&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;4.1&lt;/b&gt; Knowledge-driven Requirements for Learning Models&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;4.2&#34; data-path=&#34;chap-ex2.html&#34;&gt;&lt;a href=&#34;chap-ex2.html#subchap:ex2-experiments&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;4.2&lt;/b&gt; Experiments&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;4.2.1&#34; data-path=&#34;chap-ex2.html&#34;&gt;&lt;a href=&#34;chap-ex2.html#subsubchap:ex2-data&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;4.2.1&lt;/b&gt; Data&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;4.2.2&#34; data-path=&#34;chap-ex2.html&#34;&gt;&lt;a href=&#34;chap-ex2.html#subsubchap:ex2-inter&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;4.2.2&lt;/b&gt; Inter-model Representational Similarity&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;4.2.3&#34; data-path=&#34;chap-ex2.html&#34;&gt;&lt;a href=&#34;chap-ex2.html#subsubchap:ex2-intra&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;4.2.3&lt;/b&gt; Intra-model Representational Similarity&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;4.3&#34; data-path=&#34;chap-ex2.html&#34;&gt;&lt;a href=&#34;chap-ex2.html#subchap:ex2-summary&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;4.3&lt;/b&gt; Summary&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;5&#34; data-path=&#34;chap-ex3.html&#34;&gt;&lt;a href=&#34;chap-ex3.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;5&lt;/b&gt; &lt;strong&gt;Cognitive Phenomena in Cognitively Informed Language Models&lt;/strong&gt;&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;5.1&#34; data-path=&#34;chap-ex3.html&#34;&gt;&lt;a href=&#34;chap-ex3.html#subchap:ex3-data&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;5.1&lt;/b&gt; Data and Preprocessing&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;5.2&#34; data-path=&#34;chap-ex3.html&#34;&gt;&lt;a href=&#34;chap-ex3.html#subchap:ex3-summary&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;5.2&lt;/b&gt; Summary&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;conclusion.html#conclusion&#34;&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;&#34; data-path=&#34;conclusion.html&#34;&gt;&lt;a href=&#34;conclusion.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;Future Directions&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;appendix&#34;&gt;&lt;span&gt;&lt;b&gt;Appendix&lt;/b&gt;&lt;/span&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;A&#34; data-path=&#34;app-ling-feats.html&#34;&gt;&lt;a href=&#34;app-ling-feats.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;A&lt;/b&gt; Linguistic Features&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;A.1&#34; data-path=&#34;app-ling-feats.html&#34;&gt;&lt;a href=&#34;app-ling-feats.html#raw-text-properties-and-lexical-variety&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;A.1&lt;/b&gt; Raw Text Properties and Lexical Variety&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;A.2&#34; data-path=&#34;app-ling-feats.html&#34;&gt;&lt;a href=&#34;app-ling-feats.html#morpho-syntacting-information&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;A.2&lt;/b&gt; Morpho-syntacting Information&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;A.3&#34; data-path=&#34;app-ling-feats.html&#34;&gt;&lt;a href=&#34;app-ling-feats.html#verbal-predicate-structure&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;A.3&lt;/b&gt; Verbal Predicate Structure&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;A.4&#34; data-path=&#34;app-ling-feats.html&#34;&gt;&lt;a href=&#34;app-ling-feats.html#global-and-local-parsed-tree-structures&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;A.4&lt;/b&gt; Global and Local Parsed Tree Structures&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;A.5&#34; data-path=&#34;app-ling-feats.html&#34;&gt;&lt;a href=&#34;app-ling-feats.html#syntactic-relations&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;A.5&lt;/b&gt; Syntactic Relations&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;A.6&#34; data-path=&#34;app-ling-feats.html&#34;&gt;&lt;a href=&#34;app-ling-feats.html#subordination-phenomena&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;A.6&lt;/b&gt; Subordination Phenomena&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;B&#34; data-path=&#34;app-et-metrics.html&#34;&gt;&lt;a href=&#34;app-et-metrics.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;B&lt;/b&gt; Additional Precisions on Eye-tracking Metrics and Preprocessing&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;C&#34; data-path=&#34;app-et-modeling.html&#34;&gt;&lt;a href=&#34;app-et-modeling.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;C&lt;/b&gt; Multi-task Token-level Modeling for Gaze Metrics Prediction&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;D&#34; data-path=&#34;app-intra-sim.html&#34;&gt;&lt;a href=&#34;app-intra-sim.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;D&lt;/b&gt; Intra-model Similarity for All Models&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;E&#34; data-path=&#34;app-params.html&#34;&gt;&lt;a href=&#34;app-params.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;E&lt;/b&gt; Model parametrization&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;F&#34; data-path=&#34;app-co2.html&#34;&gt;&lt;a href=&#34;app-co2.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;F&lt;/b&gt; CO2 Emissions Related to Experiments&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;&#34; data-path=&#34;references.html&#34;&gt;&lt;a href=&#34;references.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;References&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;divider&#34;&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://gsarti.com&#34;&gt;Back to my website&lt;/a&gt;&lt;/li&gt;

&lt;/ul&gt;

      &lt;/nav&gt;
    &lt;/div&gt;

    &lt;div class=&#34;book-body&#34;&gt;
      &lt;div class=&#34;body-inner&#34;&gt;
        &lt;div class=&#34;book-header&#34; role=&#34;navigation&#34;&gt;
          &lt;h1&gt;
            &lt;i class=&#34;fa fa-circle-o-notch fa-spin&#34;&gt;&lt;/i&gt;&lt;a href=&#34;./&#34;&gt;Cognitively-informed&lt;br /&gt;
Natural Language Processing Systems&lt;br /&gt;
for Linguistic Complexity Assessment&lt;/a&gt;
          &lt;/h1&gt;
        &lt;/div&gt;

        &lt;div class=&#34;page-wrapper&#34; tabindex=&#34;-1&#34; role=&#34;main&#34;&gt;
          &lt;div class=&#34;page-inner&#34;&gt;

            &lt;section class=&#34;normal&#34; id=&#34;section-&#34;&gt;
&lt;div id=&#34;app:co2&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;F&lt;/span&gt; CO2 Emissions Related to Experiments&lt;/h1&gt;
&lt;p&gt;Experiments were conducted using the private infrastructure of the ItaliaNLP Lab&lt;a href=&#34;#fn22&#34; class=&#34;footnote-ref&#34; id=&#34;fnref22&#34;&gt;&lt;sup&gt;22&lt;/sup&gt;&lt;/a&gt; at the Institute for Computational Linguistics “A. Zampolli” (ILC-CNR) in Pisa, which has an estimated carbon efficiency of 0.358 kgCO&lt;span class=&#34;math inline&#34;&gt;\(_2\)&lt;/span&gt;eq/kWh &lt;span class=&#34;citation&#34;&gt;(Moro and Lonza &lt;a href=&#34;#ref-moro2018electricity&#34;&gt;2018&lt;/a&gt;)&lt;/span&gt;. A cumulative of roughly 500 hours of computation was performed on hardware of type Tesla K40 (TDP of 245W). Total emissions are estimated to be 52.63 kgCO&lt;span class=&#34;math inline&#34;&gt;\(_2\)&lt;/span&gt;eq&lt;/p&gt;
&lt;p&gt;Estimations were conducted using the Machine Learning Impact Calculator&lt;a href=&#34;#fn23&#34; class=&#34;footnote-ref&#34; id=&#34;fnref23&#34;&gt;&lt;sup&gt;23&lt;/sup&gt;&lt;/a&gt; presented in &lt;span class=&#34;citation&#34;&gt;Lacoste et al. (&lt;a href=&#34;#ref-lacoste2019quantifying&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;

&lt;/div&gt;
&lt;h3&gt;References&lt;/h3&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-lacoste2019quantifying&#34;&gt;
&lt;p&gt;Lacoste, Alexandre, Alexandra Luccioni, Victor Schmidt, and Thomas Dandres. 2019. “Quantifying the Carbon Emissions of Machine Learning.” &lt;em&gt;arXiv Preprint arXiv:1910.09700&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-moro2018electricity&#34;&gt;
&lt;p&gt;Moro, Alberto, and Laura Lonza. 2018. “Electricity Carbon Intensity in European Member States: Impacts on Ghg Emissions of Electric Vehicles.” &lt;em&gt;Transportation Research Part D: Transport and Environment&lt;/em&gt; 64. Elsevier: 5–14.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol start=&#34;22&#34;&gt;
&lt;li id=&#34;fn22&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://www.italianlp.it&#34;&gt;https://www.italianlp.it&lt;/a&gt;&lt;a href=&#34;app-co2.html#fnref22&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn23&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://mlco2.github.io/impact#compute&#34;&gt;https://mlco2.github.io/impact#compute&lt;/a&gt;&lt;a href=&#34;app-co2.html#fnref23&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
            &lt;/section&gt;

          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
&lt;a href=&#34;app-params.html&#34; class=&#34;navigation navigation-prev &#34; aria-label=&#34;Previous page&#34;&gt;&lt;i class=&#34;fa fa-angle-left&#34;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&#34;references.html&#34; class=&#34;navigation navigation-next &#34; aria-label=&#34;Next page&#34;&gt;&lt;i class=&#34;fa fa-angle-right&#34;&gt;&lt;/i&gt;&lt;/a&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/app.min.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/lunr.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/clipboard.min.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/plugin-search.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/plugin-sharing.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/plugin-fontsettings.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/plugin-bookdown.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/jquery.highlight.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/plugin-clipboard.js&#34;&gt;&lt;/script&gt;
&lt;script&gt;
gitbook.require([&#34;gitbook&#34;], function(gitbook) {
gitbook.start({
&#34;sharing&#34;: {
&#34;github&#34;: true,
&#34;facebook&#34;: true,
&#34;twitter&#34;: true,
&#34;linkedin&#34;: true,
&#34;weibo&#34;: false,
&#34;instapaper&#34;: false,
&#34;vk&#34;: false,
&#34;all&#34;: false
},
&#34;fontsettings&#34;: {
&#34;theme&#34;: &#34;white&#34;,
&#34;family&#34;: &#34;sans&#34;,
&#34;size&#34;: 2
},
&#34;edit&#34;: {
&#34;link&#34;: &#34;https://github.com/gsarti/master-thesis/tree/master/extra/Appendix.Rmd&#34;,
&#34;text&#34;: &#34;Edit&#34;
},
&#34;history&#34;: {
&#34;link&#34;: null,
&#34;text&#34;: null
},
&#34;view&#34;: {
&#34;link&#34;: null,
&#34;text&#34;: null
},
&#34;download&#34;: [&#34;_main.pdf&#34;],
&#34;toc&#34;: {
&#34;collapse&#34;: &#34;subsection&#34;,
&#34;scroll_highlight&#34;: true
},
&#34;info&#34;: false
});
});
&lt;/script&gt;

&lt;!-- dynamically load mathjax for compatibility with self-contained --&gt;
&lt;script&gt;
  (function () {
    var script = document.createElement(&#34;script&#34;);
    script.type = &#34;text/javascript&#34;;
    var src = &#34;true&#34;;
    if (src === &#34;&#34; || src === &#34;true&#34;) src = &#34;https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML&#34;;
    if (location.protocol !== &#34;file:&#34;)
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, &#39;&#39;);
    script.src = src;
    document.getElementsByTagName(&#34;head&#34;)[0].appendChild(script);
  })();
&lt;/script&gt;
&lt;/body&gt;

&lt;/html&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>http://localhost:1313/msc-thesis/app-et-metrics/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/msc-thesis/app-et-metrics/</guid>
      <description>&lt;!DOCTYPE html&gt;
&lt;html lang=&#34;&#34; xml:lang=&#34;&#34;&gt;
&lt;head&gt;

  &lt;meta charset=&#34;utf-8&#34; /&gt;
  &lt;meta http-equiv=&#34;X-UA-Compatible&#34; content=&#34;IE=edge&#34; /&gt;
  &lt;title&gt;B Precisions on Eye-tracking Metrics and Preprocessing | Interpreting Neural Language Models for Linguistic Complexity Assessment&lt;/title&gt;
  &lt;meta name=&#34;description&#34; content=&#34;MSc Thesis at the University of Trieste and SISSA under the supervision of Prof. Davide Crepaldi and Dott. Felice dell&#39;Orletta&#34; /&gt;
  &lt;meta name=&#34;generator&#34; content=&#34;bookdown 0.20.6 and GitBook 2.6.7&#34; /&gt;

  &lt;meta property=&#34;og:title&#34; content=&#34;B Precisions on Eye-tracking Metrics and Preprocessing | Interpreting Neural Language Models for Linguistic Complexity Assessment&#34; /&gt;
  &lt;meta property=&#34;og:type&#34; content=&#34;book&#34; /&gt;
  &lt;meta property=&#34;og:url&#34; content=&#34;https://gsarti.com/master-thesis&#34; /&gt;
  &lt;meta property=&#34;og:image&#34; content=&#34;https://gsarti.com/master-thesisfigures/cover.png&#34; /&gt;
  &lt;meta property=&#34;og:description&#34; content=&#34;MSc Thesis at the University of Trieste and SISSA under the supervision of Prof. Davide Crepaldi and Dott. Felice dell&#39;Orletta&#34; /&gt;
  &lt;meta name=&#34;github-repo&#34; content=&#34;gsarti/interpreting-complexity&#34; /&gt;

  &lt;meta name=&#34;twitter:card&#34; content=&#34;summary&#34; /&gt;
  &lt;meta name=&#34;twitter:title&#34; content=&#34;B Precisions on Eye-tracking Metrics and Preprocessing | Interpreting Neural Language Models for Linguistic Complexity Assessment&#34; /&gt;
  
  &lt;meta name=&#34;twitter:description&#34; content=&#34;MSc Thesis at the University of Trieste and SISSA under the supervision of Prof. Davide Crepaldi and Dott. Felice dell&#39;Orletta&#34; /&gt;
  &lt;meta name=&#34;twitter:image&#34; content=&#34;https://gsarti.com/master-thesisfigures/cover.png&#34; /&gt;

&lt;meta name=&#34;author&#34; content=&#34;Gabriele Sarti&#34; /&gt;



  &lt;meta name=&#34;viewport&#34; content=&#34;width=device-width, initial-scale=1&#34; /&gt;
  &lt;meta name=&#34;apple-mobile-web-app-capable&#34; content=&#34;yes&#34; /&gt;
  &lt;meta name=&#34;apple-mobile-web-app-status-bar-style&#34; content=&#34;black&#34; /&gt;
  &lt;link rel=&#34;apple-touch-icon-precomposed&#34; sizes=&#34;152x152&#34; href=&#34;figures/icons/apple-icon.png&#34; /&gt;
  &lt;link rel=&#34;shortcut icon&#34; href=&#34;figures/icons/favicon.ico&#34; type=&#34;image/x-icon&#34; /&gt;
&lt;link rel=&#34;prev&#34; href=&#34;app-ling-feats.html&#34;/&gt;
&lt;link rel=&#34;next&#34; href=&#34;app-et-modeling.html&#34;/&gt;
&lt;style type=&#34;text/css&#34;&gt;
p.abstract{
  text-align: center;
  font-weight: bold;
}
div.abstract{
  margin: auto;
  width: 90%;
}
&lt;/style&gt;
&lt;script src=&#34;libs/jquery-2.2.3/jquery.min.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;libs/gitbook-2.6.7/css/style.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;link href=&#34;libs/gitbook-2.6.7/css/plugin-table.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;link href=&#34;libs/gitbook-2.6.7/css/plugin-bookdown.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;link href=&#34;libs/gitbook-2.6.7/css/plugin-highlight.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;link href=&#34;libs/gitbook-2.6.7/css/plugin-search.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;link href=&#34;libs/gitbook-2.6.7/css/plugin-fontsettings.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;link href=&#34;libs/gitbook-2.6.7/css/plugin-clipboard.css&#34; rel=&#34;stylesheet&#34; /&gt;









&lt;script src=&#34;libs/kePrint-0.0.1/kePrint.js&#34;&gt;&lt;/script&gt;



&lt;link rel=&#34;stylesheet&#34; href=&#34;templates/style.css&#34; type=&#34;text/css&#34; /&gt;
&lt;/head&gt;

&lt;body&gt;



  &lt;div class=&#34;book without-animation with-summary font-size-2 font-family-1&#34; data-basepath=&#34;.&#34;&gt;

    &lt;div class=&#34;book-summary&#34;&gt;
      &lt;nav role=&#34;navigation&#34;&gt;

&lt;ul class=&#34;summary&#34;&gt;
&lt;li&gt;&lt;a href=&#34;introduction.html#introduction&#34;&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1&lt;/b&gt; &lt;strong&gt;Linguistic Complexity&lt;/strong&gt;&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.1&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subchap:categorizing&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.1&lt;/b&gt; Categorizing Linguistic Complexity Measures&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.2&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subchap:intrinsic&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.2&lt;/b&gt; Intrinsic Perspective&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.2.1&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subsubchap:structural&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.2.1&lt;/b&gt; Structural Linguistic Complexity&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.2.2&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subsubchap:lm-surprisal&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.2.2&lt;/b&gt; Language Modeling Surprisal&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.3&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subchap:extrinsic&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.3&lt;/b&gt; Extrinsic Perspective&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.3.1&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subsubchap:readability&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.3.1&lt;/b&gt; Automatic Readability Assessment&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.3.2&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subsubchap:pc&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.3.2&lt;/b&gt; Perceived Complexity Prediction&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.3.3&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subsubchap:eye-tracking&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.3.3&lt;/b&gt; Gaze Metrics Prediction&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.4&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subchap:garden-path&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.4&lt;/b&gt; Garden-path Sentences&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2&lt;/b&gt; &lt;strong&gt;Models of Linguistic Complexity&lt;/strong&gt;&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2.1&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html#subchap:desiderata&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2.1&lt;/b&gt; Desiderata for Models of Linguistic Complexity&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2.2&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html#subchap:nlm&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2.2&lt;/b&gt; Neural Language Models: Unsupervised Multitask Learners&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2.2.1&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html#subsubchap:syntax-nlm&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2.2.1&lt;/b&gt; Emergent Linguistic Structures in Neural Language Models&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2.3&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html#subchap:analyzing-nlm&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2.3&lt;/b&gt; Analyzing Neural Models of Complexity&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2.3.1&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html#subsubchap:probe&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2.3.1&lt;/b&gt; Probing classifiers&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2.3.2&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html#subsubchap:rsa&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2.3.2&lt;/b&gt; Representational Similarity Analysis&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2.3.3&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html#subsubchap:pwcca&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2.3.3&lt;/b&gt; Projection-Weighted Canonical Correlation Analysis&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3&lt;/b&gt; &lt;strong&gt;Complexity Phenomena in Linguistic Annotations and Language Models&lt;/strong&gt;&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3.1&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html#subchap:ex1-data&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3.1&lt;/b&gt; Data and Preprocessing&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3.2&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html#subchap:ex1-analysis&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3.2&lt;/b&gt; Analysis of Linguistic Phenomena&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3.2.1&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html#subsubchap:ex1-analysis-bins&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3.2.1&lt;/b&gt; Linguistic Phenomena in Length-controlled Bins&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3.3&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html#subchap:ex1-modeling&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3.3&lt;/b&gt; Modeling Online and Offline Linguistic Complexity&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3.3.1&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html#subsubchap:ex1-modeling-bins&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3.3.1&lt;/b&gt; Modeling Complexity in Length-controlled Bins&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3.4&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html#subchap:ex1-probing&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3.4&lt;/b&gt; Probing Linguistic Phenomena in ALBERT Representations&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3.5&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html#subchap:ex1-summary&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3.5&lt;/b&gt; Summary&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;4&#34; data-path=&#34;chap-ex2.html&#34;&gt;&lt;a href=&#34;chap-ex2.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;4&lt;/b&gt; &lt;strong&gt;Representational Similarity in Models of Complexity&lt;/strong&gt;&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;4.1&#34; data-path=&#34;chap-ex2.html&#34;&gt;&lt;a href=&#34;chap-ex2.html#knowledge-driven-requirements-for-learning-models&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;4.1&lt;/b&gt; Knowledge-driven Requirements for Learning Models&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;4.2&#34; data-path=&#34;chap-ex2.html&#34;&gt;&lt;a href=&#34;chap-ex2.html#subchap:ex2-experiments&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;4.2&lt;/b&gt; Experimentsl Evaluation&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;4.2.1&#34; data-path=&#34;chap-ex2.html&#34;&gt;&lt;a href=&#34;chap-ex2.html#subsubchap:ex2-data&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;4.2.1&lt;/b&gt; Data&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;4.2.2&#34; data-path=&#34;chap-ex2.html&#34;&gt;&lt;a href=&#34;chap-ex2.html#subsubchap:ex2-inter&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;4.2.2&lt;/b&gt; Inter-model Representational Similarity&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;4.2.3&#34; data-path=&#34;chap-ex2.html&#34;&gt;&lt;a href=&#34;chap-ex2.html#subsubchap:ex2-intra&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;4.2.3&lt;/b&gt; Intra-model Representational Similarity&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;4.3&#34; data-path=&#34;chap-ex2.html&#34;&gt;&lt;a href=&#34;chap-ex2.html#subchap:ex2-summary&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;4.3&lt;/b&gt; Summary&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;5&#34; data-path=&#34;chap-ex3.html&#34;&gt;&lt;a href=&#34;chap-ex3.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;5&lt;/b&gt; &lt;strong&gt;Gaze-informed Models for Cognitive Processing Prediction&lt;/strong&gt;&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;5.1&#34; data-path=&#34;chap-ex3.html&#34;&gt;&lt;a href=&#34;chap-ex3.html#subchap:ex3-setup&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;5.1&lt;/b&gt; Experimental Setup&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;5.2&#34; data-path=&#34;chap-ex3.html&#34;&gt;&lt;a href=&#34;chap-ex3.html#subchap:ex3-experiments&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;5.2&lt;/b&gt; Experimental Evaluation&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;5.2.1&#34; data-path=&#34;chap-ex3.html&#34;&gt;&lt;a href=&#34;chap-ex3.html#subsubchap:ex3-magnitudes&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;5.2.1&lt;/b&gt; Estimating Magnitudes of Garden-path Delays&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;5.2.2&#34; data-path=&#34;chap-ex3.html&#34;&gt;&lt;a href=&#34;chap-ex3.html#subsubchap:ex3-predicting&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;5.2.2&lt;/b&gt; Predicting Delays with Surprisal and Gaze Metrics&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;5.3&#34; data-path=&#34;chap-ex3.html&#34;&gt;&lt;a href=&#34;chap-ex3.html#subchap:ex3-summary&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;5.3&lt;/b&gt; Summary&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;conclusion.html#conclusion&#34;&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;&#34; data-path=&#34;conclusion.html&#34;&gt;&lt;a href=&#34;conclusion.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;Broader Impact and Ethical Perspectives&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;&#34; data-path=&#34;conclusion.html&#34;&gt;&lt;a href=&#34;conclusion.html#future-directions&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;Future Directions&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;appendix&#34;&gt;&lt;span&gt;&lt;b&gt;Appendix&lt;/b&gt;&lt;/span&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;A&#34; data-path=&#34;app-ling-feats.html&#34;&gt;&lt;a href=&#34;app-ling-feats.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;A&lt;/b&gt; Linguistic Features&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;A.1&#34; data-path=&#34;app-ling-feats.html&#34;&gt;&lt;a href=&#34;app-ling-feats.html#raw-text-properties-and-lexical-variety&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;A.1&lt;/b&gt; Raw Text Properties and Lexical Variety&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;A.2&#34; data-path=&#34;app-ling-feats.html&#34;&gt;&lt;a href=&#34;app-ling-feats.html#morpho-syntacting-information&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;A.2&lt;/b&gt; Morpho-syntacting Information&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;A.3&#34; data-path=&#34;app-ling-feats.html&#34;&gt;&lt;a href=&#34;app-ling-feats.html#verbal-predicate-structure&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;A.3&lt;/b&gt; Verbal Predicate Structure&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;A.4&#34; data-path=&#34;app-ling-feats.html&#34;&gt;&lt;a href=&#34;app-ling-feats.html#global-and-local-parsed-tree-structures&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;A.4&lt;/b&gt; Global and Local Parsed Tree Structures&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;A.5&#34; data-path=&#34;app-ling-feats.html&#34;&gt;&lt;a href=&#34;app-ling-feats.html#syntactic-relations&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;A.5&lt;/b&gt; Syntactic Relations&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;A.6&#34; data-path=&#34;app-ling-feats.html&#34;&gt;&lt;a href=&#34;app-ling-feats.html#subordination-phenomena&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;A.6&lt;/b&gt; Subordination Phenomena&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;B&#34; data-path=&#34;app-et-metrics.html&#34;&gt;&lt;a href=&#34;app-et-metrics.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;B&lt;/b&gt; Precisions on Eye-tracking Metrics and Preprocessing&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;C&#34; data-path=&#34;app-et-modeling.html&#34;&gt;&lt;a href=&#34;app-et-modeling.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;C&lt;/b&gt; Multi-task Token-level Regression for Gaze Metrics Prediction&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;D&#34; data-path=&#34;app-intra-sim.html&#34;&gt;&lt;a href=&#34;app-intra-sim.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;D&lt;/b&gt; Intra-model Similarity for All Models&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;E&#34; data-path=&#34;app-garden-paths-et.html&#34;&gt;&lt;a href=&#34;app-garden-paths-et.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;E&lt;/b&gt; Gaze Metrics Predictions for Garden Path Sentences&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;F&#34; data-path=&#34;app-params.html&#34;&gt;&lt;a href=&#34;app-params.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;F&lt;/b&gt; Reproducibility and Environmental Impact&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;&#34; data-path=&#34;references.html&#34;&gt;&lt;a href=&#34;references.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;References&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;divider&#34;&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://gsarti.com&#34;&gt;Back to my website&lt;/a&gt;&lt;/li&gt;

&lt;/ul&gt;

      &lt;/nav&gt;
    &lt;/div&gt;

    &lt;div class=&#34;book-body&#34;&gt;
      &lt;div class=&#34;body-inner&#34;&gt;
        &lt;div class=&#34;book-header&#34; role=&#34;navigation&#34;&gt;
          &lt;h1&gt;
            &lt;i class=&#34;fa fa-circle-o-notch fa-spin&#34;&gt;&lt;/i&gt;&lt;a href=&#34;./&#34;&gt;Interpreting Neural Language Models&lt;br /&gt;
for Linguistic Complexity Assessment&lt;/a&gt;
          &lt;/h1&gt;
        &lt;/div&gt;

        &lt;div class=&#34;page-wrapper&#34; tabindex=&#34;-1&#34; role=&#34;main&#34;&gt;
          &lt;div class=&#34;page-inner&#34;&gt;

            &lt;section class=&#34;normal&#34; id=&#34;section-&#34;&gt;
&lt;div id=&#34;app:et-metrics&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;B&lt;/span&gt; Precisions on Eye-tracking Metrics and Preprocessing&lt;/h1&gt;
&lt;table class=&#34;table&#34; style=&#34;font-size: 11px; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;caption style=&#34;font-size: initial !important;&#34;&gt;
&lt;span id=&#34;tab:et-mappings&#34;&gt;Table B.1: &lt;/span&gt;Eye-tracking mappings from dataset-specific fields to the shared set of metrics.
&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;font-weight: bold;&#34;&gt;
Metrics
&lt;/th&gt;
&lt;th style=&#34;text-align:center;font-weight: bold;&#34;&gt;
Dundee
&lt;/th&gt;
&lt;th style=&#34;text-align:center;font-weight: bold;&#34;&gt;
GECO
&lt;/th&gt;
&lt;th style=&#34;text-align:center;font-weight: bold;&#34;&gt;
ZuCo 1 &amp;amp; 2
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
First fix. dur. (FFD)
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
First_fix_dur
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
FIRST_FIXATION_DURATION
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
FFD
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
First pass dur. (FPD)
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
First_pass_dur
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
GAZE_DURATION
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
GD
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Fix. prob. (FXP)
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
Fix_prob
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
¬ WORD_SKIP
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
FXC &amp;gt; 0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Fix. count (FXC)
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
nFix
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
FIXATION_COUNT
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
FXC
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Tot. fix. Dur. (TFD)
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
Tot_fix_dur
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
TOT_READ_TIME
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
TRT
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Tot. Regres. Dur. (TRD)
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
Tot_regres_from_dur
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
GO_PAST - SEL._GO_PAST
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
GPT - GD
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;span class=&#34;custompar&#34;&gt;Univocal gaze metrics conversion&lt;/span&gt; Table &lt;a href=&#34;app-et-metrics.html#tab:et-mappings&#34;&gt;B.1&lt;/a&gt; present the conversion scheme used to obtain a unified set of eye-tracking metrics from different corpora annotations. This method follows closely the approach adopted by &lt;span class=&#34;citation&#34;&gt;Hollenstein and Zhang (&lt;a href=&#34;#ref-hollenstein-zhang-2019-entity&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt;. While the mapping is straightforward for shared metrics, the TRD metric needs to be computed for GECO and ZuCo. For GECO, the difference between go-past time (i.e. total time elapsed between the first access of a word boundary and the first access of subsequent words, including regressions) and its selective variant (i.e. go-past time only relative to the specific word, without accounting for regressions) gives an exact conversion to regression duration. Instead, in the ZuCo case, an approximate conversion using gaze duration (i.e. first pass duration) instead of selective go-past time is used since selective go-past time is not provided. ZuCo’s TRD estimate should be deemed an upper bound for regressions’ duration since gaze duration is always smaller than the selective go-past time when regressions are present and is precisely equal to it in the complete absence of regressions.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;custompar&#34;&gt;Averaging across participants&lt;/span&gt; Gaze metrics are averaged across participants for all experiments of this thesis work. Metrics missing for some participants due to skipping are replaced with the lowest recorded value across participants for that word before averaging. This procedure is preferred to zero-filling missing values since the latter produces significant drops in metrics associated with tokens skipped by multiple participants, making averaged values inconsistent with empirical observations.&lt;/p&gt;
&lt;/div&gt;
&lt;h3&gt;References&lt;/h3&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-hollenstein-zhang-2019-entity&#34;&gt;
&lt;p&gt;Hollenstein, Nora, and Ce Zhang. 2019. “Entity Recognition at First Sight: Improving NER with Eye Movement Information.” In &lt;em&gt;Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)&lt;/em&gt;, 1–10. Minneapolis, Minnesota: Association for Computational Linguistics. &lt;a href=&#34;https://doi.org/10.18653/v1/N19-1001&#34;&gt;https://doi.org/10.18653/v1/N19-1001&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
            &lt;/section&gt;

          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
&lt;a href=&#34;app-ling-feats.html&#34; class=&#34;navigation navigation-prev &#34; aria-label=&#34;Previous page&#34;&gt;&lt;i class=&#34;fa fa-angle-left&#34;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&#34;app-et-modeling.html&#34; class=&#34;navigation navigation-next &#34; aria-label=&#34;Next page&#34;&gt;&lt;i class=&#34;fa fa-angle-right&#34;&gt;&lt;/i&gt;&lt;/a&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/app.min.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/lunr.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/clipboard.min.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/plugin-search.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/plugin-sharing.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/plugin-fontsettings.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/plugin-bookdown.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/jquery.highlight.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/plugin-clipboard.js&#34;&gt;&lt;/script&gt;
&lt;script&gt;
gitbook.require([&#34;gitbook&#34;], function(gitbook) {
gitbook.start({
&#34;sharing&#34;: {
&#34;github&#34;: true,
&#34;facebook&#34;: true,
&#34;twitter&#34;: true,
&#34;linkedin&#34;: true,
&#34;weibo&#34;: false,
&#34;instapaper&#34;: false,
&#34;vk&#34;: false,
&#34;all&#34;: false
},
&#34;fontsettings&#34;: {
&#34;theme&#34;: &#34;white&#34;,
&#34;family&#34;: &#34;sans&#34;,
&#34;size&#34;: 2
},
&#34;edit&#34;: {
&#34;link&#34;: &#34;https://github.com/gsarti/master-thesis/tree/master/extra/Appendix.Rmd&#34;,
&#34;text&#34;: &#34;Edit&#34;
},
&#34;history&#34;: {
&#34;link&#34;: null,
&#34;text&#34;: null
},
&#34;view&#34;: {
&#34;link&#34;: null,
&#34;text&#34;: null
},
&#34;download&#34;: [[&#34;Sarti_2020_Interpreting_NLMs_for_LCA.pdf&#34;, &#34;PDF&#34;]],
&#34;toc&#34;: {
&#34;collapse&#34;: &#34;subsection&#34;,
&#34;scroll_highlight&#34;: true
},
&#34;info&#34;: false
});
});
&lt;/script&gt;

&lt;!-- dynamically load mathjax for compatibility with self-contained --&gt;
&lt;script&gt;
  (function () {
    var script = document.createElement(&#34;script&#34;);
    script.type = &#34;text/javascript&#34;;
    var src = &#34;true&#34;;
    if (src === &#34;&#34; || src === &#34;true&#34;) src = &#34;https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML&#34;;
    if (location.protocol !== &#34;file:&#34;)
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, &#39;&#39;);
    script.src = src;
    document.getElementsByTagName(&#34;head&#34;)[0].appendChild(script);
  })();
&lt;/script&gt;
&lt;/body&gt;

&lt;/html&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>http://localhost:1313/msc-thesis/app-et-modeling/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/msc-thesis/app-et-modeling/</guid>
      <description>&lt;!DOCTYPE html&gt;
&lt;html lang=&#34;&#34; xml:lang=&#34;&#34;&gt;
&lt;head&gt;

  &lt;meta charset=&#34;utf-8&#34; /&gt;
  &lt;meta http-equiv=&#34;X-UA-Compatible&#34; content=&#34;IE=edge&#34; /&gt;
  &lt;title&gt;C Multi-task Token-level Regression for Gaze Metrics Prediction | Interpreting Neural Language Models for Linguistic Complexity Assessment&lt;/title&gt;
  &lt;meta name=&#34;description&#34; content=&#34;MSc Thesis at the University of Trieste and SISSA under the supervision of Prof. Davide Crepaldi and Dott. Felice dell&#39;Orletta&#34; /&gt;
  &lt;meta name=&#34;generator&#34; content=&#34;bookdown 0.20.6 and GitBook 2.6.7&#34; /&gt;

  &lt;meta property=&#34;og:title&#34; content=&#34;C Multi-task Token-level Regression for Gaze Metrics Prediction | Interpreting Neural Language Models for Linguistic Complexity Assessment&#34; /&gt;
  &lt;meta property=&#34;og:type&#34; content=&#34;book&#34; /&gt;
  &lt;meta property=&#34;og:url&#34; content=&#34;https://gsarti.com/master-thesis&#34; /&gt;
  &lt;meta property=&#34;og:image&#34; content=&#34;https://gsarti.com/master-thesisfigures/cover.png&#34; /&gt;
  &lt;meta property=&#34;og:description&#34; content=&#34;MSc Thesis at the University of Trieste and SISSA under the supervision of Prof. Davide Crepaldi and Dott. Felice dell&#39;Orletta&#34; /&gt;
  &lt;meta name=&#34;github-repo&#34; content=&#34;gsarti/interpreting-complexity&#34; /&gt;

  &lt;meta name=&#34;twitter:card&#34; content=&#34;summary&#34; /&gt;
  &lt;meta name=&#34;twitter:title&#34; content=&#34;C Multi-task Token-level Regression for Gaze Metrics Prediction | Interpreting Neural Language Models for Linguistic Complexity Assessment&#34; /&gt;
  
  &lt;meta name=&#34;twitter:description&#34; content=&#34;MSc Thesis at the University of Trieste and SISSA under the supervision of Prof. Davide Crepaldi and Dott. Felice dell&#39;Orletta&#34; /&gt;
  &lt;meta name=&#34;twitter:image&#34; content=&#34;https://gsarti.com/master-thesisfigures/cover.png&#34; /&gt;

&lt;meta name=&#34;author&#34; content=&#34;Gabriele Sarti&#34; /&gt;



  &lt;meta name=&#34;viewport&#34; content=&#34;width=device-width, initial-scale=1&#34; /&gt;
  &lt;meta name=&#34;apple-mobile-web-app-capable&#34; content=&#34;yes&#34; /&gt;
  &lt;meta name=&#34;apple-mobile-web-app-status-bar-style&#34; content=&#34;black&#34; /&gt;
  &lt;link rel=&#34;apple-touch-icon-precomposed&#34; sizes=&#34;152x152&#34; href=&#34;figures/icons/apple-icon.png&#34; /&gt;
  &lt;link rel=&#34;shortcut icon&#34; href=&#34;figures/icons/favicon.ico&#34; type=&#34;image/x-icon&#34; /&gt;
&lt;link rel=&#34;prev&#34; href=&#34;app-et-metrics.html&#34;/&gt;
&lt;link rel=&#34;next&#34; href=&#34;app-intra-sim.html&#34;/&gt;
&lt;style type=&#34;text/css&#34;&gt;
p.abstract{
  text-align: center;
  font-weight: bold;
}
div.abstract{
  margin: auto;
  width: 90%;
}
&lt;/style&gt;
&lt;script src=&#34;libs/jquery-2.2.3/jquery.min.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;libs/gitbook-2.6.7/css/style.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;link href=&#34;libs/gitbook-2.6.7/css/plugin-table.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;link href=&#34;libs/gitbook-2.6.7/css/plugin-bookdown.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;link href=&#34;libs/gitbook-2.6.7/css/plugin-highlight.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;link href=&#34;libs/gitbook-2.6.7/css/plugin-search.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;link href=&#34;libs/gitbook-2.6.7/css/plugin-fontsettings.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;link href=&#34;libs/gitbook-2.6.7/css/plugin-clipboard.css&#34; rel=&#34;stylesheet&#34; /&gt;









&lt;script src=&#34;libs/kePrint-0.0.1/kePrint.js&#34;&gt;&lt;/script&gt;



&lt;link rel=&#34;stylesheet&#34; href=&#34;templates/style.css&#34; type=&#34;text/css&#34; /&gt;
&lt;/head&gt;

&lt;body&gt;



  &lt;div class=&#34;book without-animation with-summary font-size-2 font-family-1&#34; data-basepath=&#34;.&#34;&gt;

    &lt;div class=&#34;book-summary&#34;&gt;
      &lt;nav role=&#34;navigation&#34;&gt;

&lt;ul class=&#34;summary&#34;&gt;
&lt;li&gt;&lt;a href=&#34;introduction.html#introduction&#34;&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1&lt;/b&gt; &lt;strong&gt;Linguistic Complexity&lt;/strong&gt;&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.1&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subchap:categorizing&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.1&lt;/b&gt; Categorizing Linguistic Complexity Measures&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.2&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subchap:intrinsic&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.2&lt;/b&gt; Intrinsic Perspective&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.2.1&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subsubchap:structural&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.2.1&lt;/b&gt; Structural Linguistic Complexity&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.2.2&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subsubchap:lm-surprisal&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.2.2&lt;/b&gt; Language Modeling Surprisal&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.3&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subchap:extrinsic&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.3&lt;/b&gt; Extrinsic Perspective&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.3.1&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subsubchap:readability&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.3.1&lt;/b&gt; Automatic Readability Assessment&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.3.2&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subsubchap:pc&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.3.2&lt;/b&gt; Perceived Complexity Prediction&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.3.3&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subsubchap:eye-tracking&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.3.3&lt;/b&gt; Gaze Metrics Prediction&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.4&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subchap:garden-path&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.4&lt;/b&gt; Garden-path Sentences&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2&lt;/b&gt; &lt;strong&gt;Models of Linguistic Complexity&lt;/strong&gt;&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2.1&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html#subchap:desiderata&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2.1&lt;/b&gt; Desiderata for Models of Linguistic Complexity&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2.2&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html#subchap:nlm&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2.2&lt;/b&gt; Neural Language Models: Unsupervised Multitask Learners&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2.2.1&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html#subsubchap:syntax-nlm&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2.2.1&lt;/b&gt; Emergent Linguistic Structures in Neural Language Models&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2.3&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html#subchap:analyzing-nlm&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2.3&lt;/b&gt; Analyzing Neural Models of Complexity&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2.3.1&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html#subsubchap:probe&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2.3.1&lt;/b&gt; Probing classifiers&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2.3.2&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html#subsubchap:rsa&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2.3.2&lt;/b&gt; Representational Similarity Analysis&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2.3.3&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html#subsubchap:pwcca&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2.3.3&lt;/b&gt; Projection-Weighted Canonical Correlation Analysis&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3&lt;/b&gt; &lt;strong&gt;Complexity Phenomena in Linguistic Annotations and Language Models&lt;/strong&gt;&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3.1&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html#subchap:ex1-data&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3.1&lt;/b&gt; Data and Preprocessing&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3.2&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html#subchap:ex1-analysis&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3.2&lt;/b&gt; Analysis of Linguistic Phenomena&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3.2.1&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html#subsubchap:ex1-analysis-bins&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3.2.1&lt;/b&gt; Linguistic Phenomena in Length-controlled Bins&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3.3&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html#subchap:ex1-modeling&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3.3&lt;/b&gt; Modeling Online and Offline Linguistic Complexity&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3.3.1&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html#subsubchap:ex1-modeling-bins&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3.3.1&lt;/b&gt; Modeling Complexity in Length-controlled Bins&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3.4&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html#subchap:ex1-probing&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3.4&lt;/b&gt; Probing Linguistic Phenomena in ALBERT Representations&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3.5&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html#subchap:ex1-summary&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3.5&lt;/b&gt; Summary&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;4&#34; data-path=&#34;chap-ex2.html&#34;&gt;&lt;a href=&#34;chap-ex2.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;4&lt;/b&gt; &lt;strong&gt;Representational Similarity in Models of Complexity&lt;/strong&gt;&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;4.1&#34; data-path=&#34;chap-ex2.html&#34;&gt;&lt;a href=&#34;chap-ex2.html#knowledge-driven-requirements-for-learning-models&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;4.1&lt;/b&gt; Knowledge-driven Requirements for Learning Models&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;4.2&#34; data-path=&#34;chap-ex2.html&#34;&gt;&lt;a href=&#34;chap-ex2.html#subchap:ex2-experiments&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;4.2&lt;/b&gt; Experimentsl Evaluation&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;4.2.1&#34; data-path=&#34;chap-ex2.html&#34;&gt;&lt;a href=&#34;chap-ex2.html#subsubchap:ex2-data&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;4.2.1&lt;/b&gt; Data&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;4.2.2&#34; data-path=&#34;chap-ex2.html&#34;&gt;&lt;a href=&#34;chap-ex2.html#subsubchap:ex2-inter&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;4.2.2&lt;/b&gt; Inter-model Representational Similarity&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;4.2.3&#34; data-path=&#34;chap-ex2.html&#34;&gt;&lt;a href=&#34;chap-ex2.html#subsubchap:ex2-intra&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;4.2.3&lt;/b&gt; Intra-model Representational Similarity&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;4.3&#34; data-path=&#34;chap-ex2.html&#34;&gt;&lt;a href=&#34;chap-ex2.html#subchap:ex2-summary&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;4.3&lt;/b&gt; Summary&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;5&#34; data-path=&#34;chap-ex3.html&#34;&gt;&lt;a href=&#34;chap-ex3.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;5&lt;/b&gt; &lt;strong&gt;Gaze-informed Models for Cognitive Processing Prediction&lt;/strong&gt;&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;5.1&#34; data-path=&#34;chap-ex3.html&#34;&gt;&lt;a href=&#34;chap-ex3.html#subchap:ex3-setup&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;5.1&lt;/b&gt; Experimental Setup&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;5.2&#34; data-path=&#34;chap-ex3.html&#34;&gt;&lt;a href=&#34;chap-ex3.html#subchap:ex3-experiments&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;5.2&lt;/b&gt; Experimental Evaluation&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;5.2.1&#34; data-path=&#34;chap-ex3.html&#34;&gt;&lt;a href=&#34;chap-ex3.html#subsubchap:ex3-magnitudes&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;5.2.1&lt;/b&gt; Estimating Magnitudes of Garden-path Delays&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;5.2.2&#34; data-path=&#34;chap-ex3.html&#34;&gt;&lt;a href=&#34;chap-ex3.html#subsubchap:ex3-predicting&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;5.2.2&lt;/b&gt; Predicting Delays with Surprisal and Gaze Metrics&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;5.3&#34; data-path=&#34;chap-ex3.html&#34;&gt;&lt;a href=&#34;chap-ex3.html#subchap:ex3-summary&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;5.3&lt;/b&gt; Summary&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;conclusion.html#conclusion&#34;&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;&#34; data-path=&#34;conclusion.html&#34;&gt;&lt;a href=&#34;conclusion.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;Broader Impact and Ethical Perspectives&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;&#34; data-path=&#34;conclusion.html&#34;&gt;&lt;a href=&#34;conclusion.html#future-directions&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;Future Directions&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;appendix&#34;&gt;&lt;span&gt;&lt;b&gt;Appendix&lt;/b&gt;&lt;/span&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;A&#34; data-path=&#34;app-ling-feats.html&#34;&gt;&lt;a href=&#34;app-ling-feats.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;A&lt;/b&gt; Linguistic Features&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;A.1&#34; data-path=&#34;app-ling-feats.html&#34;&gt;&lt;a href=&#34;app-ling-feats.html#raw-text-properties-and-lexical-variety&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;A.1&lt;/b&gt; Raw Text Properties and Lexical Variety&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;A.2&#34; data-path=&#34;app-ling-feats.html&#34;&gt;&lt;a href=&#34;app-ling-feats.html#morpho-syntacting-information&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;A.2&lt;/b&gt; Morpho-syntacting Information&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;A.3&#34; data-path=&#34;app-ling-feats.html&#34;&gt;&lt;a href=&#34;app-ling-feats.html#verbal-predicate-structure&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;A.3&lt;/b&gt; Verbal Predicate Structure&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;A.4&#34; data-path=&#34;app-ling-feats.html&#34;&gt;&lt;a href=&#34;app-ling-feats.html#global-and-local-parsed-tree-structures&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;A.4&lt;/b&gt; Global and Local Parsed Tree Structures&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;A.5&#34; data-path=&#34;app-ling-feats.html&#34;&gt;&lt;a href=&#34;app-ling-feats.html#syntactic-relations&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;A.5&lt;/b&gt; Syntactic Relations&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;A.6&#34; data-path=&#34;app-ling-feats.html&#34;&gt;&lt;a href=&#34;app-ling-feats.html#subordination-phenomena&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;A.6&lt;/b&gt; Subordination Phenomena&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;B&#34; data-path=&#34;app-et-metrics.html&#34;&gt;&lt;a href=&#34;app-et-metrics.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;B&lt;/b&gt; Precisions on Eye-tracking Metrics and Preprocessing&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;C&#34; data-path=&#34;app-et-modeling.html&#34;&gt;&lt;a href=&#34;app-et-modeling.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;C&lt;/b&gt; Multi-task Token-level Regression for Gaze Metrics Prediction&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;D&#34; data-path=&#34;app-intra-sim.html&#34;&gt;&lt;a href=&#34;app-intra-sim.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;D&lt;/b&gt; Intra-model Similarity for All Models&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;E&#34; data-path=&#34;app-garden-paths-et.html&#34;&gt;&lt;a href=&#34;app-garden-paths-et.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;E&lt;/b&gt; Gaze Metrics Predictions for Garden Path Sentences&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;F&#34; data-path=&#34;app-params.html&#34;&gt;&lt;a href=&#34;app-params.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;F&lt;/b&gt; Reproducibility and Environmental Impact&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;&#34; data-path=&#34;references.html&#34;&gt;&lt;a href=&#34;references.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;References&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;divider&#34;&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://gsarti.com&#34;&gt;Back to my website&lt;/a&gt;&lt;/li&gt;

&lt;/ul&gt;

      &lt;/nav&gt;
    &lt;/div&gt;

    &lt;div class=&#34;book-body&#34;&gt;
      &lt;div class=&#34;body-inner&#34;&gt;
        &lt;div class=&#34;book-header&#34; role=&#34;navigation&#34;&gt;
          &lt;h1&gt;
            &lt;i class=&#34;fa fa-circle-o-notch fa-spin&#34;&gt;&lt;/i&gt;&lt;a href=&#34;./&#34;&gt;Interpreting Neural Language Models&lt;br /&gt;
for Linguistic Complexity Assessment&lt;/a&gt;
          &lt;/h1&gt;
        &lt;/div&gt;

        &lt;div class=&#34;page-wrapper&#34; tabindex=&#34;-1&#34; role=&#34;main&#34;&gt;
          &lt;div class=&#34;page-inner&#34;&gt;

            &lt;section class=&#34;normal&#34; id=&#34;section-&#34;&gt;
&lt;div id=&#34;app:et-modeling&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;C&lt;/span&gt; Multi-task Token-level Regression for Gaze Metrics Prediction&lt;/h1&gt;


&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:multitask-et&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;figures/appendix/A3_multitask_et.png&#34; alt=&#34;Multi-task token-level regression on eye-tracking annotations. Preceding punctuation is removed (1), and the sentence is tokenized while keeping track of non-initial tokens (2). Embeddings are fed to the ALBERT model (3), and non-initial representations are masked to ensure a one-to-one mapping between labels and predictions (4). Finally, task-specific prediction heads are used to predict gaze metrics in a multitask setting with hard parameter sharing (5).&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure C.1: Multi-task token-level regression on eye-tracking annotations. Preceding punctuation is removed (1), and the sentence is tokenized while keeping track of non-initial tokens (2). Embeddings are fed to the ALBERT model (3), and non-initial representations are masked to ensure a one-to-one mapping between labels and predictions (4). Finally, task-specific prediction heads are used to predict gaze metrics in a multitask setting with hard parameter sharing (5).
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;A multitask token-level regression fine-tuning approach was adopted throughout this study to predict eye-tracking metrics using neural language models. This novel approach’s choice stems from the fact that the regression task of predicting gaze metrics is inherently word-based given the granularity of eye-tracking annotations and that different gaze metrics provide complementary viewpoints over multiple stages of cognitive processing and can as such be modeled more precisely in a multitask learning setting. Figure &lt;a href=&#34;app-et-modeling.html#fig:multitask-et&#34;&gt;C.1&lt;/a&gt; presents the model’s training and inference procedure, closely matching other approaches used to train neural language models for sequence tagging tasks like POS tagging and named entity recognition.&lt;/p&gt;
&lt;p&gt;The most defining detail in the procedure is the need to preserve an exact one-to-one mapping between input words and gaze metrics annotations, which is non-trivial in light of subword tokenization approaches that represent nowadays the &lt;em&gt;de facto&lt;/em&gt; standard for training modern neural language models. To enforce such mapping, two steps are taken. First, all initial punctuation (e.g. the open parenthesis before &lt;em&gt;processing&lt;/em&gt; in Figure &lt;a href=&#34;app-et-modeling.html#fig:multitask-et&#34;&gt;C.1&lt;/a&gt; example) is removed to make the initial subword token for that word (i.e. the one preceded by whitespace) equal to the word’s first characters. Then, all non-initial subword tokens are identified in step (2), and their respective embeddings are masked in step (4) before passing the remaining initial embeddings (one per whitespace-tokenized word at this point, as for gaze metrics) to the set of prediction heads responsible for inferring individual gaze metrics. While this procedure can be regarded as suboptimal since not all learned representations are used for prediction, it is essential to remember that all the embeddings produced by attention-based neural language models are contextualized and encode information about the entire sentence and surrounding context to some extent. In this sense, initial token embeddings can be trained in this setting to predict gaze metrics relative to the whole word, effectively bypassing the issues about information loss raised by the masking procedure.&lt;/p&gt;
&lt;p&gt;Another important detail in the training and inference procedure is the standardization of metrics, which plays a key role in this setup due to the different ranges of different metrics (e.g. fixation probability is always defined in the interval &lt;span class=&#34;math inline&#34;&gt;\([0,1]\)&lt;/span&gt;, while gaze durations are integers in the scale of hundreds/thousands of milliseconds). Specifically, considering the set &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; of values assumed by a specific metric for all tokens in the eye-tracking datasets, the average &lt;span class=&#34;math inline&#34;&gt;\(\mu_X\)&lt;/span&gt; and standard deviation &lt;span class=&#34;math inline&#34;&gt;\(\sigma_X\)&lt;/span&gt; of those values are computed, and each value is transformed as:
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
X_i&amp;#39; = \frac{X_i - \mu_X}{\sigma_X}
\end{equation}\]&lt;/span&gt;
to produce a new range &lt;span class=&#34;math inline&#34;&gt;\(X&amp;#39;\)&lt;/span&gt; with average equal to &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; and standard deviation equal to &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;. Predicted values are then reconverted to the original scale as &lt;span class=&#34;math inline&#34;&gt;\(X_i = (X&amp;#39;_i \cdot \sigma_X) + \mu_X\)&lt;/span&gt; when performing inference, and training and testing metrics are computed on each metric’s original scale.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;custompar&#34;&gt;Spillover concatenation&lt;/span&gt; Cognitive processing literature reports evidence of reading times for a word being shaped not only by the predictability of the word itself but also by the predictability of the words that precede it &lt;span class=&#34;citation&#34;&gt;(Smith and Levy &lt;a href=&#34;#ref-smith-levy-2013-effect&#34;&gt;2013&lt;/a&gt;)&lt;/span&gt; in what is commonly referred to as the &lt;em&gt;spillover effect&lt;/em&gt; &lt;span class=&#34;citation&#34;&gt;(Mitchell &lt;a href=&#34;#ref-mitchell-1984-evaluation&#34;&gt;1984&lt;/a&gt;)&lt;/span&gt;. The existence of spillover has important implications in the context of this gaze metrics prediction approach since the embeddings for a single word may not contain enough information to predict the influence of preceding tokens in shaping reading behaviors. Notably, &lt;span class=&#34;citation&#34;&gt;Schijndel and Linzen (&lt;a href=&#34;#ref-schjindel-linzen-2020-single&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt; include the surprisal of the three previous words in a mixed-effect model used to estimate a surprisal-to-reading-times conversion coefficient. While it can be hypothesized that in this approach, the usage of contextualized word embeddings can automatically account for this type of interaction, the effect of leveraging preceding tokens for the current token’s metric prediction is assessed to confirm this hypothesis. A new procedure defined as &lt;em&gt;spillover concatenation&lt;/em&gt; is introduced for this purpose, in which token embeddings are augmented by performing a rolling concatenation of the &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; preceding embeddings before feeding the final representation to prediction heads. Initial tokens are padded with &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; vectors to match the fixed size defined by embedding size and the &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; parameter. For example, using spillover concatenation with &lt;span class=&#34;math inline&#34;&gt;\(n = 3\)&lt;/span&gt; within a BERT model with a hidden size of 768 involves having prediction heads taking input size of &lt;span class=&#34;math inline&#34;&gt;\(768 \cdot (3 + 1) = 3072\)&lt;/span&gt;, the size of the token embedding for which gaze metrics should be predicted plus the size of the three preceding token embeddings. In this way, information about preceding tokens is explicitly included at prediction time.&lt;/p&gt;
&lt;p&gt;Figure &lt;a href=&#34;app-et-modeling.html#fig:spillover-training&#34;&gt;C.2&lt;/a&gt; shows the validation losses during training for the two models used in the experiments of Chapter &lt;a href=&#34;chap-ex3.html#chap:ex3&#34;&gt;5&lt;/a&gt; with their counterparts using spillover concatenation. Model performances are not positively influenced by introducing the concatenation technique and remain very similar for both architectures.&lt;/p&gt;

&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:spillover-training&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;figures/appendix/A3_spillover_training.png&#34; alt=&#34;Validation total loss for GPT-2 and ALBERT over a split of the eye-tracking merged corpora with and without spillover concatenation. Model predictive performances were comparable across training and testing for the two models.&#34; width=&#34;70%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure C.2: Validation total loss for GPT-2 and ALBERT over a split of the eye-tracking merged corpora with and without spillover concatenation. Model predictive performances were comparable across training and testing for the two models.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;span class=&#34;custompar&#34;&gt;Model performances&lt;/span&gt; Table &lt;a href=&#34;app-et-modeling.html#tab:multitask-et-scores&#34;&gt;C.1&lt;/a&gt; presents the test performances of ALBERT and GPT-2 models trained with and without the spillover concatenation approach on the merge of all eye-tracking corpora. The top two rows present descriptive statistics about extreme values, the mean and standard deviation in annotations averaged across participants for each metric. It is interesting to observe that the maximum value observed for first pass duration (FPD) is higher than the one for total fixation duration (TFD). While this situation would not be possible in practice due to first pass duration being included in total reading times, it reminds us about the approximate nature of our filling-and-averaging procedure described in Appendix &lt;a href=&#34;app-et-metrics.html#app:et-metrics&#34;&gt;B&lt;/a&gt;. Comparing results to those of Table &lt;a href=&#34;chap-ex1.html#tab:ex1-results&#34;&gt;3.2&lt;/a&gt;, where gaze metrics were modeled at the sentence level, we observe much worse results in terms of explained variance for both models: while fixations and first pass duration (FXC, FXP, FPD) are generally well modeled, worse results are obtained for first and total fixation durations (FFD, TFD), and in particular for the duration of regression (TRD). These results can be attributed to the merging of different corpora that, being annotated by different participants, present very different properties, as shown in Table &lt;a href=&#34;chap-ling-comp.html#tab:et-corpora&#34;&gt;1.4&lt;/a&gt; and Figure &lt;a href=&#34;chap-ex3.html#fig:surprisal-ratios&#34;&gt;5.2&lt;/a&gt;. While on the one hand, this choice harms modeling performances, on the other hand, it provides us with more representative results for the general setting.&lt;/p&gt;
&lt;table class=&#34;table&#34; style=&#34;font-size: 11px; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;caption style=&#34;font-size: initial !important;&#34;&gt;
&lt;span id=&#34;tab:multitask-et-scores&#34;&gt;Table C.1: &lt;/span&gt;Descriptive statistics and model performances for the merged eye-tracking training corpus. Model scores are in format &lt;span class=&#34;math inline&#34;&gt;\(\text{RMSE}_{\text{MAX}}|R^2\)&lt;/span&gt;, where RMSE is the root-mean-squared error and MAX is the max error for model predictions.
&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;font-weight: bold;&#34;&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:center;font-weight: bold;&#34;&gt;
FFD
&lt;/th&gt;
&lt;th style=&#34;text-align:center;font-weight: bold;&#34;&gt;
FPD
&lt;/th&gt;
&lt;th style=&#34;text-align:center;font-weight: bold;&#34;&gt;
FXP
&lt;/th&gt;
&lt;th style=&#34;text-align:center;font-weight: bold;&#34;&gt;
FXC
&lt;/th&gt;
&lt;th style=&#34;text-align:center;font-weight: bold;&#34;&gt;
TFD
&lt;/th&gt;
&lt;th style=&#34;text-align:center;font-weight: bold;&#34;&gt;
TRD
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
min-max value
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(0-986\)&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(0-2327\)&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(0-1\)&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(0-8.18\)&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(0-1804\)&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(0-4055\)&lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\mu|\sigma\)&lt;/span&gt; statistics
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(162|50\)&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(188|86\)&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(.56|.27\)&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(.85|.53\)&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(206|87\)&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(90|122\)&lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
ALBERT
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(41_{78}|.33\)&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(61_{121}|.50\)&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(.17_{.32}|.60\)&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(.31_{.62}|.66\)&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(65_{132}|.44\)&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(110_{207}|.19\)&lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
ALBERT Spillover
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(41_{78}|.33\)&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(61_{122}|.50\)&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(.17_{.33}|.60\)&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(.31_{.62}|.66\)&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(65_{132}|.44\)&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(110_{208}|.19\)&lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
GPT-2
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(44_{83}|.23\)&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(68_{136}|.37\)&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(.18_{.35}|.56\)&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(.36_{.70}|.54\)&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(74_{149}|.28\)&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(115_{222}|.11\)&lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
GPT-2 Spillover
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(43_{83}|.26\)&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(68_{135}|.37\)&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(.19_{.35}|.50\)&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(.36_{.70}|.54\)&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(73_{146}|.30\)&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(116_{220}|.10\)&lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;In general, better performances are observed for the masked language model ALBERT, suggesting the importance of having access to bidirectional context for gaze metrics prediction. Results present additional evidence supporting the superfluity of the spillover concatenation procedure, which was henceforth dropped in the context of Chapters &lt;a href=&#34;chap-ex2.html#chap:ex2&#34;&gt;4&lt;/a&gt; and &lt;a href=&#34;chap-ex3.html#chap:ex3&#34;&gt;5&lt;/a&gt;’s experiments. Although good scores in terms of average and maximal errors are observed for all metrics, the relatively low &lt;span class=&#34;math inline&#34;&gt;\(R^2\)&lt;/span&gt; seem to suggest that large margins of improvement are still available in the context of gaze metrics predictions with neural language models.&lt;/p&gt;
&lt;/div&gt;
&lt;h3&gt;References&lt;/h3&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-mitchell-1984-evaluation&#34;&gt;
&lt;p&gt;Mitchell, Don C. 1984. “An Evaluation of Subject-Paced Reading Tasks and Other Methods for Investigating Immediate Processes in Reading.” &lt;em&gt;New Methods in Reading Comprehension Research&lt;/em&gt;, 69–89.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-schjindel-linzen-2020-single&#34;&gt;
&lt;p&gt;Schijndel, Marten van, and Tal Linzen. 2020. “Single-Stage Prediction Models Do Not Explain the Magnitude of Syntactic Disambiguation Difficulty.” &lt;em&gt;PsyArXiv Pre-Print&lt;/em&gt; sgbqy. &lt;a href=&#34;https://psyarxiv.com/sgbqy/&#34;&gt;https://psyarxiv.com/sgbqy/&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-smith-levy-2013-effect&#34;&gt;
&lt;p&gt;Smith, Nathaniel J, and Roger Levy. 2013. “The Effect of Word Predictability on Reading Time Is Logarithmic.” &lt;em&gt;Cognition&lt;/em&gt; 128 (3). Elsevier: 302–19.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
            &lt;/section&gt;

          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
&lt;a href=&#34;app-et-metrics.html&#34; class=&#34;navigation navigation-prev &#34; aria-label=&#34;Previous page&#34;&gt;&lt;i class=&#34;fa fa-angle-left&#34;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&#34;app-intra-sim.html&#34; class=&#34;navigation navigation-next &#34; aria-label=&#34;Next page&#34;&gt;&lt;i class=&#34;fa fa-angle-right&#34;&gt;&lt;/i&gt;&lt;/a&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/app.min.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/lunr.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/clipboard.min.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/plugin-search.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/plugin-sharing.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/plugin-fontsettings.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/plugin-bookdown.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/jquery.highlight.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/plugin-clipboard.js&#34;&gt;&lt;/script&gt;
&lt;script&gt;
gitbook.require([&#34;gitbook&#34;], function(gitbook) {
gitbook.start({
&#34;sharing&#34;: {
&#34;github&#34;: true,
&#34;facebook&#34;: true,
&#34;twitter&#34;: true,
&#34;linkedin&#34;: true,
&#34;weibo&#34;: false,
&#34;instapaper&#34;: false,
&#34;vk&#34;: false,
&#34;all&#34;: false
},
&#34;fontsettings&#34;: {
&#34;theme&#34;: &#34;white&#34;,
&#34;family&#34;: &#34;sans&#34;,
&#34;size&#34;: 2
},
&#34;edit&#34;: {
&#34;link&#34;: &#34;https://github.com/gsarti/master-thesis/tree/master/extra/Appendix.Rmd&#34;,
&#34;text&#34;: &#34;Edit&#34;
},
&#34;history&#34;: {
&#34;link&#34;: null,
&#34;text&#34;: null
},
&#34;view&#34;: {
&#34;link&#34;: null,
&#34;text&#34;: null
},
&#34;download&#34;: [[&#34;Sarti_2020_Interpreting_NLMs_for_LCA.pdf&#34;, &#34;PDF&#34;]],
&#34;toc&#34;: {
&#34;collapse&#34;: &#34;subsection&#34;,
&#34;scroll_highlight&#34;: true
},
&#34;info&#34;: false
});
});
&lt;/script&gt;

&lt;!-- dynamically load mathjax for compatibility with self-contained --&gt;
&lt;script&gt;
  (function () {
    var script = document.createElement(&#34;script&#34;);
    script.type = &#34;text/javascript&#34;;
    var src = &#34;true&#34;;
    if (src === &#34;&#34; || src === &#34;true&#34;) src = &#34;https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML&#34;;
    if (location.protocol !== &#34;file:&#34;)
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, &#39;&#39;);
    script.src = src;
    document.getElementsByTagName(&#34;head&#34;)[0].appendChild(script);
  })();
&lt;/script&gt;
&lt;/body&gt;

&lt;/html&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>http://localhost:1313/msc-thesis/app-garden-paths-et/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/msc-thesis/app-garden-paths-et/</guid>
      <description>&lt;!DOCTYPE html&gt;
&lt;html lang=&#34;&#34; xml:lang=&#34;&#34;&gt;
&lt;head&gt;

  &lt;meta charset=&#34;utf-8&#34; /&gt;
  &lt;meta http-equiv=&#34;X-UA-Compatible&#34; content=&#34;IE=edge&#34; /&gt;
  &lt;title&gt;E Gaze Metrics Predictions for Garden Path Sentences | Interpreting Neural Language Models for Linguistic Complexity Assessment&lt;/title&gt;
  &lt;meta name=&#34;description&#34; content=&#34;MSc Thesis at the University of Trieste and SISSA under the supervision of Prof. Davide Crepaldi and Dott. Felice dell&#39;Orletta&#34; /&gt;
  &lt;meta name=&#34;generator&#34; content=&#34;bookdown 0.20.6 and GitBook 2.6.7&#34; /&gt;

  &lt;meta property=&#34;og:title&#34; content=&#34;E Gaze Metrics Predictions for Garden Path Sentences | Interpreting Neural Language Models for Linguistic Complexity Assessment&#34; /&gt;
  &lt;meta property=&#34;og:type&#34; content=&#34;book&#34; /&gt;
  &lt;meta property=&#34;og:url&#34; content=&#34;https://gsarti.com/master-thesis&#34; /&gt;
  &lt;meta property=&#34;og:image&#34; content=&#34;https://gsarti.com/master-thesisfigures/cover.png&#34; /&gt;
  &lt;meta property=&#34;og:description&#34; content=&#34;MSc Thesis at the University of Trieste and SISSA under the supervision of Prof. Davide Crepaldi and Dott. Felice dell&#39;Orletta&#34; /&gt;
  &lt;meta name=&#34;github-repo&#34; content=&#34;gsarti/interpreting-complexity&#34; /&gt;

  &lt;meta name=&#34;twitter:card&#34; content=&#34;summary&#34; /&gt;
  &lt;meta name=&#34;twitter:title&#34; content=&#34;E Gaze Metrics Predictions for Garden Path Sentences | Interpreting Neural Language Models for Linguistic Complexity Assessment&#34; /&gt;
  
  &lt;meta name=&#34;twitter:description&#34; content=&#34;MSc Thesis at the University of Trieste and SISSA under the supervision of Prof. Davide Crepaldi and Dott. Felice dell&#39;Orletta&#34; /&gt;
  &lt;meta name=&#34;twitter:image&#34; content=&#34;https://gsarti.com/master-thesisfigures/cover.png&#34; /&gt;

&lt;meta name=&#34;author&#34; content=&#34;Gabriele Sarti&#34; /&gt;



  &lt;meta name=&#34;viewport&#34; content=&#34;width=device-width, initial-scale=1&#34; /&gt;
  &lt;meta name=&#34;apple-mobile-web-app-capable&#34; content=&#34;yes&#34; /&gt;
  &lt;meta name=&#34;apple-mobile-web-app-status-bar-style&#34; content=&#34;black&#34; /&gt;
  &lt;link rel=&#34;apple-touch-icon-precomposed&#34; sizes=&#34;152x152&#34; href=&#34;figures/icons/apple-icon.png&#34; /&gt;
  &lt;link rel=&#34;shortcut icon&#34; href=&#34;figures/icons/favicon.ico&#34; type=&#34;image/x-icon&#34; /&gt;
&lt;link rel=&#34;prev&#34; href=&#34;app-intra-sim.html&#34;/&gt;
&lt;link rel=&#34;next&#34; href=&#34;app-params.html&#34;/&gt;
&lt;style type=&#34;text/css&#34;&gt;
p.abstract{
  text-align: center;
  font-weight: bold;
}
div.abstract{
  margin: auto;
  width: 90%;
}
&lt;/style&gt;
&lt;script src=&#34;libs/jquery-2.2.3/jquery.min.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;libs/gitbook-2.6.7/css/style.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;link href=&#34;libs/gitbook-2.6.7/css/plugin-table.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;link href=&#34;libs/gitbook-2.6.7/css/plugin-bookdown.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;link href=&#34;libs/gitbook-2.6.7/css/plugin-highlight.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;link href=&#34;libs/gitbook-2.6.7/css/plugin-search.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;link href=&#34;libs/gitbook-2.6.7/css/plugin-fontsettings.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;link href=&#34;libs/gitbook-2.6.7/css/plugin-clipboard.css&#34; rel=&#34;stylesheet&#34; /&gt;









&lt;script src=&#34;libs/kePrint-0.0.1/kePrint.js&#34;&gt;&lt;/script&gt;



&lt;link rel=&#34;stylesheet&#34; href=&#34;templates/style.css&#34; type=&#34;text/css&#34; /&gt;
&lt;/head&gt;

&lt;body&gt;



  &lt;div class=&#34;book without-animation with-summary font-size-2 font-family-1&#34; data-basepath=&#34;.&#34;&gt;

    &lt;div class=&#34;book-summary&#34;&gt;
      &lt;nav role=&#34;navigation&#34;&gt;

&lt;ul class=&#34;summary&#34;&gt;
&lt;li&gt;&lt;a href=&#34;introduction.html#introduction&#34;&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1&lt;/b&gt; &lt;strong&gt;Linguistic Complexity&lt;/strong&gt;&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.1&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subchap:categorizing&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.1&lt;/b&gt; Categorizing Linguistic Complexity Measures&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.2&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subchap:intrinsic&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.2&lt;/b&gt; Intrinsic Perspective&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.2.1&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subsubchap:structural&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.2.1&lt;/b&gt; Structural Linguistic Complexity&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.2.2&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subsubchap:lm-surprisal&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.2.2&lt;/b&gt; Language Modeling Surprisal&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.3&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subchap:extrinsic&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.3&lt;/b&gt; Extrinsic Perspective&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.3.1&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subsubchap:readability&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.3.1&lt;/b&gt; Automatic Readability Assessment&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.3.2&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subsubchap:pc&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.3.2&lt;/b&gt; Perceived Complexity Prediction&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.3.3&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subsubchap:eye-tracking&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.3.3&lt;/b&gt; Gaze Metrics Prediction&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.4&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subchap:garden-path&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.4&lt;/b&gt; Garden-path Sentences&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2&lt;/b&gt; &lt;strong&gt;Models of Linguistic Complexity&lt;/strong&gt;&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2.1&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html#subchap:desiderata&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2.1&lt;/b&gt; Desiderata for Models of Linguistic Complexity&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2.2&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html#subchap:nlm&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2.2&lt;/b&gt; Neural Language Models: Unsupervised Multitask Learners&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2.2.1&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html#subsubchap:syntax-nlm&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2.2.1&lt;/b&gt; Emergent Linguistic Structures in Neural Language Models&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2.3&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html#subchap:analyzing-nlm&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2.3&lt;/b&gt; Analyzing Neural Models of Complexity&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2.3.1&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html#subsubchap:probe&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2.3.1&lt;/b&gt; Probing classifiers&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2.3.2&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html#subsubchap:rsa&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2.3.2&lt;/b&gt; Representational Similarity Analysis&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2.3.3&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html#subsubchap:pwcca&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2.3.3&lt;/b&gt; Projection-Weighted Canonical Correlation Analysis&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3&lt;/b&gt; &lt;strong&gt;Complexity Phenomena in Linguistic Annotations and Language Models&lt;/strong&gt;&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3.1&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html#subchap:ex1-data&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3.1&lt;/b&gt; Data and Preprocessing&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3.2&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html#subchap:ex1-analysis&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3.2&lt;/b&gt; Analysis of Linguistic Phenomena&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3.2.1&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html#subsubchap:ex1-analysis-bins&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3.2.1&lt;/b&gt; Linguistic Phenomena in Length-controlled Bins&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3.3&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html#subchap:ex1-modeling&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3.3&lt;/b&gt; Modeling Online and Offline Linguistic Complexity&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3.3.1&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html#subsubchap:ex1-modeling-bins&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3.3.1&lt;/b&gt; Modeling Complexity in Length-controlled Bins&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3.4&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html#subchap:ex1-probing&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3.4&lt;/b&gt; Probing Linguistic Phenomena in ALBERT Representations&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3.5&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html#subchap:ex1-summary&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3.5&lt;/b&gt; Summary&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;4&#34; data-path=&#34;chap-ex2.html&#34;&gt;&lt;a href=&#34;chap-ex2.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;4&lt;/b&gt; &lt;strong&gt;Representational Similarity in Models of Complexity&lt;/strong&gt;&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;4.1&#34; data-path=&#34;chap-ex2.html&#34;&gt;&lt;a href=&#34;chap-ex2.html#knowledge-driven-requirements-for-learning-models&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;4.1&lt;/b&gt; Knowledge-driven Requirements for Learning Models&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;4.2&#34; data-path=&#34;chap-ex2.html&#34;&gt;&lt;a href=&#34;chap-ex2.html#subchap:ex2-experiments&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;4.2&lt;/b&gt; Experimentsl Evaluation&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;4.2.1&#34; data-path=&#34;chap-ex2.html&#34;&gt;&lt;a href=&#34;chap-ex2.html#subsubchap:ex2-data&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;4.2.1&lt;/b&gt; Data&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;4.2.2&#34; data-path=&#34;chap-ex2.html&#34;&gt;&lt;a href=&#34;chap-ex2.html#subsubchap:ex2-inter&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;4.2.2&lt;/b&gt; Inter-model Representational Similarity&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;4.2.3&#34; data-path=&#34;chap-ex2.html&#34;&gt;&lt;a href=&#34;chap-ex2.html#subsubchap:ex2-intra&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;4.2.3&lt;/b&gt; Intra-model Representational Similarity&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;4.3&#34; data-path=&#34;chap-ex2.html&#34;&gt;&lt;a href=&#34;chap-ex2.html#subchap:ex2-summary&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;4.3&lt;/b&gt; Summary&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;5&#34; data-path=&#34;chap-ex3.html&#34;&gt;&lt;a href=&#34;chap-ex3.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;5&lt;/b&gt; &lt;strong&gt;Gaze-informed Models for Cognitive Processing Prediction&lt;/strong&gt;&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;5.1&#34; data-path=&#34;chap-ex3.html&#34;&gt;&lt;a href=&#34;chap-ex3.html#subchap:ex3-setup&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;5.1&lt;/b&gt; Experimental Setup&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;5.2&#34; data-path=&#34;chap-ex3.html&#34;&gt;&lt;a href=&#34;chap-ex3.html#subchap:ex3-experiments&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;5.2&lt;/b&gt; Experimental Evaluation&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;5.2.1&#34; data-path=&#34;chap-ex3.html&#34;&gt;&lt;a href=&#34;chap-ex3.html#subsubchap:ex3-magnitudes&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;5.2.1&lt;/b&gt; Estimating Magnitudes of Garden-path Delays&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;5.2.2&#34; data-path=&#34;chap-ex3.html&#34;&gt;&lt;a href=&#34;chap-ex3.html#subsubchap:ex3-predicting&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;5.2.2&lt;/b&gt; Predicting Delays with Surprisal and Gaze Metrics&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;5.3&#34; data-path=&#34;chap-ex3.html&#34;&gt;&lt;a href=&#34;chap-ex3.html#subchap:ex3-summary&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;5.3&lt;/b&gt; Summary&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;conclusion.html#conclusion&#34;&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;&#34; data-path=&#34;conclusion.html&#34;&gt;&lt;a href=&#34;conclusion.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;Broader Impact and Ethical Perspectives&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;&#34; data-path=&#34;conclusion.html&#34;&gt;&lt;a href=&#34;conclusion.html#future-directions&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;Future Directions&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;appendix&#34;&gt;&lt;span&gt;&lt;b&gt;Appendix&lt;/b&gt;&lt;/span&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;A&#34; data-path=&#34;app-ling-feats.html&#34;&gt;&lt;a href=&#34;app-ling-feats.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;A&lt;/b&gt; Linguistic Features&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;A.1&#34; data-path=&#34;app-ling-feats.html&#34;&gt;&lt;a href=&#34;app-ling-feats.html#raw-text-properties-and-lexical-variety&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;A.1&lt;/b&gt; Raw Text Properties and Lexical Variety&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;A.2&#34; data-path=&#34;app-ling-feats.html&#34;&gt;&lt;a href=&#34;app-ling-feats.html#morpho-syntacting-information&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;A.2&lt;/b&gt; Morpho-syntacting Information&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;A.3&#34; data-path=&#34;app-ling-feats.html&#34;&gt;&lt;a href=&#34;app-ling-feats.html#verbal-predicate-structure&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;A.3&lt;/b&gt; Verbal Predicate Structure&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;A.4&#34; data-path=&#34;app-ling-feats.html&#34;&gt;&lt;a href=&#34;app-ling-feats.html#global-and-local-parsed-tree-structures&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;A.4&lt;/b&gt; Global and Local Parsed Tree Structures&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;A.5&#34; data-path=&#34;app-ling-feats.html&#34;&gt;&lt;a href=&#34;app-ling-feats.html#syntactic-relations&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;A.5&lt;/b&gt; Syntactic Relations&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;A.6&#34; data-path=&#34;app-ling-feats.html&#34;&gt;&lt;a href=&#34;app-ling-feats.html#subordination-phenomena&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;A.6&lt;/b&gt; Subordination Phenomena&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;B&#34; data-path=&#34;app-et-metrics.html&#34;&gt;&lt;a href=&#34;app-et-metrics.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;B&lt;/b&gt; Precisions on Eye-tracking Metrics and Preprocessing&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;C&#34; data-path=&#34;app-et-modeling.html&#34;&gt;&lt;a href=&#34;app-et-modeling.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;C&lt;/b&gt; Multi-task Token-level Regression for Gaze Metrics Prediction&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;D&#34; data-path=&#34;app-intra-sim.html&#34;&gt;&lt;a href=&#34;app-intra-sim.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;D&lt;/b&gt; Intra-model Similarity for All Models&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;E&#34; data-path=&#34;app-garden-paths-et.html&#34;&gt;&lt;a href=&#34;app-garden-paths-et.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;E&lt;/b&gt; Gaze Metrics Predictions for Garden Path Sentences&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;F&#34; data-path=&#34;app-params.html&#34;&gt;&lt;a href=&#34;app-params.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;F&lt;/b&gt; Reproducibility and Environmental Impact&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;&#34; data-path=&#34;references.html&#34;&gt;&lt;a href=&#34;references.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;References&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;divider&#34;&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://gsarti.com&#34;&gt;Back to my website&lt;/a&gt;&lt;/li&gt;

&lt;/ul&gt;

      &lt;/nav&gt;
    &lt;/div&gt;

    &lt;div class=&#34;book-body&#34;&gt;
      &lt;div class=&#34;body-inner&#34;&gt;
        &lt;div class=&#34;book-header&#34; role=&#34;navigation&#34;&gt;
          &lt;h1&gt;
            &lt;i class=&#34;fa fa-circle-o-notch fa-spin&#34;&gt;&lt;/i&gt;&lt;a href=&#34;./&#34;&gt;Interpreting Neural Language Models&lt;br /&gt;
for Linguistic Complexity Assessment&lt;/a&gt;
          &lt;/h1&gt;
        &lt;/div&gt;

        &lt;div class=&#34;page-wrapper&#34; tabindex=&#34;-1&#34; role=&#34;main&#34;&gt;
          &lt;div class=&#34;page-inner&#34;&gt;

            &lt;section class=&#34;normal&#34; id=&#34;section-&#34;&gt;
&lt;div id=&#34;app:garden-paths-et&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;E&lt;/span&gt; Gaze Metrics Predictions for Garden Path Sentences&lt;/h1&gt;

&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:gpt2-npz-ambig-et&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;figures/appendix/A5_gpt2_npz_ambig_et.png&#34; alt=&#34;Average GPT2-ET gaze metrics predictions for the “NP/Z Ambiguity with Verb Transitivity” SyntaxGym test suite. Bars show 95% confidence intervals. Units are in ms for durations, % for FXP, and raw counts for FXC.&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure E.1: Average GPT2-ET gaze metrics predictions for the “NP/Z Ambiguity with Verb Transitivity” SyntaxGym test suite. Bars show 95% confidence intervals. Units are in ms for durations, % for FXP, and raw counts for FXC.
&lt;/p&gt;
&lt;/div&gt;

&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:gpt2-npz-obj-et&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;figures/appendix/A5_gpt2_npz_obj_et.png&#34; alt=&#34;Average GPT2-ET gaze metrics predictions for the “NP/Z Ambiguity with Overt Object” SyntaxGym test suite. Bars show 95% confidence intervals. Units are in ms for durations, % for FXP, and raw counts for FXC.&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure E.2: Average GPT2-ET gaze metrics predictions for the “NP/Z Ambiguity with Overt Object” SyntaxGym test suite. Bars show 95% confidence intervals. Units are in ms for durations, % for FXP, and raw counts for FXC.
&lt;/p&gt;
&lt;/div&gt;

&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:gpt2-mvrr-et&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;figures/appendix/A5_gpt2_mvrr_et.png&#34; alt=&#34;Average GPT2-ET gaze metrics predictions for the “MV/RR Ambiguity” SyntaxGym test suite. Bars show 95% confidence intervals. Units are in ms for durations, % for FXP, and raw counts for FXC.&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure E.3: Average GPT2-ET gaze metrics predictions for the “MV/RR Ambiguity” SyntaxGym test suite. Bars show 95% confidence intervals. Units are in ms for durations, % for FXP, and raw counts for FXC.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
            &lt;/section&gt;

          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
&lt;a href=&#34;app-intra-sim.html&#34; class=&#34;navigation navigation-prev &#34; aria-label=&#34;Previous page&#34;&gt;&lt;i class=&#34;fa fa-angle-left&#34;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&#34;app-params.html&#34; class=&#34;navigation navigation-next &#34; aria-label=&#34;Next page&#34;&gt;&lt;i class=&#34;fa fa-angle-right&#34;&gt;&lt;/i&gt;&lt;/a&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/app.min.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/lunr.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/clipboard.min.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/plugin-search.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/plugin-sharing.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/plugin-fontsettings.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/plugin-bookdown.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/jquery.highlight.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/plugin-clipboard.js&#34;&gt;&lt;/script&gt;
&lt;script&gt;
gitbook.require([&#34;gitbook&#34;], function(gitbook) {
gitbook.start({
&#34;sharing&#34;: {
&#34;github&#34;: true,
&#34;facebook&#34;: true,
&#34;twitter&#34;: true,
&#34;linkedin&#34;: true,
&#34;weibo&#34;: false,
&#34;instapaper&#34;: false,
&#34;vk&#34;: false,
&#34;all&#34;: false
},
&#34;fontsettings&#34;: {
&#34;theme&#34;: &#34;white&#34;,
&#34;family&#34;: &#34;sans&#34;,
&#34;size&#34;: 2
},
&#34;edit&#34;: {
&#34;link&#34;: &#34;https://github.com/gsarti/master-thesis/tree/master/extra/Appendix.Rmd&#34;,
&#34;text&#34;: &#34;Edit&#34;
},
&#34;history&#34;: {
&#34;link&#34;: null,
&#34;text&#34;: null
},
&#34;view&#34;: {
&#34;link&#34;: null,
&#34;text&#34;: null
},
&#34;download&#34;: [[&#34;Sarti_2020_Interpreting_NLMs_for_LCA.pdf&#34;, &#34;PDF&#34;]],
&#34;toc&#34;: {
&#34;collapse&#34;: &#34;subsection&#34;,
&#34;scroll_highlight&#34;: true
},
&#34;info&#34;: false
});
});
&lt;/script&gt;

&lt;!-- dynamically load mathjax for compatibility with self-contained --&gt;
&lt;script&gt;
  (function () {
    var script = document.createElement(&#34;script&#34;);
    script.type = &#34;text/javascript&#34;;
    var src = &#34;true&#34;;
    if (src === &#34;&#34; || src === &#34;true&#34;) src = &#34;https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML&#34;;
    if (location.protocol !== &#34;file:&#34;)
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, &#39;&#39;);
    script.src = src;
    document.getElementsByTagName(&#34;head&#34;)[0].appendChild(script);
  })();
&lt;/script&gt;
&lt;/body&gt;

&lt;/html&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>http://localhost:1313/msc-thesis/app-intra-sim/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/msc-thesis/app-intra-sim/</guid>
      <description>&lt;!DOCTYPE html&gt;
&lt;html lang=&#34;&#34; xml:lang=&#34;&#34;&gt;
&lt;head&gt;

  &lt;meta charset=&#34;utf-8&#34; /&gt;
  &lt;meta http-equiv=&#34;X-UA-Compatible&#34; content=&#34;IE=edge&#34; /&gt;
  &lt;title&gt;D Intra-model Similarity for All Models | Interpreting Neural Language Models for Linguistic Complexity Assessment&lt;/title&gt;
  &lt;meta name=&#34;description&#34; content=&#34;MSc Thesis at the University of Trieste and SISSA under the supervision of Prof. Davide Crepaldi and Dott. Felice dell&#39;Orletta&#34; /&gt;
  &lt;meta name=&#34;generator&#34; content=&#34;bookdown 0.20.6 and GitBook 2.6.7&#34; /&gt;

  &lt;meta property=&#34;og:title&#34; content=&#34;D Intra-model Similarity for All Models | Interpreting Neural Language Models for Linguistic Complexity Assessment&#34; /&gt;
  &lt;meta property=&#34;og:type&#34; content=&#34;book&#34; /&gt;
  &lt;meta property=&#34;og:url&#34; content=&#34;https://gsarti.com/master-thesis&#34; /&gt;
  &lt;meta property=&#34;og:image&#34; content=&#34;https://gsarti.com/master-thesisfigures/cover.png&#34; /&gt;
  &lt;meta property=&#34;og:description&#34; content=&#34;MSc Thesis at the University of Trieste and SISSA under the supervision of Prof. Davide Crepaldi and Dott. Felice dell&#39;Orletta&#34; /&gt;
  &lt;meta name=&#34;github-repo&#34; content=&#34;gsarti/interpreting-complexity&#34; /&gt;

  &lt;meta name=&#34;twitter:card&#34; content=&#34;summary&#34; /&gt;
  &lt;meta name=&#34;twitter:title&#34; content=&#34;D Intra-model Similarity for All Models | Interpreting Neural Language Models for Linguistic Complexity Assessment&#34; /&gt;
  
  &lt;meta name=&#34;twitter:description&#34; content=&#34;MSc Thesis at the University of Trieste and SISSA under the supervision of Prof. Davide Crepaldi and Dott. Felice dell&#39;Orletta&#34; /&gt;
  &lt;meta name=&#34;twitter:image&#34; content=&#34;https://gsarti.com/master-thesisfigures/cover.png&#34; /&gt;

&lt;meta name=&#34;author&#34; content=&#34;Gabriele Sarti&#34; /&gt;



  &lt;meta name=&#34;viewport&#34; content=&#34;width=device-width, initial-scale=1&#34; /&gt;
  &lt;meta name=&#34;apple-mobile-web-app-capable&#34; content=&#34;yes&#34; /&gt;
  &lt;meta name=&#34;apple-mobile-web-app-status-bar-style&#34; content=&#34;black&#34; /&gt;
  &lt;link rel=&#34;apple-touch-icon-precomposed&#34; sizes=&#34;152x152&#34; href=&#34;figures/icons/apple-icon.png&#34; /&gt;
  &lt;link rel=&#34;shortcut icon&#34; href=&#34;figures/icons/favicon.ico&#34; type=&#34;image/x-icon&#34; /&gt;
&lt;link rel=&#34;prev&#34; href=&#34;app-et-modeling.html&#34;/&gt;
&lt;link rel=&#34;next&#34; href=&#34;app-garden-paths-et.html&#34;/&gt;
&lt;style type=&#34;text/css&#34;&gt;
p.abstract{
  text-align: center;
  font-weight: bold;
}
div.abstract{
  margin: auto;
  width: 90%;
}
&lt;/style&gt;
&lt;script src=&#34;libs/jquery-2.2.3/jquery.min.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;libs/gitbook-2.6.7/css/style.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;link href=&#34;libs/gitbook-2.6.7/css/plugin-table.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;link href=&#34;libs/gitbook-2.6.7/css/plugin-bookdown.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;link href=&#34;libs/gitbook-2.6.7/css/plugin-highlight.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;link href=&#34;libs/gitbook-2.6.7/css/plugin-search.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;link href=&#34;libs/gitbook-2.6.7/css/plugin-fontsettings.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;link href=&#34;libs/gitbook-2.6.7/css/plugin-clipboard.css&#34; rel=&#34;stylesheet&#34; /&gt;









&lt;script src=&#34;libs/kePrint-0.0.1/kePrint.js&#34;&gt;&lt;/script&gt;



&lt;link rel=&#34;stylesheet&#34; href=&#34;templates/style.css&#34; type=&#34;text/css&#34; /&gt;
&lt;/head&gt;

&lt;body&gt;



  &lt;div class=&#34;book without-animation with-summary font-size-2 font-family-1&#34; data-basepath=&#34;.&#34;&gt;

    &lt;div class=&#34;book-summary&#34;&gt;
      &lt;nav role=&#34;navigation&#34;&gt;

&lt;ul class=&#34;summary&#34;&gt;
&lt;li&gt;&lt;a href=&#34;introduction.html#introduction&#34;&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1&lt;/b&gt; &lt;strong&gt;Linguistic Complexity&lt;/strong&gt;&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.1&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subchap:categorizing&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.1&lt;/b&gt; Categorizing Linguistic Complexity Measures&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.2&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subchap:intrinsic&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.2&lt;/b&gt; Intrinsic Perspective&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.2.1&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subsubchap:structural&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.2.1&lt;/b&gt; Structural Linguistic Complexity&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.2.2&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subsubchap:lm-surprisal&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.2.2&lt;/b&gt; Language Modeling Surprisal&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.3&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subchap:extrinsic&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.3&lt;/b&gt; Extrinsic Perspective&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.3.1&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subsubchap:readability&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.3.1&lt;/b&gt; Automatic Readability Assessment&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.3.2&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subsubchap:pc&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.3.2&lt;/b&gt; Perceived Complexity Prediction&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.3.3&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subsubchap:eye-tracking&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.3.3&lt;/b&gt; Gaze Metrics Prediction&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.4&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subchap:garden-path&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.4&lt;/b&gt; Garden-path Sentences&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2&lt;/b&gt; &lt;strong&gt;Models of Linguistic Complexity&lt;/strong&gt;&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2.1&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html#subchap:desiderata&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2.1&lt;/b&gt; Desiderata for Models of Linguistic Complexity&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2.2&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html#subchap:nlm&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2.2&lt;/b&gt; Neural Language Models: Unsupervised Multitask Learners&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2.2.1&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html#subsubchap:syntax-nlm&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2.2.1&lt;/b&gt; Emergent Linguistic Structures in Neural Language Models&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2.3&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html#subchap:analyzing-nlm&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2.3&lt;/b&gt; Analyzing Neural Models of Complexity&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2.3.1&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html#subsubchap:probe&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2.3.1&lt;/b&gt; Probing classifiers&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2.3.2&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html#subsubchap:rsa&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2.3.2&lt;/b&gt; Representational Similarity Analysis&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2.3.3&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html#subsubchap:pwcca&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2.3.3&lt;/b&gt; Projection-Weighted Canonical Correlation Analysis&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3&lt;/b&gt; &lt;strong&gt;Complexity Phenomena in Linguistic Annotations and Language Models&lt;/strong&gt;&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3.1&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html#subchap:ex1-data&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3.1&lt;/b&gt; Data and Preprocessing&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3.2&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html#subchap:ex1-analysis&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3.2&lt;/b&gt; Analysis of Linguistic Phenomena&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3.2.1&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html#subsubchap:ex1-analysis-bins&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3.2.1&lt;/b&gt; Linguistic Phenomena in Length-controlled Bins&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3.3&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html#subchap:ex1-modeling&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3.3&lt;/b&gt; Modeling Online and Offline Linguistic Complexity&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3.3.1&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html#subsubchap:ex1-modeling-bins&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3.3.1&lt;/b&gt; Modeling Complexity in Length-controlled Bins&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3.4&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html#subchap:ex1-probing&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3.4&lt;/b&gt; Probing Linguistic Phenomena in ALBERT Representations&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3.5&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html#subchap:ex1-summary&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3.5&lt;/b&gt; Summary&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;4&#34; data-path=&#34;chap-ex2.html&#34;&gt;&lt;a href=&#34;chap-ex2.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;4&lt;/b&gt; &lt;strong&gt;Representational Similarity in Models of Complexity&lt;/strong&gt;&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;4.1&#34; data-path=&#34;chap-ex2.html&#34;&gt;&lt;a href=&#34;chap-ex2.html#knowledge-driven-requirements-for-learning-models&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;4.1&lt;/b&gt; Knowledge-driven Requirements for Learning Models&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;4.2&#34; data-path=&#34;chap-ex2.html&#34;&gt;&lt;a href=&#34;chap-ex2.html#subchap:ex2-experiments&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;4.2&lt;/b&gt; Experimentsl Evaluation&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;4.2.1&#34; data-path=&#34;chap-ex2.html&#34;&gt;&lt;a href=&#34;chap-ex2.html#subsubchap:ex2-data&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;4.2.1&lt;/b&gt; Data&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;4.2.2&#34; data-path=&#34;chap-ex2.html&#34;&gt;&lt;a href=&#34;chap-ex2.html#subsubchap:ex2-inter&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;4.2.2&lt;/b&gt; Inter-model Representational Similarity&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;4.2.3&#34; data-path=&#34;chap-ex2.html&#34;&gt;&lt;a href=&#34;chap-ex2.html#subsubchap:ex2-intra&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;4.2.3&lt;/b&gt; Intra-model Representational Similarity&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;4.3&#34; data-path=&#34;chap-ex2.html&#34;&gt;&lt;a href=&#34;chap-ex2.html#subchap:ex2-summary&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;4.3&lt;/b&gt; Summary&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;5&#34; data-path=&#34;chap-ex3.html&#34;&gt;&lt;a href=&#34;chap-ex3.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;5&lt;/b&gt; &lt;strong&gt;Gaze-informed Models for Cognitive Processing Prediction&lt;/strong&gt;&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;5.1&#34; data-path=&#34;chap-ex3.html&#34;&gt;&lt;a href=&#34;chap-ex3.html#subchap:ex3-setup&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;5.1&lt;/b&gt; Experimental Setup&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;5.2&#34; data-path=&#34;chap-ex3.html&#34;&gt;&lt;a href=&#34;chap-ex3.html#subchap:ex3-experiments&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;5.2&lt;/b&gt; Experimental Evaluation&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;5.2.1&#34; data-path=&#34;chap-ex3.html&#34;&gt;&lt;a href=&#34;chap-ex3.html#subsubchap:ex3-magnitudes&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;5.2.1&lt;/b&gt; Estimating Magnitudes of Garden-path Delays&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;5.2.2&#34; data-path=&#34;chap-ex3.html&#34;&gt;&lt;a href=&#34;chap-ex3.html#subsubchap:ex3-predicting&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;5.2.2&lt;/b&gt; Predicting Delays with Surprisal and Gaze Metrics&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;5.3&#34; data-path=&#34;chap-ex3.html&#34;&gt;&lt;a href=&#34;chap-ex3.html#subchap:ex3-summary&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;5.3&lt;/b&gt; Summary&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;conclusion.html#conclusion&#34;&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;&#34; data-path=&#34;conclusion.html&#34;&gt;&lt;a href=&#34;conclusion.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;Broader Impact and Ethical Perspectives&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;&#34; data-path=&#34;conclusion.html&#34;&gt;&lt;a href=&#34;conclusion.html#future-directions&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;Future Directions&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;appendix&#34;&gt;&lt;span&gt;&lt;b&gt;Appendix&lt;/b&gt;&lt;/span&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;A&#34; data-path=&#34;app-ling-feats.html&#34;&gt;&lt;a href=&#34;app-ling-feats.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;A&lt;/b&gt; Linguistic Features&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;A.1&#34; data-path=&#34;app-ling-feats.html&#34;&gt;&lt;a href=&#34;app-ling-feats.html#raw-text-properties-and-lexical-variety&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;A.1&lt;/b&gt; Raw Text Properties and Lexical Variety&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;A.2&#34; data-path=&#34;app-ling-feats.html&#34;&gt;&lt;a href=&#34;app-ling-feats.html#morpho-syntacting-information&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;A.2&lt;/b&gt; Morpho-syntacting Information&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;A.3&#34; data-path=&#34;app-ling-feats.html&#34;&gt;&lt;a href=&#34;app-ling-feats.html#verbal-predicate-structure&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;A.3&lt;/b&gt; Verbal Predicate Structure&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;A.4&#34; data-path=&#34;app-ling-feats.html&#34;&gt;&lt;a href=&#34;app-ling-feats.html#global-and-local-parsed-tree-structures&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;A.4&lt;/b&gt; Global and Local Parsed Tree Structures&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;A.5&#34; data-path=&#34;app-ling-feats.html&#34;&gt;&lt;a href=&#34;app-ling-feats.html#syntactic-relations&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;A.5&lt;/b&gt; Syntactic Relations&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;A.6&#34; data-path=&#34;app-ling-feats.html&#34;&gt;&lt;a href=&#34;app-ling-feats.html#subordination-phenomena&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;A.6&lt;/b&gt; Subordination Phenomena&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;B&#34; data-path=&#34;app-et-metrics.html&#34;&gt;&lt;a href=&#34;app-et-metrics.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;B&lt;/b&gt; Precisions on Eye-tracking Metrics and Preprocessing&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;C&#34; data-path=&#34;app-et-modeling.html&#34;&gt;&lt;a href=&#34;app-et-modeling.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;C&lt;/b&gt; Multi-task Token-level Regression for Gaze Metrics Prediction&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;D&#34; data-path=&#34;app-intra-sim.html&#34;&gt;&lt;a href=&#34;app-intra-sim.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;D&lt;/b&gt; Intra-model Similarity for All Models&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;E&#34; data-path=&#34;app-garden-paths-et.html&#34;&gt;&lt;a href=&#34;app-garden-paths-et.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;E&lt;/b&gt; Gaze Metrics Predictions for Garden Path Sentences&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;F&#34; data-path=&#34;app-params.html&#34;&gt;&lt;a href=&#34;app-params.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;F&lt;/b&gt; Reproducibility and Environmental Impact&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;&#34; data-path=&#34;references.html&#34;&gt;&lt;a href=&#34;references.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;References&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;divider&#34;&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://gsarti.com&#34;&gt;Back to my website&lt;/a&gt;&lt;/li&gt;

&lt;/ul&gt;

      &lt;/nav&gt;
    &lt;/div&gt;

    &lt;div class=&#34;book-body&#34;&gt;
      &lt;div class=&#34;body-inner&#34;&gt;
        &lt;div class=&#34;book-header&#34; role=&#34;navigation&#34;&gt;
          &lt;h1&gt;
            &lt;i class=&#34;fa fa-circle-o-notch fa-spin&#34;&gt;&lt;/i&gt;&lt;a href=&#34;./&#34;&gt;Interpreting Neural Language Models&lt;br /&gt;
for Linguistic Complexity Assessment&lt;/a&gt;
          &lt;/h1&gt;
        &lt;/div&gt;

        &lt;div class=&#34;page-wrapper&#34; tabindex=&#34;-1&#34; role=&#34;main&#34;&gt;
          &lt;div class=&#34;page-inner&#34;&gt;

            &lt;section class=&#34;normal&#34; id=&#34;section-&#34;&gt;
&lt;div id=&#34;app:intra-sim&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;D&lt;/span&gt; Intra-model Similarity for All Models&lt;/h1&gt;



&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:intra-pc&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;figures/appendix/A4_rsa_intra_cls_pc.png&#34; alt=&#34;Intra-model RSA (left) and PWCCA (right) scores across layers’ combinations for the ALBERT model fine-tuned on perceived complexity annotations (PC) using the [CLS] token (top), the all-token average (middle), and all tokens (bottom) representations. Layer -1 corresponds to the last layer before prediction heads.&#34; width=&#34;48%&#34; /&gt;&lt;img src=&#34;figures/appendix/A4_pwcca_intra_cls_pc.png&#34; alt=&#34;Intra-model RSA (left) and PWCCA (right) scores across layers’ combinations for the ALBERT model fine-tuned on perceived complexity annotations (PC) using the [CLS] token (top), the all-token average (middle), and all tokens (bottom) representations. Layer -1 corresponds to the last layer before prediction heads.&#34; width=&#34;48%&#34; /&gt;&lt;img src=&#34;figures/appendix/A4_rsa_intra_mean_pc.png&#34; alt=&#34;Intra-model RSA (left) and PWCCA (right) scores across layers’ combinations for the ALBERT model fine-tuned on perceived complexity annotations (PC) using the [CLS] token (top), the all-token average (middle), and all tokens (bottom) representations. Layer -1 corresponds to the last layer before prediction heads.&#34; width=&#34;48%&#34; /&gt;&lt;img src=&#34;figures/appendix/A4_pwcca_intra_mean_pc.png&#34; alt=&#34;Intra-model RSA (left) and PWCCA (right) scores across layers’ combinations for the ALBERT model fine-tuned on perceived complexity annotations (PC) using the [CLS] token (top), the all-token average (middle), and all tokens (bottom) representations. Layer -1 corresponds to the last layer before prediction heads.&#34; width=&#34;48%&#34; /&gt;&lt;img src=&#34;figures/appendix/A4_rsa_intra_tokens_pc.png&#34; alt=&#34;Intra-model RSA (left) and PWCCA (right) scores across layers’ combinations for the ALBERT model fine-tuned on perceived complexity annotations (PC) using the [CLS] token (top), the all-token average (middle), and all tokens (bottom) representations. Layer -1 corresponds to the last layer before prediction heads.&#34; width=&#34;48%&#34; /&gt;&lt;img src=&#34;figures/appendix/A4_pwcca_intra_tokens_pc.png&#34; alt=&#34;Intra-model RSA (left) and PWCCA (right) scores across layers’ combinations for the ALBERT model fine-tuned on perceived complexity annotations (PC) using the [CLS] token (top), the all-token average (middle), and all tokens (bottom) representations. Layer -1 corresponds to the last layer before prediction heads.&#34; width=&#34;48%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure D.1: Intra-model RSA (left) and PWCCA (right) scores across layers’ combinations for the ALBERT model fine-tuned on perceived complexity annotations (&lt;strong&gt;PC&lt;/strong&gt;) using the [CLS] token (top), the all-token average (middle), and all tokens (bottom) representations. Layer -1 corresponds to the last layer before prediction heads.
&lt;/p&gt;
&lt;/div&gt;


&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:intra-et&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;figures/appendix/A4_rsa_intra_cls_et.png&#34; alt=&#34;Intra-model RSA (left) and PWCCA (right) scores across layers’ combinations for the ALBERT model fine-tuned in parallel on gaze metrics (ET) using the [CLS] token (top), the all-token average (middle), and all tokens (bottom) representations. Layer -1 corresponds to the last layer before prediction heads.&#34; width=&#34;50%&#34; /&gt;&lt;img src=&#34;figures/appendix/A4_pwcca_intra_cls_et.png&#34; alt=&#34;Intra-model RSA (left) and PWCCA (right) scores across layers’ combinations for the ALBERT model fine-tuned in parallel on gaze metrics (ET) using the [CLS] token (top), the all-token average (middle), and all tokens (bottom) representations. Layer -1 corresponds to the last layer before prediction heads.&#34; width=&#34;50%&#34; /&gt;&lt;img src=&#34;figures/appendix/A4_rsa_intra_mean_et.png&#34; alt=&#34;Intra-model RSA (left) and PWCCA (right) scores across layers’ combinations for the ALBERT model fine-tuned in parallel on gaze metrics (ET) using the [CLS] token (top), the all-token average (middle), and all tokens (bottom) representations. Layer -1 corresponds to the last layer before prediction heads.&#34; width=&#34;50%&#34; /&gt;&lt;img src=&#34;figures/appendix/A4_pwcca_intra_mean_et.png&#34; alt=&#34;Intra-model RSA (left) and PWCCA (right) scores across layers’ combinations for the ALBERT model fine-tuned in parallel on gaze metrics (ET) using the [CLS] token (top), the all-token average (middle), and all tokens (bottom) representations. Layer -1 corresponds to the last layer before prediction heads.&#34; width=&#34;50%&#34; /&gt;&lt;img src=&#34;figures/appendix/A4_rsa_intra_tokens_et.png&#34; alt=&#34;Intra-model RSA (left) and PWCCA (right) scores across layers’ combinations for the ALBERT model fine-tuned in parallel on gaze metrics (ET) using the [CLS] token (top), the all-token average (middle), and all tokens (bottom) representations. Layer -1 corresponds to the last layer before prediction heads.&#34; width=&#34;50%&#34; /&gt;&lt;img src=&#34;figures/appendix/A4_pwcca_intra_tokens_et.png&#34; alt=&#34;Intra-model RSA (left) and PWCCA (right) scores across layers’ combinations for the ALBERT model fine-tuned in parallel on gaze metrics (ET) using the [CLS] token (top), the all-token average (middle), and all tokens (bottom) representations. Layer -1 corresponds to the last layer before prediction heads.&#34; width=&#34;50%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure D.2: Intra-model RSA (left) and PWCCA (right) scores across layers’ combinations for the ALBERT model fine-tuned in parallel on gaze metrics (&lt;strong&gt;ET&lt;/strong&gt;) using the [CLS] token (top), the all-token average (middle), and all tokens (bottom) representations. Layer -1 corresponds to the last layer before prediction heads.
&lt;/p&gt;
&lt;/div&gt;


&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:intra-ra&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;figures/appendix/A4_rsa_intra_cls_ra.png&#34; alt=&#34;Intra-model RSA (left) and PWCCA (right) scores across layers’ combinations for the ALBERT model fine-tuned on readability assessment annotations (RA) using the [CLS] token (top), the all-token average (middle), and all tokens (bottom) representations. Layer -1 corresponds to the last layer before prediction heads.&#34; width=&#34;50%&#34; /&gt;&lt;img src=&#34;figures/appendix/A4_pwcca_intra_cls_ra.png&#34; alt=&#34;Intra-model RSA (left) and PWCCA (right) scores across layers’ combinations for the ALBERT model fine-tuned on readability assessment annotations (RA) using the [CLS] token (top), the all-token average (middle), and all tokens (bottom) representations. Layer -1 corresponds to the last layer before prediction heads.&#34; width=&#34;50%&#34; /&gt;&lt;img src=&#34;figures/appendix/A4_rsa_intra_mean_ra.png&#34; alt=&#34;Intra-model RSA (left) and PWCCA (right) scores across layers’ combinations for the ALBERT model fine-tuned on readability assessment annotations (RA) using the [CLS] token (top), the all-token average (middle), and all tokens (bottom) representations. Layer -1 corresponds to the last layer before prediction heads.&#34; width=&#34;50%&#34; /&gt;&lt;img src=&#34;figures/appendix/A4_pwcca_intra_mean_ra.png&#34; alt=&#34;Intra-model RSA (left) and PWCCA (right) scores across layers’ combinations for the ALBERT model fine-tuned on readability assessment annotations (RA) using the [CLS] token (top), the all-token average (middle), and all tokens (bottom) representations. Layer -1 corresponds to the last layer before prediction heads.&#34; width=&#34;50%&#34; /&gt;&lt;img src=&#34;figures/appendix/A4_rsa_intra_tokens_ra.png&#34; alt=&#34;Intra-model RSA (left) and PWCCA (right) scores across layers’ combinations for the ALBERT model fine-tuned on readability assessment annotations (RA) using the [CLS] token (top), the all-token average (middle), and all tokens (bottom) representations. Layer -1 corresponds to the last layer before prediction heads.&#34; width=&#34;50%&#34; /&gt;&lt;img src=&#34;figures/appendix/A4_pwcca_intra_tokens_ra.png&#34; alt=&#34;Intra-model RSA (left) and PWCCA (right) scores across layers’ combinations for the ALBERT model fine-tuned on readability assessment annotations (RA) using the [CLS] token (top), the all-token average (middle), and all tokens (bottom) representations. Layer -1 corresponds to the last layer before prediction heads.&#34; width=&#34;50%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure D.3: Intra-model RSA (left) and PWCCA (right) scores across layers’ combinations for the ALBERT model fine-tuned on readability assessment annotations (&lt;strong&gt;RA&lt;/strong&gt;) using the [CLS] token (top), the all-token average (middle), and all tokens (bottom) representations. Layer -1 corresponds to the last layer before prediction heads.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
            &lt;/section&gt;

          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
&lt;a href=&#34;app-et-modeling.html&#34; class=&#34;navigation navigation-prev &#34; aria-label=&#34;Previous page&#34;&gt;&lt;i class=&#34;fa fa-angle-left&#34;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&#34;app-garden-paths-et.html&#34; class=&#34;navigation navigation-next &#34; aria-label=&#34;Next page&#34;&gt;&lt;i class=&#34;fa fa-angle-right&#34;&gt;&lt;/i&gt;&lt;/a&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/app.min.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/lunr.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/clipboard.min.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/plugin-search.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/plugin-sharing.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/plugin-fontsettings.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/plugin-bookdown.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/jquery.highlight.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/plugin-clipboard.js&#34;&gt;&lt;/script&gt;
&lt;script&gt;
gitbook.require([&#34;gitbook&#34;], function(gitbook) {
gitbook.start({
&#34;sharing&#34;: {
&#34;github&#34;: true,
&#34;facebook&#34;: true,
&#34;twitter&#34;: true,
&#34;linkedin&#34;: true,
&#34;weibo&#34;: false,
&#34;instapaper&#34;: false,
&#34;vk&#34;: false,
&#34;all&#34;: false
},
&#34;fontsettings&#34;: {
&#34;theme&#34;: &#34;white&#34;,
&#34;family&#34;: &#34;sans&#34;,
&#34;size&#34;: 2
},
&#34;edit&#34;: {
&#34;link&#34;: &#34;https://github.com/gsarti/master-thesis/tree/master/extra/Appendix.Rmd&#34;,
&#34;text&#34;: &#34;Edit&#34;
},
&#34;history&#34;: {
&#34;link&#34;: null,
&#34;text&#34;: null
},
&#34;view&#34;: {
&#34;link&#34;: null,
&#34;text&#34;: null
},
&#34;download&#34;: [[&#34;Sarti_2020_Interpreting_NLMs_for_LCA.pdf&#34;, &#34;PDF&#34;]],
&#34;toc&#34;: {
&#34;collapse&#34;: &#34;subsection&#34;,
&#34;scroll_highlight&#34;: true
},
&#34;info&#34;: false
});
});
&lt;/script&gt;

&lt;!-- dynamically load mathjax for compatibility with self-contained --&gt;
&lt;script&gt;
  (function () {
    var script = document.createElement(&#34;script&#34;);
    script.type = &#34;text/javascript&#34;;
    var src = &#34;true&#34;;
    if (src === &#34;&#34; || src === &#34;true&#34;) src = &#34;https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML&#34;;
    if (location.protocol !== &#34;file:&#34;)
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, &#39;&#39;);
    script.src = src;
    document.getElementsByTagName(&#34;head&#34;)[0].appendChild(script);
  })();
&lt;/script&gt;
&lt;/body&gt;

&lt;/html&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>http://localhost:1313/msc-thesis/app-ling-feats/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/msc-thesis/app-ling-feats/</guid>
      <description>&lt;!DOCTYPE html&gt;
&lt;html lang=&#34;&#34; xml:lang=&#34;&#34;&gt;
&lt;head&gt;

  &lt;meta charset=&#34;utf-8&#34; /&gt;
  &lt;meta http-equiv=&#34;X-UA-Compatible&#34; content=&#34;IE=edge&#34; /&gt;
  &lt;title&gt;A Linguistic Features | Interpreting Neural Language Models for Linguistic Complexity Assessment&lt;/title&gt;
  &lt;meta name=&#34;description&#34; content=&#34;MSc Thesis at the University of Trieste and SISSA under the supervision of Prof. Davide Crepaldi and Dott. Felice dell&#39;Orletta&#34; /&gt;
  &lt;meta name=&#34;generator&#34; content=&#34;bookdown 0.20.6 and GitBook 2.6.7&#34; /&gt;

  &lt;meta property=&#34;og:title&#34; content=&#34;A Linguistic Features | Interpreting Neural Language Models for Linguistic Complexity Assessment&#34; /&gt;
  &lt;meta property=&#34;og:type&#34; content=&#34;book&#34; /&gt;
  &lt;meta property=&#34;og:url&#34; content=&#34;https://gsarti.com/master-thesis&#34; /&gt;
  &lt;meta property=&#34;og:image&#34; content=&#34;https://gsarti.com/master-thesisfigures/cover.png&#34; /&gt;
  &lt;meta property=&#34;og:description&#34; content=&#34;MSc Thesis at the University of Trieste and SISSA under the supervision of Prof. Davide Crepaldi and Dott. Felice dell&#39;Orletta&#34; /&gt;
  &lt;meta name=&#34;github-repo&#34; content=&#34;gsarti/interpreting-complexity&#34; /&gt;

  &lt;meta name=&#34;twitter:card&#34; content=&#34;summary&#34; /&gt;
  &lt;meta name=&#34;twitter:title&#34; content=&#34;A Linguistic Features | Interpreting Neural Language Models for Linguistic Complexity Assessment&#34; /&gt;
  
  &lt;meta name=&#34;twitter:description&#34; content=&#34;MSc Thesis at the University of Trieste and SISSA under the supervision of Prof. Davide Crepaldi and Dott. Felice dell&#39;Orletta&#34; /&gt;
  &lt;meta name=&#34;twitter:image&#34; content=&#34;https://gsarti.com/master-thesisfigures/cover.png&#34; /&gt;

&lt;meta name=&#34;author&#34; content=&#34;Gabriele Sarti&#34; /&gt;



  &lt;meta name=&#34;viewport&#34; content=&#34;width=device-width, initial-scale=1&#34; /&gt;
  &lt;meta name=&#34;apple-mobile-web-app-capable&#34; content=&#34;yes&#34; /&gt;
  &lt;meta name=&#34;apple-mobile-web-app-status-bar-style&#34; content=&#34;black&#34; /&gt;
  &lt;link rel=&#34;apple-touch-icon-precomposed&#34; sizes=&#34;152x152&#34; href=&#34;figures/icons/apple-icon.png&#34; /&gt;
  &lt;link rel=&#34;shortcut icon&#34; href=&#34;figures/icons/favicon.ico&#34; type=&#34;image/x-icon&#34; /&gt;
&lt;link rel=&#34;prev&#34; href=&#34;conclusion.html&#34;/&gt;
&lt;link rel=&#34;next&#34; href=&#34;app-et-metrics.html&#34;/&gt;
&lt;style type=&#34;text/css&#34;&gt;
p.abstract{
  text-align: center;
  font-weight: bold;
}
div.abstract{
  margin: auto;
  width: 90%;
}
&lt;/style&gt;
&lt;script src=&#34;libs/jquery-2.2.3/jquery.min.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;libs/gitbook-2.6.7/css/style.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;link href=&#34;libs/gitbook-2.6.7/css/plugin-table.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;link href=&#34;libs/gitbook-2.6.7/css/plugin-bookdown.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;link href=&#34;libs/gitbook-2.6.7/css/plugin-highlight.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;link href=&#34;libs/gitbook-2.6.7/css/plugin-search.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;link href=&#34;libs/gitbook-2.6.7/css/plugin-fontsettings.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;link href=&#34;libs/gitbook-2.6.7/css/plugin-clipboard.css&#34; rel=&#34;stylesheet&#34; /&gt;









&lt;script src=&#34;libs/kePrint-0.0.1/kePrint.js&#34;&gt;&lt;/script&gt;



&lt;link rel=&#34;stylesheet&#34; href=&#34;templates/style.css&#34; type=&#34;text/css&#34; /&gt;
&lt;/head&gt;

&lt;body&gt;



  &lt;div class=&#34;book without-animation with-summary font-size-2 font-family-1&#34; data-basepath=&#34;.&#34;&gt;

    &lt;div class=&#34;book-summary&#34;&gt;
      &lt;nav role=&#34;navigation&#34;&gt;

&lt;ul class=&#34;summary&#34;&gt;
&lt;li&gt;&lt;a href=&#34;introduction.html#introduction&#34;&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1&lt;/b&gt; &lt;strong&gt;Linguistic Complexity&lt;/strong&gt;&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.1&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subchap:categorizing&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.1&lt;/b&gt; Categorizing Linguistic Complexity Measures&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.2&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subchap:intrinsic&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.2&lt;/b&gt; Intrinsic Perspective&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.2.1&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subsubchap:structural&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.2.1&lt;/b&gt; Structural Linguistic Complexity&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.2.2&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subsubchap:lm-surprisal&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.2.2&lt;/b&gt; Language Modeling Surprisal&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.3&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subchap:extrinsic&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.3&lt;/b&gt; Extrinsic Perspective&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.3.1&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subsubchap:readability&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.3.1&lt;/b&gt; Automatic Readability Assessment&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.3.2&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subsubchap:pc&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.3.2&lt;/b&gt; Perceived Complexity Prediction&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.3.3&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subsubchap:eye-tracking&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.3.3&lt;/b&gt; Gaze Metrics Prediction&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.4&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subchap:garden-path&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.4&lt;/b&gt; Garden-path Sentences&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2&lt;/b&gt; &lt;strong&gt;Models of Linguistic Complexity&lt;/strong&gt;&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2.1&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html#subchap:desiderata&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2.1&lt;/b&gt; Desiderata for Models of Linguistic Complexity&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2.2&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html#subchap:nlm&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2.2&lt;/b&gt; Neural Language Models: Unsupervised Multitask Learners&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2.2.1&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html#subsubchap:syntax-nlm&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2.2.1&lt;/b&gt; Emergent Linguistic Structures in Neural Language Models&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2.3&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html#subchap:analyzing-nlm&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2.3&lt;/b&gt; Analyzing Neural Models of Complexity&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2.3.1&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html#subsubchap:probe&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2.3.1&lt;/b&gt; Probing classifiers&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2.3.2&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html#subsubchap:rsa&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2.3.2&lt;/b&gt; Representational Similarity Analysis&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2.3.3&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html#subsubchap:pwcca&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2.3.3&lt;/b&gt; Projection-Weighted Canonical Correlation Analysis&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3&lt;/b&gt; &lt;strong&gt;Complexity Phenomena in Linguistic Annotations and Language Models&lt;/strong&gt;&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3.1&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html#subchap:ex1-data&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3.1&lt;/b&gt; Data and Preprocessing&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3.2&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html#subchap:ex1-analysis&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3.2&lt;/b&gt; Analysis of Linguistic Phenomena&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3.2.1&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html#subsubchap:ex1-analysis-bins&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3.2.1&lt;/b&gt; Linguistic Phenomena in Length-controlled Bins&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3.3&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html#subchap:ex1-modeling&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3.3&lt;/b&gt; Modeling Online and Offline Linguistic Complexity&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3.3.1&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html#subsubchap:ex1-modeling-bins&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3.3.1&lt;/b&gt; Modeling Complexity in Length-controlled Bins&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3.4&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html#subchap:ex1-probing&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3.4&lt;/b&gt; Probing Linguistic Phenomena in ALBERT Representations&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3.5&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html#subchap:ex1-summary&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3.5&lt;/b&gt; Summary&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;4&#34; data-path=&#34;chap-ex2.html&#34;&gt;&lt;a href=&#34;chap-ex2.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;4&lt;/b&gt; &lt;strong&gt;Representational Similarity in Models of Complexity&lt;/strong&gt;&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;4.1&#34; data-path=&#34;chap-ex2.html&#34;&gt;&lt;a href=&#34;chap-ex2.html#knowledge-driven-requirements-for-learning-models&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;4.1&lt;/b&gt; Knowledge-driven Requirements for Learning Models&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;4.2&#34; data-path=&#34;chap-ex2.html&#34;&gt;&lt;a href=&#34;chap-ex2.html#subchap:ex2-experiments&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;4.2&lt;/b&gt; Experimentsl Evaluation&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;4.2.1&#34; data-path=&#34;chap-ex2.html&#34;&gt;&lt;a href=&#34;chap-ex2.html#subsubchap:ex2-data&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;4.2.1&lt;/b&gt; Data&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;4.2.2&#34; data-path=&#34;chap-ex2.html&#34;&gt;&lt;a href=&#34;chap-ex2.html#subsubchap:ex2-inter&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;4.2.2&lt;/b&gt; Inter-model Representational Similarity&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;4.2.3&#34; data-path=&#34;chap-ex2.html&#34;&gt;&lt;a href=&#34;chap-ex2.html#subsubchap:ex2-intra&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;4.2.3&lt;/b&gt; Intra-model Representational Similarity&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;4.3&#34; data-path=&#34;chap-ex2.html&#34;&gt;&lt;a href=&#34;chap-ex2.html#subchap:ex2-summary&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;4.3&lt;/b&gt; Summary&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;5&#34; data-path=&#34;chap-ex3.html&#34;&gt;&lt;a href=&#34;chap-ex3.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;5&lt;/b&gt; &lt;strong&gt;Gaze-informed Models for Cognitive Processing Prediction&lt;/strong&gt;&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;5.1&#34; data-path=&#34;chap-ex3.html&#34;&gt;&lt;a href=&#34;chap-ex3.html#subchap:ex3-setup&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;5.1&lt;/b&gt; Experimental Setup&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;5.2&#34; data-path=&#34;chap-ex3.html&#34;&gt;&lt;a href=&#34;chap-ex3.html#subchap:ex3-experiments&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;5.2&lt;/b&gt; Experimental Evaluation&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;5.2.1&#34; data-path=&#34;chap-ex3.html&#34;&gt;&lt;a href=&#34;chap-ex3.html#subsubchap:ex3-magnitudes&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;5.2.1&lt;/b&gt; Estimating Magnitudes of Garden-path Delays&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;5.2.2&#34; data-path=&#34;chap-ex3.html&#34;&gt;&lt;a href=&#34;chap-ex3.html#subsubchap:ex3-predicting&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;5.2.2&lt;/b&gt; Predicting Delays with Surprisal and Gaze Metrics&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;5.3&#34; data-path=&#34;chap-ex3.html&#34;&gt;&lt;a href=&#34;chap-ex3.html#subchap:ex3-summary&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;5.3&lt;/b&gt; Summary&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;conclusion.html#conclusion&#34;&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;&#34; data-path=&#34;conclusion.html&#34;&gt;&lt;a href=&#34;conclusion.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;Broader Impact and Ethical Perspectives&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;&#34; data-path=&#34;conclusion.html&#34;&gt;&lt;a href=&#34;conclusion.html#future-directions&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;Future Directions&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;appendix&#34;&gt;&lt;span&gt;&lt;b&gt;Appendix&lt;/b&gt;&lt;/span&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;A&#34; data-path=&#34;app-ling-feats.html&#34;&gt;&lt;a href=&#34;app-ling-feats.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;A&lt;/b&gt; Linguistic Features&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;A.1&#34; data-path=&#34;app-ling-feats.html&#34;&gt;&lt;a href=&#34;app-ling-feats.html#raw-text-properties-and-lexical-variety&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;A.1&lt;/b&gt; Raw Text Properties and Lexical Variety&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;A.2&#34; data-path=&#34;app-ling-feats.html&#34;&gt;&lt;a href=&#34;app-ling-feats.html#morpho-syntacting-information&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;A.2&lt;/b&gt; Morpho-syntacting Information&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;A.3&#34; data-path=&#34;app-ling-feats.html&#34;&gt;&lt;a href=&#34;app-ling-feats.html#verbal-predicate-structure&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;A.3&lt;/b&gt; Verbal Predicate Structure&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;A.4&#34; data-path=&#34;app-ling-feats.html&#34;&gt;&lt;a href=&#34;app-ling-feats.html#global-and-local-parsed-tree-structures&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;A.4&lt;/b&gt; Global and Local Parsed Tree Structures&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;A.5&#34; data-path=&#34;app-ling-feats.html&#34;&gt;&lt;a href=&#34;app-ling-feats.html#syntactic-relations&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;A.5&lt;/b&gt; Syntactic Relations&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;A.6&#34; data-path=&#34;app-ling-feats.html&#34;&gt;&lt;a href=&#34;app-ling-feats.html#subordination-phenomena&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;A.6&lt;/b&gt; Subordination Phenomena&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;B&#34; data-path=&#34;app-et-metrics.html&#34;&gt;&lt;a href=&#34;app-et-metrics.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;B&lt;/b&gt; Precisions on Eye-tracking Metrics and Preprocessing&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;C&#34; data-path=&#34;app-et-modeling.html&#34;&gt;&lt;a href=&#34;app-et-modeling.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;C&lt;/b&gt; Multi-task Token-level Regression for Gaze Metrics Prediction&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;D&#34; data-path=&#34;app-intra-sim.html&#34;&gt;&lt;a href=&#34;app-intra-sim.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;D&lt;/b&gt; Intra-model Similarity for All Models&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;E&#34; data-path=&#34;app-garden-paths-et.html&#34;&gt;&lt;a href=&#34;app-garden-paths-et.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;E&lt;/b&gt; Gaze Metrics Predictions for Garden Path Sentences&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;F&#34; data-path=&#34;app-params.html&#34;&gt;&lt;a href=&#34;app-params.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;F&lt;/b&gt; Reproducibility and Environmental Impact&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;&#34; data-path=&#34;references.html&#34;&gt;&lt;a href=&#34;references.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;References&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;divider&#34;&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://gsarti.com&#34;&gt;Back to my website&lt;/a&gt;&lt;/li&gt;

&lt;/ul&gt;

      &lt;/nav&gt;
    &lt;/div&gt;

    &lt;div class=&#34;book-body&#34;&gt;
      &lt;div class=&#34;body-inner&#34;&gt;
        &lt;div class=&#34;book-header&#34; role=&#34;navigation&#34;&gt;
          &lt;h1&gt;
            &lt;i class=&#34;fa fa-circle-o-notch fa-spin&#34;&gt;&lt;/i&gt;&lt;a href=&#34;./&#34;&gt;Interpreting Neural Language Models&lt;br /&gt;
for Linguistic Complexity Assessment&lt;/a&gt;
          &lt;/h1&gt;
        &lt;/div&gt;

        &lt;div class=&#34;page-wrapper&#34; tabindex=&#34;-1&#34; role=&#34;main&#34;&gt;
          &lt;div class=&#34;page-inner&#34;&gt;

            &lt;section class=&#34;normal&#34; id=&#34;section-&#34;&gt;
&lt;div id=&#34;app:ling-feats&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;A&lt;/span&gt; Linguistic Features&lt;/h1&gt;
&lt;p&gt;The following list of features was used in the context of Chapter &lt;a href=&#34;chap-ex1.html#chap:ex1&#34;&gt;3&lt;/a&gt; experiments and is a summary of the full set of features presented in &lt;span class=&#34;citation&#34;&gt;Brunato et al. (&lt;a href=&#34;#ref-brunato-etal-2020-profiling&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt;:&lt;/p&gt;
&lt;div id=&#34;raw-text-properties-and-lexical-variety&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;A.1&lt;/span&gt; Raw Text Properties and Lexical Variety&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Sentence length&lt;/strong&gt; (&lt;em&gt;n_tokens&lt;/em&gt;): Length of the sentence in terms of number of tokens.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Word length&lt;/strong&gt; (&lt;em&gt;char_per_tok&lt;/em&gt;): Average number of characters per word in a sentence, excluding punctuation.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Type/Token Ratio for forms and lemmas&lt;/strong&gt; (&lt;em&gt;ttr_form, ttr_lemma&lt;/em&gt;): Ratio between the number of lexical types and the number of tokens within a sentence.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;morpho-syntacting-information&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;A.2&lt;/span&gt; Morpho-syntacting Information&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Distribution of grammatical categories&lt;/strong&gt; (&lt;em&gt;upos_dist_*, xpos_dist_*&lt;/em&gt;): Percentage distribution in the sentence of the 17 core part-of-speech categories present in the Universal POS tagset (adjective, adverb, interjection, noun, proper noun, verb, adposition, auxiliary, coordinating conjunction, determiner, numeral, particle, pronoun and subordinating conjunction, punctuation, and symbols).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Lexical density&lt;/strong&gt; (&lt;em&gt;lexical_density&lt;/em&gt;): Ratio of content words (verbs, nouns, adjectives, and adverbs) over the total number of words in a sentence.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Inflectional morphology&lt;/strong&gt; (&lt;em&gt;aux_mood_*, aux_tense_*): Percentage distribution in the sentence of a set of inflectional features (&lt;/em&gt;Mood, Number, Person, Tense and Verbal Form*) over lexical verbs and auxiliaries of each sentence.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;verbal-predicate-structure&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;A.3&lt;/span&gt; Verbal Predicate Structure&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Distribution of verbal heads&lt;/strong&gt; (&lt;em&gt;vb_head_per_sent&lt;/em&gt;): Number of verbal heads in the sentence, corresponding to the number of main or subordinate clauses co-occurring in it.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Distribution of verbal roots&lt;/strong&gt; (&lt;em&gt;dep_dist_root&lt;/em&gt;): Percentage of verbal roots out of the total sentence roots.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Verb arity&lt;/strong&gt; (&lt;em&gt;verb_arity&lt;/em&gt;): Average number of dependency links sharing the same verbal head per sentence, excluding punctuation and copula dependencies.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;global-and-local-parsed-tree-structures&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;A.4&lt;/span&gt; Global and Local Parsed Tree Structures&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Syntactic tree depth&lt;/strong&gt; (&lt;em&gt;parse_depth&lt;/em&gt;): Maximum syntactic tree depth extracted for the sentence, i.e., the longest path in terms of dependency links from the root of the dependency tree to some leaf.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Average and maximum length of dependency links&lt;/strong&gt; (&lt;em&gt;avg_links_len, max_links_len&lt;/em&gt;)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Number and average length of prepositional chains&lt;/strong&gt; (&lt;em&gt;n_prep_chains, prep_chain_len&lt;/em&gt;), with the latter expressed in number of tokens.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Subject-object ordering&lt;/strong&gt; (&lt;em&gt;subj_pre, subj_post, obj_pre, obj_post&lt;/em&gt;): Relative order of the subject and object arguments with respect to the verbal root of the clause in the sentence.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;syntactic-relations&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;A.5&lt;/span&gt; Syntactic Relations&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Distribution of dependency relations&lt;/strong&gt; (&lt;em&gt;dep_dist_*&lt;/em&gt;): Percentage distribution of the 37 universal relations in the UD dependency annotation scheme.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;subordination-phenomena&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;A.6&lt;/span&gt; Subordination Phenomena&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Distribution of main and subordinate clauses&lt;/strong&gt; (&lt;em&gt;princ_prop_dist, sub_prop_dist&lt;/em&gt;): Percentage distribution of main vs subordinate clauses in the sentence.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Relative ordering of subordinates&lt;/strong&gt; (&lt;em&gt;sub_pre, sub_post&lt;/em&gt;): As for subjects and objects, whether the subordinate occurs in pre-verbal or post-verbal position in the sentence.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Average length of embedded subordinates&lt;/strong&gt; (&lt;em&gt;sub_chain_len&lt;/em&gt;): Average length of subordinate clauses recursively embedded into each other to form a subordinate chain.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Readers are referred to the original paper by &lt;span class=&#34;citation&#34;&gt;Brunato et al. (&lt;a href=&#34;#ref-brunato-etal-2020-profiling&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt; and the Profiling-UD webpage&lt;a href=&#34;#fn24&#34; class=&#34;footnote-ref&#34; id=&#34;fnref24&#34;&gt;&lt;sup&gt;24&lt;/sup&gt;&lt;/a&gt; for additional details on linguistic features.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;h3&gt;References&lt;/h3&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-brunato-etal-2020-profiling&#34;&gt;
&lt;p&gt;Brunato, Dominique, Andrea Cimino, Felice Dell’Orletta, Giulia Venturi, and Simonetta Montemagni. 2020. “Profiling-UD: A Tool for Linguistic Profiling of Texts.” In &lt;em&gt;Proceedings of the 12th Language Resources and Evaluation Conference&lt;/em&gt;, 7145–51. Marseille, France: European Language Resources Association. &lt;a href=&#34;https://www.aclweb.org/anthology/2020.lrec-1.883&#34;&gt;https://www.aclweb.org/anthology/2020.lrec-1.883&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol start=&#34;24&#34;&gt;
&lt;li id=&#34;fn24&#34;&gt;&lt;p&gt;&lt;a href=&#34;http://linguistic-%20profiling.italianlp.it&#34;&gt;http://linguistic-profiling.italianlp.it&lt;/a&gt;&lt;a href=&#34;app-ling-feats.html#fnref24&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
            &lt;/section&gt;

          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
&lt;a href=&#34;conclusion.html&#34; class=&#34;navigation navigation-prev &#34; aria-label=&#34;Previous page&#34;&gt;&lt;i class=&#34;fa fa-angle-left&#34;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&#34;app-et-metrics.html&#34; class=&#34;navigation navigation-next &#34; aria-label=&#34;Next page&#34;&gt;&lt;i class=&#34;fa fa-angle-right&#34;&gt;&lt;/i&gt;&lt;/a&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/app.min.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/lunr.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/clipboard.min.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/plugin-search.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/plugin-sharing.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/plugin-fontsettings.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/plugin-bookdown.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/jquery.highlight.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/plugin-clipboard.js&#34;&gt;&lt;/script&gt;
&lt;script&gt;
gitbook.require([&#34;gitbook&#34;], function(gitbook) {
gitbook.start({
&#34;sharing&#34;: {
&#34;github&#34;: true,
&#34;facebook&#34;: true,
&#34;twitter&#34;: true,
&#34;linkedin&#34;: true,
&#34;weibo&#34;: false,
&#34;instapaper&#34;: false,
&#34;vk&#34;: false,
&#34;all&#34;: false
},
&#34;fontsettings&#34;: {
&#34;theme&#34;: &#34;white&#34;,
&#34;family&#34;: &#34;sans&#34;,
&#34;size&#34;: 2
},
&#34;edit&#34;: {
&#34;link&#34;: &#34;https://github.com/gsarti/master-thesis/tree/master/extra/Appendix.Rmd&#34;,
&#34;text&#34;: &#34;Edit&#34;
},
&#34;history&#34;: {
&#34;link&#34;: null,
&#34;text&#34;: null
},
&#34;view&#34;: {
&#34;link&#34;: null,
&#34;text&#34;: null
},
&#34;download&#34;: [[&#34;Sarti_2020_Interpreting_NLMs_for_LCA.pdf&#34;, &#34;PDF&#34;]],
&#34;toc&#34;: {
&#34;collapse&#34;: &#34;subsection&#34;,
&#34;scroll_highlight&#34;: true
},
&#34;info&#34;: false
});
});
&lt;/script&gt;

&lt;!-- dynamically load mathjax for compatibility with self-contained --&gt;
&lt;script&gt;
  (function () {
    var script = document.createElement(&#34;script&#34;);
    script.type = &#34;text/javascript&#34;;
    var src = &#34;true&#34;;
    if (src === &#34;&#34; || src === &#34;true&#34;) src = &#34;https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML&#34;;
    if (location.protocol !== &#34;file:&#34;)
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, &#39;&#39;);
    script.src = src;
    document.getElementsByTagName(&#34;head&#34;)[0].appendChild(script);
  })();
&lt;/script&gt;
&lt;/body&gt;

&lt;/html&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>http://localhost:1313/msc-thesis/app-params/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/msc-thesis/app-params/</guid>
      <description>&lt;!DOCTYPE html&gt;
&lt;html lang=&#34;&#34; xml:lang=&#34;&#34;&gt;
&lt;head&gt;

  &lt;meta charset=&#34;utf-8&#34; /&gt;
  &lt;meta http-equiv=&#34;X-UA-Compatible&#34; content=&#34;IE=edge&#34; /&gt;
  &lt;title&gt;F Reproducibility and Environmental Impact | Interpreting Neural Language Models for Linguistic Complexity Assessment&lt;/title&gt;
  &lt;meta name=&#34;description&#34; content=&#34;MSc Thesis at the University of Trieste and SISSA under the supervision of Prof. Davide Crepaldi and Dott. Felice dell&#39;Orletta&#34; /&gt;
  &lt;meta name=&#34;generator&#34; content=&#34;bookdown 0.20.6 and GitBook 2.6.7&#34; /&gt;

  &lt;meta property=&#34;og:title&#34; content=&#34;F Reproducibility and Environmental Impact | Interpreting Neural Language Models for Linguistic Complexity Assessment&#34; /&gt;
  &lt;meta property=&#34;og:type&#34; content=&#34;book&#34; /&gt;
  &lt;meta property=&#34;og:url&#34; content=&#34;https://gsarti.com/master-thesis&#34; /&gt;
  &lt;meta property=&#34;og:image&#34; content=&#34;https://gsarti.com/master-thesisfigures/cover.png&#34; /&gt;
  &lt;meta property=&#34;og:description&#34; content=&#34;MSc Thesis at the University of Trieste and SISSA under the supervision of Prof. Davide Crepaldi and Dott. Felice dell&#39;Orletta&#34; /&gt;
  &lt;meta name=&#34;github-repo&#34; content=&#34;gsarti/interpreting-complexity&#34; /&gt;

  &lt;meta name=&#34;twitter:card&#34; content=&#34;summary&#34; /&gt;
  &lt;meta name=&#34;twitter:title&#34; content=&#34;F Reproducibility and Environmental Impact | Interpreting Neural Language Models for Linguistic Complexity Assessment&#34; /&gt;
  
  &lt;meta name=&#34;twitter:description&#34; content=&#34;MSc Thesis at the University of Trieste and SISSA under the supervision of Prof. Davide Crepaldi and Dott. Felice dell&#39;Orletta&#34; /&gt;
  &lt;meta name=&#34;twitter:image&#34; content=&#34;https://gsarti.com/master-thesisfigures/cover.png&#34; /&gt;

&lt;meta name=&#34;author&#34; content=&#34;Gabriele Sarti&#34; /&gt;



  &lt;meta name=&#34;viewport&#34; content=&#34;width=device-width, initial-scale=1&#34; /&gt;
  &lt;meta name=&#34;apple-mobile-web-app-capable&#34; content=&#34;yes&#34; /&gt;
  &lt;meta name=&#34;apple-mobile-web-app-status-bar-style&#34; content=&#34;black&#34; /&gt;
  &lt;link rel=&#34;apple-touch-icon-precomposed&#34; sizes=&#34;152x152&#34; href=&#34;figures/icons/apple-icon.png&#34; /&gt;
  &lt;link rel=&#34;shortcut icon&#34; href=&#34;figures/icons/favicon.ico&#34; type=&#34;image/x-icon&#34; /&gt;
&lt;link rel=&#34;prev&#34; href=&#34;app-garden-paths-et.html&#34;/&gt;
&lt;link rel=&#34;next&#34; href=&#34;references.html&#34;/&gt;
&lt;style type=&#34;text/css&#34;&gt;
p.abstract{
  text-align: center;
  font-weight: bold;
}
div.abstract{
  margin: auto;
  width: 90%;
}
&lt;/style&gt;
&lt;script src=&#34;libs/jquery-2.2.3/jquery.min.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;libs/gitbook-2.6.7/css/style.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;link href=&#34;libs/gitbook-2.6.7/css/plugin-table.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;link href=&#34;libs/gitbook-2.6.7/css/plugin-bookdown.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;link href=&#34;libs/gitbook-2.6.7/css/plugin-highlight.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;link href=&#34;libs/gitbook-2.6.7/css/plugin-search.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;link href=&#34;libs/gitbook-2.6.7/css/plugin-fontsettings.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;link href=&#34;libs/gitbook-2.6.7/css/plugin-clipboard.css&#34; rel=&#34;stylesheet&#34; /&gt;









&lt;script src=&#34;libs/kePrint-0.0.1/kePrint.js&#34;&gt;&lt;/script&gt;



&lt;link rel=&#34;stylesheet&#34; href=&#34;templates/style.css&#34; type=&#34;text/css&#34; /&gt;
&lt;/head&gt;

&lt;body&gt;



  &lt;div class=&#34;book without-animation with-summary font-size-2 font-family-1&#34; data-basepath=&#34;.&#34;&gt;

    &lt;div class=&#34;book-summary&#34;&gt;
      &lt;nav role=&#34;navigation&#34;&gt;

&lt;ul class=&#34;summary&#34;&gt;
&lt;li&gt;&lt;a href=&#34;introduction.html#introduction&#34;&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1&lt;/b&gt; &lt;strong&gt;Linguistic Complexity&lt;/strong&gt;&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.1&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subchap:categorizing&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.1&lt;/b&gt; Categorizing Linguistic Complexity Measures&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.2&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subchap:intrinsic&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.2&lt;/b&gt; Intrinsic Perspective&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.2.1&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subsubchap:structural&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.2.1&lt;/b&gt; Structural Linguistic Complexity&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.2.2&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subsubchap:lm-surprisal&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.2.2&lt;/b&gt; Language Modeling Surprisal&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.3&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subchap:extrinsic&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.3&lt;/b&gt; Extrinsic Perspective&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.3.1&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subsubchap:readability&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.3.1&lt;/b&gt; Automatic Readability Assessment&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.3.2&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subsubchap:pc&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.3.2&lt;/b&gt; Perceived Complexity Prediction&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.3.3&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subsubchap:eye-tracking&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.3.3&lt;/b&gt; Gaze Metrics Prediction&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.4&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subchap:garden-path&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.4&lt;/b&gt; Garden-path Sentences&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2&lt;/b&gt; &lt;strong&gt;Models of Linguistic Complexity&lt;/strong&gt;&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2.1&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html#subchap:desiderata&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2.1&lt;/b&gt; Desiderata for Models of Linguistic Complexity&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2.2&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html#subchap:nlm&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2.2&lt;/b&gt; Neural Language Models: Unsupervised Multitask Learners&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2.2.1&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html#subsubchap:syntax-nlm&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2.2.1&lt;/b&gt; Emergent Linguistic Structures in Neural Language Models&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2.3&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html#subchap:analyzing-nlm&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2.3&lt;/b&gt; Analyzing Neural Models of Complexity&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2.3.1&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html#subsubchap:probe&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2.3.1&lt;/b&gt; Probing classifiers&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2.3.2&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html#subsubchap:rsa&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2.3.2&lt;/b&gt; Representational Similarity Analysis&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2.3.3&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html#subsubchap:pwcca&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2.3.3&lt;/b&gt; Projection-Weighted Canonical Correlation Analysis&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3&lt;/b&gt; &lt;strong&gt;Complexity Phenomena in Linguistic Annotations and Language Models&lt;/strong&gt;&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3.1&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html#subchap:ex1-data&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3.1&lt;/b&gt; Data and Preprocessing&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3.2&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html#subchap:ex1-analysis&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3.2&lt;/b&gt; Analysis of Linguistic Phenomena&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3.2.1&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html#subsubchap:ex1-analysis-bins&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3.2.1&lt;/b&gt; Linguistic Phenomena in Length-controlled Bins&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3.3&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html#subchap:ex1-modeling&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3.3&lt;/b&gt; Modeling Online and Offline Linguistic Complexity&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3.3.1&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html#subsubchap:ex1-modeling-bins&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3.3.1&lt;/b&gt; Modeling Complexity in Length-controlled Bins&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3.4&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html#subchap:ex1-probing&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3.4&lt;/b&gt; Probing Linguistic Phenomena in ALBERT Representations&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3.5&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html#subchap:ex1-summary&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3.5&lt;/b&gt; Summary&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;4&#34; data-path=&#34;chap-ex2.html&#34;&gt;&lt;a href=&#34;chap-ex2.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;4&lt;/b&gt; &lt;strong&gt;Representational Similarity in Models of Complexity&lt;/strong&gt;&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;4.1&#34; data-path=&#34;chap-ex2.html&#34;&gt;&lt;a href=&#34;chap-ex2.html#knowledge-driven-requirements-for-learning-models&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;4.1&lt;/b&gt; Knowledge-driven Requirements for Learning Models&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;4.2&#34; data-path=&#34;chap-ex2.html&#34;&gt;&lt;a href=&#34;chap-ex2.html#subchap:ex2-experiments&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;4.2&lt;/b&gt; Experimentsl Evaluation&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;4.2.1&#34; data-path=&#34;chap-ex2.html&#34;&gt;&lt;a href=&#34;chap-ex2.html#subsubchap:ex2-data&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;4.2.1&lt;/b&gt; Data&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;4.2.2&#34; data-path=&#34;chap-ex2.html&#34;&gt;&lt;a href=&#34;chap-ex2.html#subsubchap:ex2-inter&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;4.2.2&lt;/b&gt; Inter-model Representational Similarity&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;4.2.3&#34; data-path=&#34;chap-ex2.html&#34;&gt;&lt;a href=&#34;chap-ex2.html#subsubchap:ex2-intra&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;4.2.3&lt;/b&gt; Intra-model Representational Similarity&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;4.3&#34; data-path=&#34;chap-ex2.html&#34;&gt;&lt;a href=&#34;chap-ex2.html#subchap:ex2-summary&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;4.3&lt;/b&gt; Summary&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;5&#34; data-path=&#34;chap-ex3.html&#34;&gt;&lt;a href=&#34;chap-ex3.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;5&lt;/b&gt; &lt;strong&gt;Gaze-informed Models for Cognitive Processing Prediction&lt;/strong&gt;&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;5.1&#34; data-path=&#34;chap-ex3.html&#34;&gt;&lt;a href=&#34;chap-ex3.html#subchap:ex3-setup&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;5.1&lt;/b&gt; Experimental Setup&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;5.2&#34; data-path=&#34;chap-ex3.html&#34;&gt;&lt;a href=&#34;chap-ex3.html#subchap:ex3-experiments&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;5.2&lt;/b&gt; Experimental Evaluation&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;5.2.1&#34; data-path=&#34;chap-ex3.html&#34;&gt;&lt;a href=&#34;chap-ex3.html#subsubchap:ex3-magnitudes&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;5.2.1&lt;/b&gt; Estimating Magnitudes of Garden-path Delays&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;5.2.2&#34; data-path=&#34;chap-ex3.html&#34;&gt;&lt;a href=&#34;chap-ex3.html#subsubchap:ex3-predicting&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;5.2.2&lt;/b&gt; Predicting Delays with Surprisal and Gaze Metrics&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;5.3&#34; data-path=&#34;chap-ex3.html&#34;&gt;&lt;a href=&#34;chap-ex3.html#subchap:ex3-summary&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;5.3&lt;/b&gt; Summary&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;conclusion.html#conclusion&#34;&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;&#34; data-path=&#34;conclusion.html&#34;&gt;&lt;a href=&#34;conclusion.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;Broader Impact and Ethical Perspectives&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;&#34; data-path=&#34;conclusion.html&#34;&gt;&lt;a href=&#34;conclusion.html#future-directions&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;Future Directions&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;appendix&#34;&gt;&lt;span&gt;&lt;b&gt;Appendix&lt;/b&gt;&lt;/span&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;A&#34; data-path=&#34;app-ling-feats.html&#34;&gt;&lt;a href=&#34;app-ling-feats.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;A&lt;/b&gt; Linguistic Features&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;A.1&#34; data-path=&#34;app-ling-feats.html&#34;&gt;&lt;a href=&#34;app-ling-feats.html#raw-text-properties-and-lexical-variety&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;A.1&lt;/b&gt; Raw Text Properties and Lexical Variety&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;A.2&#34; data-path=&#34;app-ling-feats.html&#34;&gt;&lt;a href=&#34;app-ling-feats.html#morpho-syntacting-information&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;A.2&lt;/b&gt; Morpho-syntacting Information&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;A.3&#34; data-path=&#34;app-ling-feats.html&#34;&gt;&lt;a href=&#34;app-ling-feats.html#verbal-predicate-structure&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;A.3&lt;/b&gt; Verbal Predicate Structure&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;A.4&#34; data-path=&#34;app-ling-feats.html&#34;&gt;&lt;a href=&#34;app-ling-feats.html#global-and-local-parsed-tree-structures&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;A.4&lt;/b&gt; Global and Local Parsed Tree Structures&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;A.5&#34; data-path=&#34;app-ling-feats.html&#34;&gt;&lt;a href=&#34;app-ling-feats.html#syntactic-relations&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;A.5&lt;/b&gt; Syntactic Relations&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;A.6&#34; data-path=&#34;app-ling-feats.html&#34;&gt;&lt;a href=&#34;app-ling-feats.html#subordination-phenomena&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;A.6&lt;/b&gt; Subordination Phenomena&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;B&#34; data-path=&#34;app-et-metrics.html&#34;&gt;&lt;a href=&#34;app-et-metrics.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;B&lt;/b&gt; Precisions on Eye-tracking Metrics and Preprocessing&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;C&#34; data-path=&#34;app-et-modeling.html&#34;&gt;&lt;a href=&#34;app-et-modeling.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;C&lt;/b&gt; Multi-task Token-level Regression for Gaze Metrics Prediction&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;D&#34; data-path=&#34;app-intra-sim.html&#34;&gt;&lt;a href=&#34;app-intra-sim.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;D&lt;/b&gt; Intra-model Similarity for All Models&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;E&#34; data-path=&#34;app-garden-paths-et.html&#34;&gt;&lt;a href=&#34;app-garden-paths-et.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;E&lt;/b&gt; Gaze Metrics Predictions for Garden Path Sentences&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;F&#34; data-path=&#34;app-params.html&#34;&gt;&lt;a href=&#34;app-params.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;F&lt;/b&gt; Reproducibility and Environmental Impact&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;&#34; data-path=&#34;references.html&#34;&gt;&lt;a href=&#34;references.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;References&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;divider&#34;&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://gsarti.com&#34;&gt;Back to my website&lt;/a&gt;&lt;/li&gt;

&lt;/ul&gt;

      &lt;/nav&gt;
    &lt;/div&gt;

    &lt;div class=&#34;book-body&#34;&gt;
      &lt;div class=&#34;body-inner&#34;&gt;
        &lt;div class=&#34;book-header&#34; role=&#34;navigation&#34;&gt;
          &lt;h1&gt;
            &lt;i class=&#34;fa fa-circle-o-notch fa-spin&#34;&gt;&lt;/i&gt;&lt;a href=&#34;./&#34;&gt;Interpreting Neural Language Models&lt;br /&gt;
for Linguistic Complexity Assessment&lt;/a&gt;
          &lt;/h1&gt;
        &lt;/div&gt;

        &lt;div class=&#34;page-wrapper&#34; tabindex=&#34;-1&#34; role=&#34;main&#34;&gt;
          &lt;div class=&#34;page-inner&#34;&gt;

            &lt;section class=&#34;normal&#34; id=&#34;section-&#34;&gt;
&lt;div id=&#34;app:params&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;F&lt;/span&gt; Reproducibility and Environmental Impact&lt;/h1&gt;
&lt;table class=&#34;table&#34; style=&#34;font-size: 11px; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;caption style=&#34;font-size: initial !important;&#34;&gt;
&lt;span id=&#34;tab:train-params&#34;&gt;Table F.1: &lt;/span&gt;Variable training parameters used in the experiments of this study. MTL stands for multitask learning.
&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;border-bottom:hidden&#34; colspan=&#34;1&#34;&gt;
&lt;/th&gt;
&lt;th style=&#34;border-bottom:hidden; padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; font-weight: bold; &#34; colspan=&#34;3&#34;&gt;
&lt;div style=&#34;border-bottom: 1px solid #ddd; padding-bottom: 5px; &#34;&gt;
Chapter 3
&lt;/div&gt;
&lt;/th&gt;
&lt;th style=&#34;border-bottom:hidden; padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; font-weight: bold; &#34; colspan=&#34;3&#34;&gt;
&lt;div style=&#34;border-bottom: 1px solid #ddd; padding-bottom: 5px; &#34;&gt;
Chapter 4
&lt;/div&gt;
&lt;/th&gt;
&lt;th style=&#34;border-bottom:hidden; padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; font-weight: bold; &#34; colspan=&#34;2&#34;&gt;
&lt;div style=&#34;border-bottom: 1px solid #ddd; padding-bottom: 5px; &#34;&gt;
Chapter 5
&lt;/div&gt;
&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
PC
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
ET
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
Probes
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
PC
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
ET
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
RA
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
ALBERT
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
GPT-2
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
fine-tuning
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
standard
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
MTL
&lt;/td&gt;
&lt;td style=&#34;text-align:center;border-right:1px solid;&#34;&gt;
MTL
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
standard
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
MTL
&lt;/td&gt;
&lt;td style=&#34;text-align:center;border-right:1px solid;&#34;&gt;
standard
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
MTL
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
MTL
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
granularity
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
sent.
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
sent.
&lt;/td&gt;
&lt;td style=&#34;text-align:center;border-right:1px solid;&#34;&gt;
sent.
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
sent.
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
word
&lt;/td&gt;
&lt;td style=&#34;text-align:center;border-right:1px solid;&#34;&gt;
sent.
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
word
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
word
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
freeze LM &lt;span class=&#34;math inline&#34;&gt;\(w\)&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
❌
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
❌
&lt;/td&gt;
&lt;td style=&#34;text-align:center;border-right:1px solid;&#34;&gt;
✅
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
❌
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
❌
&lt;/td&gt;
&lt;td style=&#34;text-align:center;border-right:1px solid;&#34;&gt;
❌
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
❌
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
❌
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
weighted loss
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
✅
&lt;/td&gt;
&lt;td style=&#34;text-align:center;border-right:1px solid;&#34;&gt;
❌
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
❌
&lt;/td&gt;
&lt;td style=&#34;text-align:center;border-right:1px solid;&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
❌
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
❌
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
CV folds
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:center;border-right:1px solid;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center;border-right:1px solid;&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
early stopping
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
✅
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
✅
&lt;/td&gt;
&lt;td style=&#34;text-align:center;border-right:1px solid;&#34;&gt;
❌
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
✅
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
✅
&lt;/td&gt;
&lt;td style=&#34;text-align:center;border-right:1px solid;&#34;&gt;
✅
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
✅
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
✅
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
training epochs
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
15
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
15
&lt;/td&gt;
&lt;td style=&#34;text-align:center;border-right:1px solid;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
15
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
15
&lt;/td&gt;
&lt;td style=&#34;text-align:center;border-right:1px solid;&#34;&gt;
15
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
15
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
15
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
patience
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:center;border-right:1px solid;&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:center;border-right:1px solid;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
5
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
evaluation steps
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
20
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
40
&lt;/td&gt;
&lt;td style=&#34;text-align:center;border-right:1px solid;&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
20
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
100
&lt;/td&gt;
&lt;td style=&#34;text-align:center;border-right:1px solid;&#34;&gt;
80
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
100
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
100
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;custompar&#34;&gt;Tools&lt;/span&gt; Experiments were executed on a Ubuntu 18.04 LTS server, using a NVIDIA K40 GPU with 12GB RAM and CUDA 10.1. Relevant Python libraries used throughout the study with their respective versions are: 🤗 &lt;code&gt;transformers 2.11.0&lt;/code&gt; for accessing pre-trained Transformer language models, &lt;code&gt;farm 0.4.5&lt;/code&gt; for multitask learning, &lt;code&gt;torch 1.3.0&lt;/code&gt; as a backed for deep learning, and &lt;code&gt;syntaxgym 0.5.3&lt;/code&gt; for Chapter &lt;a href=&#34;chap-ex3.html#chap:ex3&#34;&gt;5&lt;/a&gt; experiments. Python 3.6.3 was used for all training scripts. A custom adaptation of the Oxforddown template was used for this thesis.&lt;a href=&#34;#fn25&#34; class=&#34;footnote-ref&#34; id=&#34;fnref25&#34;&gt;&lt;sup&gt;25&lt;/sup&gt;&lt;/a&gt; Code for reproducibility purposes is available at the address &lt;a href=&#34;https://github.com/gsarti/interpreting-complexity&#34;&gt;https://github.com/gsarti/interpreting-complexity&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;custompar&#34;&gt;Model Training&lt;/span&gt; Table &lt;a href=&#34;app-params.html#tab:train-params&#34;&gt;F.1&lt;/a&gt; present the set of variable training parameters used in all the experiments of this study. Besides those, a set of fixed parameters was also used: all experiments were performed using a batch size of 32 observations, a maximum sequence length of 128 tokens, a linear training schedule with one-tenth of total steps used as warmup steps, the &lt;em&gt;AdamW&lt;/em&gt; optimizer &lt;span class=&#34;citation&#34;&gt;(Loshchilov and Hutter &lt;a href=&#34;#ref-loshchilov-hutter-2019-decoupled&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt; with weight decay equal to &lt;span class=&#34;math inline&#34;&gt;\(0.01\)&lt;/span&gt;, and a learning rate of &lt;span class=&#34;math inline&#34;&gt;\(10^{-5}\)&lt;/span&gt;. No hyperparameter search was performed due to time limitations.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;custompar&#34;&gt;Tokenization&lt;/span&gt; All tokenizers used in the experiments used cased text and were based respectively on the SentencePiece approach &lt;span class=&#34;citation&#34;&gt;(Kudo and Richardson &lt;a href=&#34;#ref-kudo-richardson-2018-sentencepiece&#34;&gt;2018&lt;/a&gt;)&lt;/span&gt; for ALBERT and a custom version of Byte-Pair Encoding tokenization &lt;span class=&#34;citation&#34;&gt;(Sennrich, Haddow, and Birch &lt;a href=&#34;#ref-sennrich-etal-2016-neural&#34;&gt;2016&lt;/a&gt;)&lt;/span&gt; with token-like whitespaces for GPT-2. Default &lt;code&gt;AlbertTokenizer&lt;/code&gt; and &lt;code&gt;GPT2Tokenizer&lt;/code&gt; classes available in the 🤗 &lt;code&gt;transformers&lt;/code&gt; library with pretrained tokenizers were used for this purpose. The vocabulary used by those had size 30’000 for ALBERT and 50’257 for GPT-2, including special tokens.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;custompar&#34;&gt;Architecture&lt;/span&gt; The default parameters for the 🤗 &lt;code&gt;transformers&lt;/code&gt; checkpoints of ALBERT and GPT-2 (specifically, &lt;code&gt;albert-base-v2&lt;/code&gt; and &lt;code&gt;gpt2&lt;/code&gt; in the Model Hub) were used for this study. Concretely, this means embeddings and hidden sizes of 128 and 3072 for ALBERT and tied embedding-hidden size of 768 for GPT-2, 12 transformer blocks using 12 heads for multi-head self-attention each, and a smoothed variant of the Gaussian Error Linear Unit (GELU) as nonlinearity &lt;span class=&#34;citation&#34;&gt;(Hendrycks and Gimpel &lt;a href=&#34;#ref-hendrycks-gimpel-2016-gaussian&#34;&gt;2016&lt;/a&gt;)&lt;/span&gt;. GPT-2 has an embedding and attention dropout rate of 0.1 and a layer normalization &lt;span class=&#34;citation&#34;&gt;(Ba, Kiros, and Hinton &lt;a href=&#34;#ref-ba-etal-2016-layer&#34;&gt;2016&lt;/a&gt;)&lt;/span&gt; epsilon of &lt;span class=&#34;math inline&#34;&gt;\(10^{-5}\)&lt;/span&gt;, while ALBERT employs a classifier dropout rate of 0.1 and a layer normalization epsilon of &lt;span class=&#34;math inline&#34;&gt;\(10^{-12}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;custompar&#34;&gt;CO2 Emissions Related to Experiments&lt;/span&gt; Experiments were conducted using the private infrastructure of the ItaliaNLP Lab&lt;a href=&#34;#fn26&#34; class=&#34;footnote-ref&#34; id=&#34;fnref26&#34;&gt;&lt;sup&gt;26&lt;/sup&gt;&lt;/a&gt; at the Institute for Computational Linguistics “A. Zampolli” (ILC-CNR) in Pisa, which has an estimated carbon efficiency of 0.321 kgCO&lt;span class=&#34;math inline&#34;&gt;\(_2\)&lt;/span&gt;eq/kWh &lt;span class=&#34;citation&#34;&gt;(Moro and Lonza &lt;a href=&#34;#ref-moro2018electricity&#34;&gt;2018&lt;/a&gt;)&lt;/span&gt;. A cumulative of roughly 100 hours of computation was performed on a Tesla K40 GPU (TDP of 245W). Total emissions are estimated to be 7.86 kgCO&lt;span class=&#34;math inline&#34;&gt;\(_2\)&lt;/span&gt;eq. Estimations were conducted using the Machine Learning Impact Calculator&lt;a href=&#34;#fn27&#34; class=&#34;footnote-ref&#34; id=&#34;fnref27&#34;&gt;&lt;sup&gt;27&lt;/sup&gt;&lt;/a&gt; presented in &lt;span class=&#34;citation&#34;&gt;Lacoste et al. (&lt;a href=&#34;#ref-lacoste2019quantifying&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In-detail reports of all experimental runsre produced automatically using the MLFlow&lt;a href=&#34;#fn28&#34; class=&#34;footnote-ref&#34; id=&#34;fnref28&#34;&gt;&lt;sup&gt;28&lt;/sup&gt;&lt;/a&gt; tool and are available at the following address: &lt;a href=&#34;https://public-mlflow.deepset.ai/#/experiments/99&#34;&gt;https://public-mlflow.deepset.ai/#/experiments/99&lt;/a&gt;.&lt;/p&gt;

&lt;/div&gt;
&lt;h3&gt;References&lt;/h3&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-ba-etal-2016-layer&#34;&gt;
&lt;p&gt;Ba, Jimmy, J. Kiros, and Geoffrey E. Hinton. 2016. “Layer Normalization.” &lt;em&gt;ArXiv Pre-Print&lt;/em&gt; 1607.06450. &lt;a href=&#34;https://arxiv.org/abs/1607.06450&#34;&gt;https://arxiv.org/abs/1607.06450&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-hendrycks-gimpel-2016-gaussian&#34;&gt;
&lt;p&gt;Hendrycks, Dan, and Kevin Gimpel. 2016. “Gaussian Error Linear Units (Gelus).” &lt;em&gt;ArXiv Pre-Print&lt;/em&gt; 1606.08415. &lt;a href=&#34;https://arxiv.org/abs/1606.08415&#34;&gt;https://arxiv.org/abs/1606.08415&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-kudo-richardson-2018-sentencepiece&#34;&gt;
&lt;p&gt;Kudo, Taku, and John Richardson. 2018. “SentencePiece: A Simple and Language Independent Subword Tokenizer and Detokenizer for Neural Text Processing.” In &lt;em&gt;Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations&lt;/em&gt;, 66–71. Brussels, Belgium: Association for Computational Linguistics. &lt;a href=&#34;https://doi.org/10.18653/v1/D18-2012&#34;&gt;https://doi.org/10.18653/v1/D18-2012&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-lacoste2019quantifying&#34;&gt;
&lt;p&gt;Lacoste, Alexandre, Alexandra Luccioni, Victor Schmidt, and Thomas Dandres. 2019. “Quantifying the Carbon Emissions of Machine Learning.” &lt;em&gt;ArXiv Pre-Print&lt;/em&gt; 1910.09700.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-loshchilov-hutter-2019-decoupled&#34;&gt;
&lt;p&gt;Loshchilov, I., and F. Hutter. 2019. “Decoupled Weight Decay Regularization.” In &lt;em&gt;Proceeding of the 7th International Conference on Learning Representations (Iclr’19)&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-moro2018electricity&#34;&gt;
&lt;p&gt;Moro, Alberto, and Laura Lonza. 2018. “Electricity Carbon Intensity in European Member States: Impacts on Ghg Emissions of Electric Vehicles.” &lt;em&gt;Transportation Research Part D: Transport and Environment&lt;/em&gt; 64. Elsevier: 5–14.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-sennrich-etal-2016-neural&#34;&gt;
&lt;p&gt;Sennrich, Rico, Barry Haddow, and Alexandra Birch. 2016. “Neural Machine Translation of Rare Words with Subword Units.” In &lt;em&gt;Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)&lt;/em&gt;, 1715–25. Berlin, Germany: Association for Computational Linguistics. &lt;a href=&#34;https://doi.org/10.18653/v1/P16-1162&#34;&gt;https://doi.org/10.18653/v1/P16-1162&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol start=&#34;25&#34;&gt;
&lt;li id=&#34;fn25&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/AI-Student-Society/thesisdown-it&#34; class=&#34;uri&#34;&gt;https://github.com/AI-Student-Society/thesisdown-it&lt;/a&gt;&lt;a href=&#34;app-params.html#fnref25&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn26&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://www.italianlp.it&#34;&gt;https://www.italianlp.it&lt;/a&gt;&lt;a href=&#34;app-params.html#fnref26&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn27&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://mlco2.github.io/impact#compute&#34;&gt;https://mlco2.github.io/impact#compute&lt;/a&gt;&lt;a href=&#34;app-params.html#fnref27&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn28&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://mlflow.org/&#34; class=&#34;uri&#34;&gt;https://mlflow.org/&lt;/a&gt;&lt;a href=&#34;app-params.html#fnref28&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
            &lt;/section&gt;

          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
&lt;a href=&#34;app-garden-paths-et.html&#34; class=&#34;navigation navigation-prev &#34; aria-label=&#34;Previous page&#34;&gt;&lt;i class=&#34;fa fa-angle-left&#34;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&#34;references.html&#34; class=&#34;navigation navigation-next &#34; aria-label=&#34;Next page&#34;&gt;&lt;i class=&#34;fa fa-angle-right&#34;&gt;&lt;/i&gt;&lt;/a&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/app.min.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/lunr.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/clipboard.min.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/plugin-search.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/plugin-sharing.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/plugin-fontsettings.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/plugin-bookdown.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/jquery.highlight.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/plugin-clipboard.js&#34;&gt;&lt;/script&gt;
&lt;script&gt;
gitbook.require([&#34;gitbook&#34;], function(gitbook) {
gitbook.start({
&#34;sharing&#34;: {
&#34;github&#34;: true,
&#34;facebook&#34;: true,
&#34;twitter&#34;: true,
&#34;linkedin&#34;: true,
&#34;weibo&#34;: false,
&#34;instapaper&#34;: false,
&#34;vk&#34;: false,
&#34;all&#34;: false
},
&#34;fontsettings&#34;: {
&#34;theme&#34;: &#34;white&#34;,
&#34;family&#34;: &#34;sans&#34;,
&#34;size&#34;: 2
},
&#34;edit&#34;: {
&#34;link&#34;: &#34;https://github.com/gsarti/master-thesis/tree/master/extra/Appendix.Rmd&#34;,
&#34;text&#34;: &#34;Edit&#34;
},
&#34;history&#34;: {
&#34;link&#34;: null,
&#34;text&#34;: null
},
&#34;view&#34;: {
&#34;link&#34;: null,
&#34;text&#34;: null
},
&#34;download&#34;: [[&#34;Sarti_2020_Interpreting_NLMs_for_LCA.pdf&#34;, &#34;PDF&#34;]],
&#34;toc&#34;: {
&#34;collapse&#34;: &#34;subsection&#34;,
&#34;scroll_highlight&#34;: true
},
&#34;info&#34;: false
});
});
&lt;/script&gt;

&lt;!-- dynamically load mathjax for compatibility with self-contained --&gt;
&lt;script&gt;
  (function () {
    var script = document.createElement(&#34;script&#34;);
    script.type = &#34;text/javascript&#34;;
    var src = &#34;true&#34;;
    if (src === &#34;&#34; || src === &#34;true&#34;) src = &#34;https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML&#34;;
    if (location.protocol !== &#34;file:&#34;)
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, &#39;&#39;);
    script.src = src;
    document.getElementsByTagName(&#34;head&#34;)[0].appendChild(script);
  })();
&lt;/script&gt;
&lt;/body&gt;

&lt;/html&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>http://localhost:1313/msc-thesis/chap-ex1/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/msc-thesis/chap-ex1/</guid>
      <description>&lt;!DOCTYPE html&gt;
&lt;html lang=&#34;&#34; xml:lang=&#34;&#34;&gt;
&lt;head&gt;

  &lt;meta charset=&#34;utf-8&#34; /&gt;
  &lt;meta http-equiv=&#34;X-UA-Compatible&#34; content=&#34;IE=edge&#34; /&gt;
  &lt;title&gt;3 Complexity Phenomena in Linguistic Annotations and Language Models | Interpreting Neural Language Models for Linguistic Complexity Assessment&lt;/title&gt;
  &lt;meta name=&#34;description&#34; content=&#34;MSc Thesis at the University of Trieste and SISSA under the supervision of Prof. Davide Crepaldi and Dott. Felice dell&#39;Orletta&#34; /&gt;
  &lt;meta name=&#34;generator&#34; content=&#34;bookdown 0.20.6 and GitBook 2.6.7&#34; /&gt;

  &lt;meta property=&#34;og:title&#34; content=&#34;3 Complexity Phenomena in Linguistic Annotations and Language Models | Interpreting Neural Language Models for Linguistic Complexity Assessment&#34; /&gt;
  &lt;meta property=&#34;og:type&#34; content=&#34;book&#34; /&gt;
  &lt;meta property=&#34;og:url&#34; content=&#34;https://gsarti.com/master-thesis&#34; /&gt;
  &lt;meta property=&#34;og:image&#34; content=&#34;https://gsarti.com/master-thesisfigures/cover.png&#34; /&gt;
  &lt;meta property=&#34;og:description&#34; content=&#34;MSc Thesis at the University of Trieste and SISSA under the supervision of Prof. Davide Crepaldi and Dott. Felice dell&#39;Orletta&#34; /&gt;
  &lt;meta name=&#34;github-repo&#34; content=&#34;gsarti/interpreting-complexity&#34; /&gt;

  &lt;meta name=&#34;twitter:card&#34; content=&#34;summary&#34; /&gt;
  &lt;meta name=&#34;twitter:title&#34; content=&#34;3 Complexity Phenomena in Linguistic Annotations and Language Models | Interpreting Neural Language Models for Linguistic Complexity Assessment&#34; /&gt;
  
  &lt;meta name=&#34;twitter:description&#34; content=&#34;MSc Thesis at the University of Trieste and SISSA under the supervision of Prof. Davide Crepaldi and Dott. Felice dell&#39;Orletta&#34; /&gt;
  &lt;meta name=&#34;twitter:image&#34; content=&#34;https://gsarti.com/master-thesisfigures/cover.png&#34; /&gt;

&lt;meta name=&#34;author&#34; content=&#34;Gabriele Sarti&#34; /&gt;



  &lt;meta name=&#34;viewport&#34; content=&#34;width=device-width, initial-scale=1&#34; /&gt;
  &lt;meta name=&#34;apple-mobile-web-app-capable&#34; content=&#34;yes&#34; /&gt;
  &lt;meta name=&#34;apple-mobile-web-app-status-bar-style&#34; content=&#34;black&#34; /&gt;
  &lt;link rel=&#34;apple-touch-icon-precomposed&#34; sizes=&#34;152x152&#34; href=&#34;figures/icons/apple-icon.png&#34; /&gt;
  &lt;link rel=&#34;shortcut icon&#34; href=&#34;figures/icons/favicon.ico&#34; type=&#34;image/x-icon&#34; /&gt;
&lt;link rel=&#34;prev&#34; href=&#34;chap-models.html&#34;/&gt;
&lt;link rel=&#34;next&#34; href=&#34;chap-ex2.html&#34;/&gt;
&lt;style type=&#34;text/css&#34;&gt;
p.abstract{
  text-align: center;
  font-weight: bold;
}
div.abstract{
  margin: auto;
  width: 90%;
}
&lt;/style&gt;
&lt;script src=&#34;libs/jquery-2.2.3/jquery.min.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;libs/gitbook-2.6.7/css/style.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;link href=&#34;libs/gitbook-2.6.7/css/plugin-table.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;link href=&#34;libs/gitbook-2.6.7/css/plugin-bookdown.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;link href=&#34;libs/gitbook-2.6.7/css/plugin-highlight.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;link href=&#34;libs/gitbook-2.6.7/css/plugin-search.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;link href=&#34;libs/gitbook-2.6.7/css/plugin-fontsettings.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;link href=&#34;libs/gitbook-2.6.7/css/plugin-clipboard.css&#34; rel=&#34;stylesheet&#34; /&gt;









&lt;script src=&#34;libs/kePrint-0.0.1/kePrint.js&#34;&gt;&lt;/script&gt;



&lt;link rel=&#34;stylesheet&#34; href=&#34;templates/style.css&#34; type=&#34;text/css&#34; /&gt;
&lt;/head&gt;

&lt;body&gt;



  &lt;div class=&#34;book without-animation with-summary font-size-2 font-family-1&#34; data-basepath=&#34;.&#34;&gt;

    &lt;div class=&#34;book-summary&#34;&gt;
      &lt;nav role=&#34;navigation&#34;&gt;

&lt;ul class=&#34;summary&#34;&gt;
&lt;li&gt;&lt;a href=&#34;introduction.html#introduction&#34;&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1&lt;/b&gt; &lt;strong&gt;Linguistic Complexity&lt;/strong&gt;&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.1&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subchap:categorizing&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.1&lt;/b&gt; Categorizing Linguistic Complexity Measures&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.2&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subchap:intrinsic&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.2&lt;/b&gt; Intrinsic Perspective&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.2.1&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subsubchap:structural&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.2.1&lt;/b&gt; Structural Linguistic Complexity&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.2.2&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subsubchap:lm-surprisal&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.2.2&lt;/b&gt; Language Modeling Surprisal&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.3&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subchap:extrinsic&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.3&lt;/b&gt; Extrinsic Perspective&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.3.1&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subsubchap:readability&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.3.1&lt;/b&gt; Automatic Readability Assessment&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.3.2&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subsubchap:pc&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.3.2&lt;/b&gt; Perceived Complexity Prediction&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.3.3&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subsubchap:eye-tracking&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.3.3&lt;/b&gt; Gaze Metrics Prediction&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.4&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subchap:garden-path&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.4&lt;/b&gt; Garden-path Sentences&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2&lt;/b&gt; &lt;strong&gt;Models of Linguistic Complexity&lt;/strong&gt;&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2.1&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html#subchap:desiderata&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2.1&lt;/b&gt; Desiderata for Models of Linguistic Complexity&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2.2&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html#subchap:nlm&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2.2&lt;/b&gt; Neural Language Models: Unsupervised Multitask Learners&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2.2.1&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html#subsubchap:syntax-nlm&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2.2.1&lt;/b&gt; Emergent Linguistic Structures in Neural Language Models&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2.3&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html#subchap:analyzing-nlm&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2.3&lt;/b&gt; Analyzing Neural Models of Complexity&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2.3.1&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html#subsubchap:probe&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2.3.1&lt;/b&gt; Probing classifiers&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2.3.2&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html#subsubchap:rsa&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2.3.2&lt;/b&gt; Representational Similarity Analysis&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2.3.3&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html#subsubchap:pwcca&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2.3.3&lt;/b&gt; Projection-Weighted Canonical Correlation Analysis&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3&lt;/b&gt; &lt;strong&gt;Complexity Phenomena in Linguistic Annotations and Language Models&lt;/strong&gt;&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3.1&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html#subchap:ex1-data&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3.1&lt;/b&gt; Data and Preprocessing&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3.2&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html#subchap:ex1-analysis&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3.2&lt;/b&gt; Analysis of Linguistic Phenomena&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3.2.1&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html#subsubchap:ex1-analysis-bins&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3.2.1&lt;/b&gt; Linguistic Phenomena in Length-controlled Bins&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3.3&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html#subchap:ex1-modeling&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3.3&lt;/b&gt; Modeling Online and Offline Linguistic Complexity&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3.3.1&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html#subsubchap:ex1-modeling-bins&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3.3.1&lt;/b&gt; Modeling Complexity in Length-controlled Bins&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3.4&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html#subchap:ex1-probing&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3.4&lt;/b&gt; Probing Linguistic Phenomena in ALBERT Representations&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3.5&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html#subchap:ex1-summary&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3.5&lt;/b&gt; Summary&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;4&#34; data-path=&#34;chap-ex2.html&#34;&gt;&lt;a href=&#34;chap-ex2.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;4&lt;/b&gt; &lt;strong&gt;Representational Similarity in Models of Complexity&lt;/strong&gt;&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;4.1&#34; data-path=&#34;chap-ex2.html&#34;&gt;&lt;a href=&#34;chap-ex2.html#knowledge-driven-requirements-for-learning-models&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;4.1&lt;/b&gt; Knowledge-driven Requirements for Learning Models&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;4.2&#34; data-path=&#34;chap-ex2.html&#34;&gt;&lt;a href=&#34;chap-ex2.html#subchap:ex2-experiments&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;4.2&lt;/b&gt; Experimentsl Evaluation&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;4.2.1&#34; data-path=&#34;chap-ex2.html&#34;&gt;&lt;a href=&#34;chap-ex2.html#subsubchap:ex2-data&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;4.2.1&lt;/b&gt; Data&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;4.2.2&#34; data-path=&#34;chap-ex2.html&#34;&gt;&lt;a href=&#34;chap-ex2.html#subsubchap:ex2-inter&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;4.2.2&lt;/b&gt; Inter-model Representational Similarity&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;4.2.3&#34; data-path=&#34;chap-ex2.html&#34;&gt;&lt;a href=&#34;chap-ex2.html#subsubchap:ex2-intra&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;4.2.3&lt;/b&gt; Intra-model Representational Similarity&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;4.3&#34; data-path=&#34;chap-ex2.html&#34;&gt;&lt;a href=&#34;chap-ex2.html#subchap:ex2-summary&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;4.3&lt;/b&gt; Summary&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;5&#34; data-path=&#34;chap-ex3.html&#34;&gt;&lt;a href=&#34;chap-ex3.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;5&lt;/b&gt; &lt;strong&gt;Gaze-informed Models for Cognitive Processing Prediction&lt;/strong&gt;&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;5.1&#34; data-path=&#34;chap-ex3.html&#34;&gt;&lt;a href=&#34;chap-ex3.html#subchap:ex3-setup&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;5.1&lt;/b&gt; Experimental Setup&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;5.2&#34; data-path=&#34;chap-ex3.html&#34;&gt;&lt;a href=&#34;chap-ex3.html#subchap:ex3-experiments&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;5.2&lt;/b&gt; Experimental Evaluation&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;5.2.1&#34; data-path=&#34;chap-ex3.html&#34;&gt;&lt;a href=&#34;chap-ex3.html#subsubchap:ex3-magnitudes&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;5.2.1&lt;/b&gt; Estimating Magnitudes of Garden-path Delays&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;5.2.2&#34; data-path=&#34;chap-ex3.html&#34;&gt;&lt;a href=&#34;chap-ex3.html#subsubchap:ex3-predicting&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;5.2.2&lt;/b&gt; Predicting Delays with Surprisal and Gaze Metrics&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;5.3&#34; data-path=&#34;chap-ex3.html&#34;&gt;&lt;a href=&#34;chap-ex3.html#subchap:ex3-summary&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;5.3&lt;/b&gt; Summary&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;conclusion.html#conclusion&#34;&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;&#34; data-path=&#34;conclusion.html&#34;&gt;&lt;a href=&#34;conclusion.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;Broader Impact and Ethical Perspectives&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;&#34; data-path=&#34;conclusion.html&#34;&gt;&lt;a href=&#34;conclusion.html#future-directions&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;Future Directions&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;appendix&#34;&gt;&lt;span&gt;&lt;b&gt;Appendix&lt;/b&gt;&lt;/span&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;A&#34; data-path=&#34;app-ling-feats.html&#34;&gt;&lt;a href=&#34;app-ling-feats.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;A&lt;/b&gt; Linguistic Features&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;A.1&#34; data-path=&#34;app-ling-feats.html&#34;&gt;&lt;a href=&#34;app-ling-feats.html#raw-text-properties-and-lexical-variety&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;A.1&lt;/b&gt; Raw Text Properties and Lexical Variety&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;A.2&#34; data-path=&#34;app-ling-feats.html&#34;&gt;&lt;a href=&#34;app-ling-feats.html#morpho-syntacting-information&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;A.2&lt;/b&gt; Morpho-syntacting Information&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;A.3&#34; data-path=&#34;app-ling-feats.html&#34;&gt;&lt;a href=&#34;app-ling-feats.html#verbal-predicate-structure&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;A.3&lt;/b&gt; Verbal Predicate Structure&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;A.4&#34; data-path=&#34;app-ling-feats.html&#34;&gt;&lt;a href=&#34;app-ling-feats.html#global-and-local-parsed-tree-structures&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;A.4&lt;/b&gt; Global and Local Parsed Tree Structures&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;A.5&#34; data-path=&#34;app-ling-feats.html&#34;&gt;&lt;a href=&#34;app-ling-feats.html#syntactic-relations&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;A.5&lt;/b&gt; Syntactic Relations&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;A.6&#34; data-path=&#34;app-ling-feats.html&#34;&gt;&lt;a href=&#34;app-ling-feats.html#subordination-phenomena&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;A.6&lt;/b&gt; Subordination Phenomena&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;B&#34; data-path=&#34;app-et-metrics.html&#34;&gt;&lt;a href=&#34;app-et-metrics.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;B&lt;/b&gt; Precisions on Eye-tracking Metrics and Preprocessing&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;C&#34; data-path=&#34;app-et-modeling.html&#34;&gt;&lt;a href=&#34;app-et-modeling.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;C&lt;/b&gt; Multi-task Token-level Regression for Gaze Metrics Prediction&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;D&#34; data-path=&#34;app-intra-sim.html&#34;&gt;&lt;a href=&#34;app-intra-sim.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;D&lt;/b&gt; Intra-model Similarity for All Models&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;E&#34; data-path=&#34;app-garden-paths-et.html&#34;&gt;&lt;a href=&#34;app-garden-paths-et.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;E&lt;/b&gt; Gaze Metrics Predictions for Garden Path Sentences&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;F&#34; data-path=&#34;app-params.html&#34;&gt;&lt;a href=&#34;app-params.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;F&lt;/b&gt; Reproducibility and Environmental Impact&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;&#34; data-path=&#34;references.html&#34;&gt;&lt;a href=&#34;references.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;References&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;divider&#34;&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://gsarti.com&#34;&gt;Back to my website&lt;/a&gt;&lt;/li&gt;

&lt;/ul&gt;

      &lt;/nav&gt;
    &lt;/div&gt;

    &lt;div class=&#34;book-body&#34;&gt;
      &lt;div class=&#34;body-inner&#34;&gt;
        &lt;div class=&#34;book-header&#34; role=&#34;navigation&#34;&gt;
          &lt;h1&gt;
            &lt;i class=&#34;fa fa-circle-o-notch fa-spin&#34;&gt;&lt;/i&gt;&lt;a href=&#34;./&#34;&gt;Interpreting Neural Language Models&lt;br /&gt;
for Linguistic Complexity Assessment&lt;/a&gt;
          &lt;/h1&gt;
        &lt;/div&gt;

        &lt;div class=&#34;page-wrapper&#34; tabindex=&#34;-1&#34; role=&#34;main&#34;&gt;
          &lt;div class=&#34;page-inner&#34;&gt;

            &lt;section class=&#34;normal&#34; id=&#34;section-&#34;&gt;
&lt;div id=&#34;chap:ex1&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;3&lt;/span&gt; &lt;strong&gt;Complexity Phenomena in Linguistic Annotations and Language Models&lt;/strong&gt;&lt;/h1&gt;
&lt;p&gt;&lt;!-- this will include a mini table of contents--&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;This chapter investigates the relationship between online gaze metrics and offline perceived complexity judgments by studying how the two viewpoints are represented by a neural language model trained on human-produced data. First, a preliminary analysis of linguistic phenomena associated with the two complexity viewpoints is performed, highlighting similarities and differences across metrics. The effectiveness of a regressor based on explicit linguistic features is then evaluated for sentence complexity prediction and compared to the results obtained by a fine-tuned neural language model with contextual representations. In conclusion, the linguistic competence inside the language model’s embeddings is probed before and after fine-tuning, showing how linguistic information encoded in representations changes as the model learns to predict complexity.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Given the conceptual similarity between raw cognitive processing and human perception of complexity, this chapter investigates whether the relation between eye-tracking metrics and complexity judgments can be highlighted empirically in human annotations and language model representations. With this aim, linguistic features associated with various sentence-level structural phenomena are analyzed in terms of their correlation with offline and online complexity metrics. The performance of models using either complexity-related explicit features or contextualized word embeddings is evaluated, focusing mainly on the neural language model ALBERT &lt;span class=&#34;citation&#34;&gt;(Lan et al. &lt;a href=&#34;#ref-lan-etal-2020-albert&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt; introduced in Section &lt;a href=&#34;chap-models.html#subchap:nlm&#34;&gt;2.2&lt;/a&gt;. The results highlight how both explicit features and learned representations obtain comparable performances when predicting complexity scores. Finally, the focus is shifted to studying how complexity-related properties are encoded in the representations of ALBERT.&lt;/p&gt;
&lt;p&gt;This perspective goes in the direction of exploiting human processing data to address the interpretability issues of unsupervised language representations &lt;span class=&#34;citation&#34;&gt;(Hollenstein, Torre, et al. &lt;a href=&#34;#ref-hollenstein-etal-2019-cognival&#34;&gt;2019&lt;/a&gt;; Gauthier and Levy &lt;a href=&#34;#ref-gauthier-levy-2019-linking&#34;&gt;2019&lt;/a&gt;; Abnar et al. &lt;a href=&#34;#ref-abnar-etal-2019-blackbox&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt;, leveraging the &lt;em&gt;probing task&lt;/em&gt; approach introduced in Section &lt;a href=&#34;chap-models.html#subsubchap:probe&#34;&gt;2.3.1&lt;/a&gt;. It is observed that online and offline complexity fine-tuning produces a consequent increase in probing performances for complexity-related features during probing experiments. This investigation has the specific purpose of studying whether and how learning a new task affects the linguistic properties encoded in pretrained representations. While pre-trained models have been widely studied using probing methods, the effect of fine-tuning on encoded information was seldom investigated. To my best knowledge, no previous work has taken into account sentence complexity assessment as a fine-tuning task for NLMs. Results suggest that the model’s abilities during training are interpretable from a linguistic perspective and are possibly related to its predictive capabilities for complexity assessment.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;custompar&#34;&gt;Contributions&lt;/span&gt; This is the first work displaying the connection between online and offline complexity metrics and studying how a neural language model represents them. This work:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Provides a comprehensive analysis of linguistic phenomena correlated with eye-tracking data and human perception of complexity, addressing similarities and differences from a linguistically-motivated perspective across metrics and at different levels of granularity;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Compares the performance of models using both explicit features and unsupervised contextual representations when predicting online and offline sentence complexity; and&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Shows the natural emergence of complexity-related linguistic phenomena in the representations of language models trained on complexity metrics.&lt;a href=&#34;#fn16&#34; class=&#34;footnote-ref&#34; id=&#34;fnref16&#34;&gt;&lt;sup&gt;16&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;subchap:ex1-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;3.1&lt;/span&gt; Data and Preprocessing&lt;/h2&gt;
&lt;p&gt;The experiments of this chapter leverage two corpora, each capturing different aspects of linguistic complexity:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;custompar&#34;&gt;Eye-tracking&lt;/span&gt; For online complexity metrics, only the monolingual English portion of GECO &lt;span class=&#34;citation&#34;&gt;(Cop et al. &lt;a href=&#34;#ref-cop-etal-2017-presenting&#34;&gt;2017&lt;/a&gt;)&lt;/span&gt;, presented in Section &lt;a href=&#34;chap-ling-comp.html#subsubchap:eye-tracking&#34;&gt;1.3.3&lt;/a&gt;, was used. Four online metrics spanning multiple phases of cognitive processing are selected, respectively: &lt;em&gt;first pass duration&lt;/em&gt; (FPD), &lt;em&gt;total fixation count&lt;/em&gt; (FXC), &lt;em&gt;total fixation duration&lt;/em&gt; (TFD) and &lt;em&gt;total regression duration&lt;/em&gt; (TRD) (see Table &lt;a href=&#34;chap-ling-comp.html#tab:et-metrics&#34;&gt;1.3&lt;/a&gt; for more details). Metrics are sum-aggregated at sentence-level and averaged across participants to obtain a single label for each metric-sentence pair. As a final step to make the corpus more suitable for linguistic complexity analysis, all utterances with fewer than five words, deemed uninteresting from a cognitive processing perspective, are removed.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;custompar&#34;&gt;Perceived Complexity&lt;/span&gt; For the offline evaluation of sentence complexity, the English portion of the corpus by &lt;span class=&#34;citation&#34;&gt;Brunato et al. (&lt;a href=&#34;#ref-brunato-etal-2018-sentence&#34;&gt;2018&lt;/a&gt;)&lt;/span&gt; was used (Section &lt;a href=&#34;chap-ling-comp.html#subsubchap:pc&#34;&gt;1.3.2&lt;/a&gt;). Sentences in the corpus have uniformly-distributed lengths ranging between 10 and 35 tokens. Each sentence is associated with 20 ratings of perceived-complexity on a 1-to-7 point scale. Duplicates and sentences for which less than half of the annotators agreed on a score in the range &lt;span class=&#34;math inline&#34;&gt;\(\mu_n \pm \sigma_n\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\mu_n\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma_n\)&lt;/span&gt; are respectively the average and standard deviation of all annotators’ judgments for sentence &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; were removed to reduce noise coming from the annotation procedure. Again, scores are averaged across annotators to obtain a single metric for each sentence.&lt;/p&gt;
&lt;p&gt;Table &lt;a href=&#34;chap-ex1.html#tab:ex1-stats&#34;&gt;3.1&lt;/a&gt; presents an overview of the two corpora after preprocessing. The resulting eye-tracking (ET) corpus contains roughly four times more sentences than the perceived complexity (PC) one, with shorter words and sentences on average. The differences in sizes and domains between the two corpora account for multi-genre linguistic phenomena in the following analysis.&lt;/p&gt;
&lt;table class=&#34;table&#34; style=&#34;font-size: 11px; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;caption style=&#34;font-size: initial !important;&#34;&gt;
&lt;span id=&#34;tab:ex1-stats&#34;&gt;Table 3.1: &lt;/span&gt;Descriptive statistics of the two sentence-level corpora after the preprocessing procedure.
&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;font-weight: bold;&#34;&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:center;font-weight: bold;&#34;&gt;
Perceived Complexity
&lt;/th&gt;
&lt;th style=&#34;text-align:center;font-weight: bold;&#34;&gt;
Eye-tracking (GECO)
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
labels
&lt;/td&gt;
&lt;td style=&#34;text-align:center;width: 10em; &#34;&gt;
PC
&lt;/td&gt;
&lt;td style=&#34;text-align:center;width: 12em; &#34;&gt;
FPD, FXC, TFD, TRD
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
domain(s)
&lt;/td&gt;
&lt;td style=&#34;text-align:center;width: 10em; &#34;&gt;
financial news
&lt;/td&gt;
&lt;td style=&#34;text-align:center;width: 12em; &#34;&gt;
literature
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
aggregation steps
&lt;/td&gt;
&lt;td style=&#34;text-align:center;width: 10em; &#34;&gt;
avg. annotators
&lt;/td&gt;
&lt;td style=&#34;text-align:center;width: 12em; &#34;&gt;
sentence sum-aggregation + avg. participants
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
filtering steps
&lt;/td&gt;
&lt;td style=&#34;text-align:center;width: 10em; &#34;&gt;
filtering by agreement + remove duplicates
&lt;/td&gt;
&lt;td style=&#34;text-align:center;width: 12em; &#34;&gt;
min. length &amp;gt; 5
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
# of sentences
&lt;/td&gt;
&lt;td style=&#34;text-align:center;width: 10em; &#34;&gt;
1115
&lt;/td&gt;
&lt;td style=&#34;text-align:center;width: 12em; &#34;&gt;
4041
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
# of tokens
&lt;/td&gt;
&lt;td style=&#34;text-align:center;width: 10em; &#34;&gt;
21723
&lt;/td&gt;
&lt;td style=&#34;text-align:center;width: 12em; &#34;&gt;
52131
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
avg. sent. length
&lt;/td&gt;
&lt;td style=&#34;text-align:center;width: 10em; &#34;&gt;
19.48
&lt;/td&gt;
&lt;td style=&#34;text-align:center;width: 12em; &#34;&gt;
12.9
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
avg. token length
&lt;/td&gt;
&lt;td style=&#34;text-align:center;width: 10em; &#34;&gt;
4.95
&lt;/td&gt;
&lt;td style=&#34;text-align:center;width: 12em; &#34;&gt;
4.6
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr grouplength=&#34;6&#34;&gt;
&lt;td colspan=&#34;3&#34; style=&#34;border-bottom: 1px solid;&#34;&gt;
&lt;strong&gt;Length-binned subsets (# of sentences)&lt;/strong&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left; padding-left: 2em;&#34; indentlevel=&#34;1&#34;&gt;
Bin 10±1 size
&lt;/td&gt;
&lt;td style=&#34;text-align:center;width: 10em; &#34;&gt;
173
&lt;/td&gt;
&lt;td style=&#34;text-align:center;width: 12em; &#34;&gt;
899
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left; padding-left: 2em;&#34; indentlevel=&#34;1&#34;&gt;
Bin 15±1 size
&lt;/td&gt;
&lt;td style=&#34;text-align:center;width: 10em; &#34;&gt;
163
&lt;/td&gt;
&lt;td style=&#34;text-align:center;width: 12em; &#34;&gt;
568
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left; padding-left: 2em;&#34; indentlevel=&#34;1&#34;&gt;
Bin 20±1 size
&lt;/td&gt;
&lt;td style=&#34;text-align:center;width: 10em; &#34;&gt;
164
&lt;/td&gt;
&lt;td style=&#34;text-align:center;width: 12em; &#34;&gt;
341
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left; padding-left: 2em;&#34; indentlevel=&#34;1&#34;&gt;
Bin 25±1 size
&lt;/td&gt;
&lt;td style=&#34;text-align:center;width: 10em; &#34;&gt;
151
&lt;/td&gt;
&lt;td style=&#34;text-align:center;width: 12em; &#34;&gt;
215
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left; padding-left: 2em;&#34; indentlevel=&#34;1&#34;&gt;
Bin 30±1 size
&lt;/td&gt;
&lt;td style=&#34;text-align:center;width: 10em; &#34;&gt;
165
&lt;/td&gt;
&lt;td style=&#34;text-align:center;width: 12em; &#34;&gt;
131
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left; padding-left: 2em;&#34; indentlevel=&#34;1&#34;&gt;
Bin 35±1 size
&lt;/td&gt;
&lt;td style=&#34;text-align:center;width: 10em; &#34;&gt;
147
&lt;/td&gt;
&lt;td style=&#34;text-align:center;width: 12em; &#34;&gt;
63
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;subchap:ex1-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;3.2&lt;/span&gt; Analysis of Linguistic Phenomena&lt;/h2&gt;
&lt;p&gt;As a first step to investigate the connection between the two complexity paradigms, the correlation of online and offline complexity labels with various linguistic phenomena is evaluated. The Profiling-UD tool &lt;span class=&#34;citation&#34;&gt;(Brunato et al. &lt;a href=&#34;#ref-brunato-etal-2020-profiling&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt; introduced in Section &lt;a href=&#34;chap-ling-comp.html#subsubchap:structural&#34;&gt;1.2.1&lt;/a&gt; is used to annotate each sentence in our corpora and extract from it ~100 features representing their linguistic structure according to the Universal Dependencies formalism &lt;span class=&#34;citation&#34;&gt;(Nivre et al. &lt;a href=&#34;#ref-nivre-etal-2016-universal&#34;&gt;2016&lt;/a&gt;)&lt;/span&gt;. These features capture a comprehensive set of phenomena, from basic information (e.g. sentence and word length) to more complex aspects of sentence structure (e.g. parse tree depth, verb arity), including properties related to sentence complexity at different levels of description. A summary of the most relevant features is presented in Appendix &lt;a href=&#34;app-ling-feats.html#app:ling-feats&#34;&gt;A&lt;/a&gt;. Features are ranked using their Spearman’s correlation score with complexity metrics, and scores are leveraged to highlight the relation between linguistic phenomena and complexity paradigms.&lt;/p&gt;

&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:feat-heatmap&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;figures/3_feat_heatmap.png&#34; alt=&#34;Ranking of the most correlated linguistic features for selected metrics. All of Spearman’s correlation coefficients have \(p&amp;lt;0.001\).&#34; width=&#34;50%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 3.1: Ranking of the most correlated linguistic features for selected metrics. All of Spearman’s correlation coefficients have &lt;span class=&#34;math inline&#34;&gt;\(p&amp;lt;0.001\)&lt;/span&gt;.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The correlation scores analysis highlights how features showing a significant correlation with eye-tracking metrics are twice as many as those correlating with PC scores and generally tend to have higher coefficients, except for the total regression duration (TRD) metric. Nevertheless, the most correlated features are the same across all metrics. Figure &lt;a href=&#34;chap-ex1.html#fig:feat-heatmap&#34;&gt;3.1&lt;/a&gt; reports correlation scores for features showing a strong connection (&lt;span class=&#34;math inline&#34;&gt;\(|\rho|&amp;gt;0.3\)&lt;/span&gt;) with at least one of the evaluated metrics. As expected, sentence length (&lt;em&gt;n_tokens&lt;/em&gt;) and other related features capturing structural complexity aspects occupy the top positions in the ranking. Among those, we can note the length of dependency links (&lt;em&gt;max_links_len, avg_links_len&lt;/em&gt;) and the depth of the whole parse tree or selected sub-trees, i.e. nominal chains headed by a preposition (&lt;em&gt;parse_depth, n_prep_chains&lt;/em&gt;).
Similarly, the distribution of subordinate clauses (&lt;em&gt;sub_prop_dist, sub_post&lt;/em&gt;) is positively correlated with all metrics but with a more substantial effect for eye-tracking ones, especially in the presence of longer embedded chains (&lt;em&gt;sub_chain_len&lt;/em&gt;).
Interestingly, the presence of numbers (&lt;em&gt;upos_NUM, dep_nummod&lt;/em&gt;) affects only the offline perception of complexity, while it is never strongly correlated with all eye-tracking metrics. This finding is expected since numbers are very short tokens and, like other functional POS, were never found to be strongly correlated with online reading in our results. Conversely, numerical information has been identified as a factor hampering sentence readability and understanding &lt;span class=&#34;citation&#34;&gt;(Rello et al. &lt;a href=&#34;#ref-rello-etal-2013-one&#34;&gt;2013&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;div id=&#34;subsubchap:ex1-analysis-bins&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;header-section-number&#34;&gt;3.2.1&lt;/span&gt; Linguistic Phenomena in Length-controlled Bins&lt;/h3&gt;
&lt;p&gt;Unsurprisingly, sentence length is the most correlated predictor for all complexity metrics. Since many linguistic features highlighted in our analysis are strongly related to sentence length, we tested whether they maintain a relevant influence when this parameter is controlled. To this end, Spearman’s correlation was computed between features and complexity tasks, but this time considering bins of sentences having approximately the same length. Specifically, we split each corpus into six bins of sentences with 10, 15, 20, 25, 30, and 35 tokens, respectively, with a range of ±1 tokens per bin to select a reasonable number of sentences for our analysis. Resulting subsets have a relatively constant size for the PC corpus, which was constructed ad-hoc to have such uniform length distribution, but have a sharply decreasing size for the eye-tracking corpus (see Table &lt;a href=&#34;chap-ex1.html#tab:ex1-stats&#34;&gt;3.1&lt;/a&gt;, bott. While deemed appropriate in the context of this correlation analysis, the disparity in bin sizes may play a significant role in hampering the performances of models trained on binned linguistic complexity data. This perspective is discussed in Section &lt;a href=&#34;chap-ex1.html#subchap:ex1-modeling&#34;&gt;3.3&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:feat-bin-heatmap&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;figures/3_feat_bin_heatmap.png&#34; alt=&#34;Rankings of the most correlated linguistic features for metrics within length-binned subsets of the two corpora. Squares show the correlation between features (left axis) and a complexity metric (top) at a specific bin of length (bottom). Coefficients \(\geq\) 0.2 or \(\leq\) -0.2 are highlighted, and have \(p&amp;lt;0.001\).&#34; width=&#34;95%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 3.2: Rankings of the most correlated linguistic features for metrics within length-binned subsets of the two corpora. Squares show the correlation between features (left axis) and a complexity metric (top) at a specific bin of length (bottom). Coefficients &lt;span class=&#34;math inline&#34;&gt;\(\geq\)&lt;/span&gt; 0.2 or &lt;span class=&#34;math inline&#34;&gt;\(\leq\)&lt;/span&gt; -0.2 are highlighted, and have &lt;span class=&#34;math inline&#34;&gt;\(p&amp;lt;0.001\)&lt;/span&gt;.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Figure &lt;a href=&#34;chap-ex1.html#fig:feat-bin-heatmap&#34;&gt;3.2&lt;/a&gt; reports the new rankings of the most correlated linguistic features within each bin across complexity metrics (&lt;span class=&#34;math inline&#34;&gt;\(|\rho| &amp;gt; 0.2\)&lt;/span&gt;). Again, we observe that features showing a significant correlation with complexity scores are fewer for PC bins than for eye-tracking ones. This fact depends on controlling for sentence length and the small size of bins for the whole dataset. As in the coarse-grained analysis, TRD is the eye-tracking metric less correlated to linguistic features, while the other three (FXC, FPD, TFD) show a homogeneous behavior across bins. For the latter, vocabulary-related features (token-type ratio, average word length, lexical density) are always positive and top-ranked in all bins, especially when considering shorter sentences (i.e. from 10 to 20 tokens). For PC, this is true only for some of them (word length and lexical density). On another note, features encoding numerical information are still highly correlated with the offline perception of complexity in almost all bins.&lt;/p&gt;
&lt;p&gt;Interestingly, features modeling subordination phenomena extracted from fixed-length sentences exhibit a reverse trend than when extracted from the whole corpus, i.e. they are negatively correlated with judgments. If, on the one hand, an increase in the presence of subordination for longer sentences (possibly making sentences more convoluted) was expected, on the other hand, when the length is controlled, findings suggest that subordinate structures are not necessarily perceived as a symptom of sentence complexity.&lt;/p&gt;
&lt;p&gt;The analysis also highlights how linguistic features relevant to online and offline complexity are different when controlling for sentence length. This aspect, in particular, was not evident from the previous coarse-grained analysis. Despite blocking sentence length, gaze measures are still significantly connected to length-related phenomena (high correlation with &lt;em&gt;n_tokens&lt;/em&gt; at various length bins). This observation can be possibly due to the ±1 margin applied for sentence selection and the high sensitivity of behavioral metrics to small input changes.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;subchap:ex1-modeling&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;3.3&lt;/span&gt; Modeling Online and Offline Linguistic Complexity&lt;/h2&gt;
&lt;p&gt;Given the high correlations reported above, the next step involves quantifying the importance of explicit linguistic features from a modeling standpoint. Table &lt;a href=&#34;chap-ex1.html#tab:ex1-results&#34;&gt;3.2&lt;/a&gt; presents the RMSE and &lt;span class=&#34;math inline&#34;&gt;\(R^2\)&lt;/span&gt; scores of predictions made by baselines and models for the selected complexity metrics. Performances are tested with a 5-fold cross-validation regression with a fixed random seed on each metric. Our baselines use average metric scores of all training sentences (&lt;em&gt;Avg. score&lt;/em&gt;) and average scores of sentences binned by their length, expressed in number of tokens, as predictions (&lt;em&gt;Bin average&lt;/em&gt;). The two linear SVM models leverage explicit linguistic features, using respectively only the &lt;em&gt;n_tokens&lt;/em&gt; feature (&lt;em&gt;SVM length&lt;/em&gt;) and the whole set of linguistic features presented above (&lt;em&gt;SVM feats&lt;/em&gt;). Besides those, the performances of a state-of-the-art Transformer neural language model relying entirely on contextual word embeddings are equally tested. &lt;em&gt;ALBERT&lt;/em&gt; (&lt;span class=&#34;citation&#34;&gt;Lan et al. (&lt;a href=&#34;#ref-lan-etal-2020-albert&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt;; see Section &lt;a href=&#34;chap-models.html#subchap:nlm&#34;&gt;2.2&lt;/a&gt;) as a lightweight yet effective alternative to BERT &lt;span class=&#34;citation&#34;&gt;(Devlin et al. &lt;a href=&#34;#ref-devlin-etal-2019-bert&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt; for obtaining contextual word representations, using its last-layer &lt;code&gt;[CLS]&lt;/code&gt; sentence embedding as input for a linear regressor during fine-tuning and testing. We selected the last layer representations, despite strong evidence on the importance of intermediate representation in encoding language properties, because we aim to investigate how superficial layers encode complexity-related competence. Given the availability of parallel eye-tracking annotations, we train ALBERT using multitask learning with hard parameter sharing &lt;span class=&#34;citation&#34;&gt;(Caruana &lt;a href=&#34;#ref-caruana-1997-multitask&#34;&gt;1997&lt;/a&gt;)&lt;/span&gt; on gaze metrics.&lt;a href=&#34;#fn17&#34; class=&#34;footnote-ref&#34; id=&#34;fnref17&#34;&gt;&lt;sup&gt;17&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;table class=&#34;table&#34; style=&#34;font-size: 11px; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;caption style=&#34;font-size: initial !important;&#34;&gt;
&lt;span id=&#34;tab:ex1-results&#34;&gt;Table 3.2: &lt;/span&gt;Average Root-Mean-Square Error (&lt;span class=&#34;math inline&#34;&gt;\(\sqrt{E^2}\)&lt;/span&gt;) and &lt;span class=&#34;math inline&#34;&gt;\(R^2\)&lt;/span&gt; score values for sentence-level complexity predictions using 5-fold cross-validation. Lower &lt;span class=&#34;math inline&#34;&gt;\(\sqrt{E^2}\)&lt;/span&gt; and higher &lt;span class=&#34;math inline&#34;&gt;\(R^2\)&lt;/span&gt; are better.
&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;border-bottom:hidden&#34; colspan=&#34;1&#34;&gt;
&lt;/th&gt;
&lt;th style=&#34;border-bottom:hidden; padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; font-weight: bold; &#34; colspan=&#34;2&#34;&gt;
&lt;div style=&#34;border-bottom: 1px solid #ddd; padding-bottom: 5px; &#34;&gt;
PC
&lt;/div&gt;
&lt;/th&gt;
&lt;th style=&#34;border-bottom:hidden; padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; font-weight: bold; &#34; colspan=&#34;2&#34;&gt;
&lt;div style=&#34;border-bottom: 1px solid #ddd; padding-bottom: 5px; &#34;&gt;
FXC
&lt;/div&gt;
&lt;/th&gt;
&lt;th style=&#34;border-bottom:hidden; padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; font-weight: bold; &#34; colspan=&#34;2&#34;&gt;
&lt;div style=&#34;border-bottom: 1px solid #ddd; padding-bottom: 5px; &#34;&gt;
FPD
&lt;/div&gt;
&lt;/th&gt;
&lt;th style=&#34;border-bottom:hidden; padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; font-weight: bold; &#34; colspan=&#34;2&#34;&gt;
&lt;div style=&#34;border-bottom: 1px solid #ddd; padding-bottom: 5px; &#34;&gt;
TFD
&lt;/div&gt;
&lt;/th&gt;
&lt;th style=&#34;border-bottom:hidden; padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; font-weight: bold; &#34; colspan=&#34;2&#34;&gt;
&lt;div style=&#34;border-bottom: 1px solid #ddd; padding-bottom: 5px; &#34;&gt;
TRD
&lt;/div&gt;
&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\sqrt{E^2}\)&lt;/span&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(R^2\)&lt;/span&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\sqrt{E^2}\)&lt;/span&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(R^2\)&lt;/span&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\sqrt{E^2}\)&lt;/span&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(R^2\)&lt;/span&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\sqrt{E^2}\)&lt;/span&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(R^2\)&lt;/span&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\sqrt{E^2}\)&lt;/span&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(R^2\)&lt;/span&gt;
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr grouplength=&#34;2&#34;&gt;
&lt;td colspan=&#34;11&#34; style=&#34;border-bottom: 1px solid;&#34;&gt;
&lt;strong&gt;Statistical baselines&lt;/strong&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;width: 10em;  padding-left: 2em;&#34; indentlevel=&#34;1&#34;&gt;
Avg. score
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.87&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;6.17&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.06&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;1078&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.06&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;1297&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.06&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;540&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.03&lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;width: 10em;  padding-left: 2em;&#34; indentlevel=&#34;1&#34;&gt;
Bin average
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.53&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.62&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;2.36&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.86&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;374&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.89&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;532&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.85&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;403&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.45&lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr grouplength=&#34;2&#34;&gt;
&lt;td colspan=&#34;11&#34; style=&#34;border-bottom: 1px solid;&#34;&gt;
&lt;strong&gt;Explicit features&lt;/strong&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;width: 10em;  padding-left: 2em;&#34; indentlevel=&#34;1&#34;&gt;
SVM length
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.54&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.62&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;2.19&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.88&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;343&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.9&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;494&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.86&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;405&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.45&lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;width: 10em;  padding-left: 2em;&#34; indentlevel=&#34;1&#34;&gt;
SVM feats
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34; font-weight: bold;    &#34;&gt;0.44&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.74&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34; font-weight: bold;    &#34;&gt;1.77&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34; font-weight: bold;    &#34;&gt;0.92&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34; font-weight: bold;    &#34;&gt;287&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34; font-weight: bold;    &#34;&gt;0.93&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34; font-weight: bold;    &#34;&gt;435&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34; font-weight: bold;    &#34;&gt;0.92&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;400&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.46&lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr grouplength=&#34;1&#34;&gt;
&lt;td colspan=&#34;11&#34; style=&#34;border-bottom: 1px solid;&#34;&gt;
&lt;strong&gt;Learned representations&lt;/strong&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;width: 10em;  padding-left: 2em;&#34; indentlevel=&#34;1&#34;&gt;
ALBERT
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34; font-weight: bold;    &#34;&gt;0.44&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34; font-weight: bold;    &#34;&gt;0.75&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;1.98&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34; font-weight: bold;    &#34;&gt;0.92&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;302&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34; font-weight: bold;    &#34;&gt;0.93&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34; font-weight: bold;    &#34;&gt;435&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.9&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34; font-weight: bold;    &#34;&gt;382&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34; font-weight: bold;    &#34;&gt;0.49&lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;From Table &lt;a href=&#34;chap-ex1.html#tab:ex1-results&#34;&gt;3.2&lt;/a&gt; it can be noted that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The length-binned average baseline is very effective in predicting complexity scores and gaze metrics, which is unsurprising given the extreme correlation between length and complexity metrics presented in Figure &lt;a href=&#34;chap-ex1.html#fig:feat-heatmap&#34;&gt;3.1&lt;/a&gt;;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The &lt;em&gt;SVM feats&lt;/em&gt; model shows considerable improvements if compared to the length-only SVM model for all complexity metrics, highlighting how length alone accounts for much but not for the entirety of variance in complexity scores;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;ALBERT performs on-par with the SVM feats model on all complexity metrics despite the small dimension of the fine-tuning corpora and the absence of explicit linguistic information.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A possible interpretation of ALBERT’s strong performances is that the model implicitly develops competence related to phenomena encoded by linguistic features while training on online and offline complexity prediction. We explore this perspective in Section &lt;a href=&#34;chap-ex1.html#subchap:ex1-probing&#34;&gt;3.4&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;subsubchap:ex1-modeling-bins&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;header-section-number&#34;&gt;3.3.1&lt;/span&gt; Modeling Complexity in Length-controlled Bins&lt;/h3&gt;

&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:models-bin-scores&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;figures/3_models_bin_scores.png&#34; alt=&#34;Average Root-Mean-Square Error (RMSE) scores for models in Table 3.2, performing 5-fold cross-validation on the length-binned subsets used for Figure 3.2. Lower scores are better.&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 3.3: Average Root-Mean-Square Error (RMSE) scores for models in Table &lt;a href=&#34;chap-ex1.html#tab:ex1-results&#34;&gt;3.2&lt;/a&gt;, performing 5-fold cross-validation on the length-binned subsets used for Figure &lt;a href=&#34;chap-ex1.html#fig:feat-bin-heatmap&#34;&gt;3.2&lt;/a&gt;. Lower scores are better.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Similarly to the approach adopted in Section &lt;a href=&#34;chap-ex1.html#subsubchap:ex1-analysis-bins&#34;&gt;3.2.1&lt;/a&gt;, the performances of models are tested on length-binned data to verify their consistency in the context of length-controlled sequences. Figure &lt;a href=&#34;chap-ex1.html#fig:models-bin-scores&#34;&gt;3.3&lt;/a&gt; presents RMSE scores averaged with 5-fold cross-validation over the length-binned sentences subsets for all complexity metrics. It can be observed that ALBERT outperforms the SVM with linguistic features on nearly all bins and metrics, showing the largest gains on intermediate bins for PC and gaze durations (FPD, TFD, TRD). Interestingly, models’ overall performances follow a length-dependent increasing trend for eye-tracking metrics, but not for PC. This behavior can be possibly explained in terms of the high sensibility to length previously highlighted for online metrics, as well as the broad variability in bin dimensions. It can also be observed how the SVM model based on explicit linguistic features (&lt;em&gt;SVM feats&lt;/em&gt;) performs poorly on larger bins for all tasks, sometimes being even worse than the bin-average baseline. While this behavior seems surprising given the positive influence of features highlighted in Table &lt;a href=&#34;chap-ex1.html#tab:ex1-results&#34;&gt;3.2&lt;/a&gt;, this phenomenon can be attributed to the small dimension of longer bins, which negatively impacts the generalization capabilities of the regressor. The relatively better scores achieved by ALBERT in those, instead, support the effectiveness of information stored in pretrained language representations when a limited number of examples are available.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;subchap:ex1-probing&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;3.4&lt;/span&gt; Probing Linguistic Phenomena in ALBERT Representations&lt;/h2&gt;
&lt;p&gt;As shown in the previous section, ALBERT performances in complexity predictions are comparable to those of an SVM relying on explicit linguistic features and even better than those when controlling for length. The &lt;em&gt;probing task&lt;/em&gt; interpretability paradigm (Section &lt;a href=&#34;chap-models.html#subsubchap:probe&#34;&gt;2.3.1&lt;/a&gt;) is adopted to investigate if ALBERT encodes the linguistic knowledge that we identified as strongly correlated with online and perceived sentence complexity during training and prediction. In particular, the aim of this investigation is two-fold:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Probing ALBERT’s innate competence in relation to the broad spectrum of linguistic features described in Appendix &lt;a href=&#34;app-ling-feats.html#app:ling-feats&#34;&gt;A&lt;/a&gt;; and&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Verifying whether, and in which respect, this competence is affected by a fine-tuning process on the complexity assessment metrics.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Three UD English treebanks spanning different textual genres – &lt;strong&gt;EWT, GUM, and ParTUT&lt;/strong&gt; respectively by &lt;span class=&#34;citation&#34;&gt;Silveira et al. (&lt;a href=&#34;#ref-silveira-etal-2014-gold&#34;&gt;2014&lt;/a&gt;)&lt;/span&gt;, &lt;span class=&#34;citation&#34;&gt;Zeldes (&lt;a href=&#34;#ref-zeldes-2017-gum&#34;&gt;2017&lt;/a&gt;)&lt;/span&gt;, and &lt;span class=&#34;citation&#34;&gt;Sanguinetti and Bosco (&lt;a href=&#34;#ref-sanguinetti-etal-2015-partut&#34;&gt;2015&lt;/a&gt;)&lt;/span&gt; – were aggregated, obtaining a final corpus of 18,079 sentences with gold linguistic information which was used to conduct probing experiments. The Profiling-UD tool was again leveraged to extract &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; sentence-level linguistic features &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{Z}=z_1, \dots, z_n\)&lt;/span&gt; from gold linguistic annotations. Representations &lt;span class=&#34;math inline&#34;&gt;\(A(x)\)&lt;/span&gt; were generated for all corpus sentences using the last-layer &lt;code&gt;[CLS]&lt;/code&gt; embedding of a pretrained ALBERT base model without additional fine-tuning, and &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; single-layer perceptron regressors &lt;span class=&#34;math inline&#34;&gt;\(g_i: A(x) \rightarrow z_i\)&lt;/span&gt; are trained to map representations &lt;span class=&#34;math inline&#34;&gt;\(A(x)\)&lt;/span&gt; to each linguistic feature &lt;span class=&#34;math inline&#34;&gt;\(z_i\)&lt;/span&gt;. Finally, the error and &lt;span class=&#34;math inline&#34;&gt;\(R^2\)&lt;/span&gt; scores of each &lt;span class=&#34;math inline&#34;&gt;\(g_i\)&lt;/span&gt; were evaluated as proxies for the quality of representations &lt;span class=&#34;math inline&#34;&gt;\(A(x)\)&lt;/span&gt; in encoding their respective linguistic feature &lt;span class=&#34;math inline&#34;&gt;\(z_i\)&lt;/span&gt;. The same evaluation is repeated for ALBERTs fine-tuned respectively on perceived complexity labels (PC) and on all eye-tracking labels with multitask learning (ET), averaging scores with 5-fold cross-validation. A selected subset of results is shown on the left side of Table &lt;a href=&#34;chap-ex1.html#tab:probes&#34;&gt;3.3&lt;/a&gt;.&lt;/p&gt;
&lt;table class=&#34;table&#34; style=&#34;font-size: 11px; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;caption style=&#34;font-size: initial !important;&#34;&gt;
&lt;span id=&#34;tab:probes&#34;&gt;Table 3.3: &lt;/span&gt;Root MSE (&lt;span class=&#34;math inline&#34;&gt;\(\sqrt{E^2}\)&lt;/span&gt;) and &lt;span class=&#34;math inline&#34;&gt;\(R^2\)&lt;/span&gt; scores for diagnostic regressors trained on ALBERT representations, respectively, without fine-tuning (Base), with PC and eye-tracking (ET) fine-tuning on all data (left) and on the &lt;span class=&#34;math inline&#34;&gt;\(10 \pm 1\)&lt;/span&gt; length-binned subset (right).  values highlight relevant increases in &lt;span class=&#34;math inline&#34;&gt;\(R^2\)&lt;/span&gt; from Base.
&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;border-bottom:hidden&#34; colspan=&#34;1&#34;&gt;
&lt;/th&gt;
&lt;th style=&#34;border-bottom:hidden; padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; font-weight: bold; &#34; colspan=&#34;2&#34;&gt;
&lt;div style=&#34;border-bottom: 1px solid #ddd; padding-bottom: 5px; &#34;&gt;
Base
&lt;/div&gt;
&lt;/th&gt;
&lt;th style=&#34;border-bottom:hidden; padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; font-weight: bold; &#34; colspan=&#34;2&#34;&gt;
&lt;div style=&#34;border-bottom: 1px solid #ddd; padding-bottom: 5px; &#34;&gt;
PC
&lt;/div&gt;
&lt;/th&gt;
&lt;th style=&#34;border-bottom:hidden; padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; font-weight: bold; &#34; colspan=&#34;2&#34;&gt;
&lt;div style=&#34;border-bottom: 1px solid #ddd; padding-bottom: 5px; &#34;&gt;
ET
&lt;/div&gt;
&lt;/th&gt;
&lt;th style=&#34;border-bottom:hidden; padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; font-weight: bold; &#34; colspan=&#34;2&#34;&gt;
&lt;div style=&#34;border-bottom: 1px solid #ddd; padding-bottom: 5px; &#34;&gt;
PC10±1
&lt;/div&gt;
&lt;/th&gt;
&lt;th style=&#34;border-bottom:hidden; padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; font-weight: bold; &#34; colspan=&#34;2&#34;&gt;
&lt;div style=&#34;border-bottom: 1px solid #ddd; padding-bottom: 5px; &#34;&gt;
ET10±1
&lt;/div&gt;
&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\sqrt{E^2}\)&lt;/span&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(R^2\)&lt;/span&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\sqrt{E^2}\)&lt;/span&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(R^2\)&lt;/span&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\sqrt{E^2}\)&lt;/span&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(R^2\)&lt;/span&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\sqrt{E^2}\)&lt;/span&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(R^2\)&lt;/span&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\sqrt{E^2}\)&lt;/span&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
&lt;span class=&#34;math inline&#34;&gt;\(R^2\)&lt;/span&gt;
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
n_tokens
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
8.19
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.26
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
4.66
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34; font-weight: bold;    &#34;&gt;0.76&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2.87
&lt;/td&gt;
&lt;td style=&#34;text-align:center;border-right:1px solid;&#34;&gt;
&lt;span style=&#34; font-weight: bold;    &#34;&gt;0.91&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
8.66
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.18&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
6.71
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34; font-weight: bold;    &#34;&gt;0.51&lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
parse_depth
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1.47
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.18
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1.18
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34; font-weight: bold;    &#34;&gt;0.48&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1.04
&lt;/td&gt;
&lt;td style=&#34;text-align:center;border-right:1px solid;&#34;&gt;
&lt;span style=&#34; font-weight: bold;    &#34;&gt;0.6&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1.50
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.16&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1.22
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34; font-weight: bold;    &#34;&gt;0.43&lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
vb_head_per_sent
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1.38
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.15
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1.26
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34; font-weight: bold;    &#34;&gt;0.3&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1.14
&lt;/td&gt;
&lt;td style=&#34;text-align:center;border-right:1px solid;&#34;&gt;
&lt;span style=&#34; font-weight: bold;    &#34;&gt;0.42&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1.44
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.09&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1.30
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34; font-weight: bold;    &#34;&gt;0.25&lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
xpos_dist_.
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.05
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.13
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.04
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34; font-weight: bold;    &#34;&gt;0.41&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.04
&lt;/td&gt;
&lt;td style=&#34;text-align:center;border-right:1px solid;&#34;&gt;
&lt;span style=&#34; font-weight: bold;    &#34;&gt;0.42&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.04
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.18&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.04
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34; font-weight: bold;    &#34;&gt;0.38&lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
avg_links_len
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.58
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.12
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.53
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34; font-weight: bold;    &#34;&gt;0.29&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.52
&lt;/td&gt;
&lt;td style=&#34;text-align:center;border-right:1px solid;&#34;&gt;
&lt;span style=&#34; font-weight: bold;    &#34;&gt;0.31&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.59
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.1&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.56
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34; font-weight: bold;    &#34;&gt;0.2&lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
max_links_len
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
5.20
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.12
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
4.08
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34; font-weight: bold;    &#34;&gt;0.46&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
3.75
&lt;/td&gt;
&lt;td style=&#34;text-align:center;border-right:1px solid;&#34;&gt;
&lt;span style=&#34; font-weight: bold;    &#34;&gt;0.54&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
5.24
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.11&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
4.73
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34; font-weight: bold;    &#34;&gt;0.28&lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
n_prep_chains
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.74
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.11
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.67
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34; font-weight: bold;    &#34;&gt;0.26&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.66
&lt;/td&gt;
&lt;td style=&#34;text-align:center;border-right:1px solid;&#34;&gt;
&lt;span style=&#34; font-weight: bold;    &#34;&gt;0.29&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.72
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.14&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.69
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34; font-weight: bold;    &#34;&gt;0.21&lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
sub_prop_dist
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.35
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.09
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.33
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.13&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.31
&lt;/td&gt;
&lt;td style=&#34;text-align:center;border-right:1px solid;&#34;&gt;
&lt;span style=&#34; font-weight: bold;    &#34;&gt;0.22&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.34
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.05&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.32
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.15&lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
upos_dist_PRON
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.08
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.09
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.08
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.14&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.08
&lt;/td&gt;
&lt;td style=&#34;text-align:center;border-right:1px solid;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.07&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.07
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34; font-weight: bold;    &#34;&gt;0.23&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.08
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.15&lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
pos_dist_NUM
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.05
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.08
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.05
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.06&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.05
&lt;/td&gt;
&lt;td style=&#34;text-align:center;border-right:1px solid;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.02&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.05
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34; font-weight: bold;    &#34;&gt;0.16&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.05
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.06&lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
dep_dist_nsubj
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.06
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.08
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.06
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.1&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.06
&lt;/td&gt;
&lt;td style=&#34;text-align:center;border-right:1px solid;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.05&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.05
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34; font-weight: bold;    &#34;&gt;0.17&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.06
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.11&lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
char_per_tok
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.89
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.07
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.87
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.12&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.90
&lt;/td&gt;
&lt;td style=&#34;text-align:center;border-right:1px solid;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.05&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.82
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34; font-weight: bold;    &#34;&gt;0.22&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.86
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.14&lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
prep_chain_len
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.60
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.07
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.57
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34; font-weight: bold;    &#34;&gt;0.17&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.56
&lt;/td&gt;
&lt;td style=&#34;text-align:center;border-right:1px solid;&#34;&gt;
&lt;span style=&#34; font-weight: bold;    &#34;&gt;0.19&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.59
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.12&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.56
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34; font-weight: bold;    &#34;&gt;0.18&lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
sub_chain_len
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.70
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.07
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.67
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34; font-weight: bold;    &#34;&gt;0.15&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.62
&lt;/td&gt;
&lt;td style=&#34;text-align:center;border-right:1px solid;&#34;&gt;
&lt;span style=&#34; font-weight: bold;    &#34;&gt;0.26&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.71
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.04&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.66
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34; font-weight: bold;    &#34;&gt;0.16&lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
dep_dist_punct
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.07
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.06
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.07
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.06&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.07
&lt;/td&gt;
&lt;td style=&#34;text-align:center;border-right:1px solid;&#34;&gt;
&lt;span style=&#34; font-weight: bold;    &#34;&gt;0.14&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.07
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.06&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.07
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34; font-weight: bold;    &#34;&gt;0.14&lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
dep_dist_nmod
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.05
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.06
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.05
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.07&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.05
&lt;/td&gt;
&lt;td style=&#34;text-align:center;border-right:1px solid;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.06&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.05
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.09&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.05
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.09&lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
sub_post
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.44
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.05
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.46
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.12&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.44
&lt;/td&gt;
&lt;td style=&#34;text-align:center;border-right:1px solid;&#34;&gt;
&lt;span style=&#34; font-weight: bold;    &#34;&gt;0.18&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.47
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.05&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.45
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34; font-weight: bold;    &#34;&gt;0.14&lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
dep_dist_case
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.07
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.05
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.06
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.06&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.07
&lt;/td&gt;
&lt;td style=&#34;text-align:center;border-right:1px solid;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.08&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.07
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.07&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.07
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.1&lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
lexical_density
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.14
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.05
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.13
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.03&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.13
&lt;/td&gt;
&lt;td style=&#34;text-align:center;border-right:1px solid;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.03&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.13
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34; font-weight: bold;    &#34;&gt;0.13&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.13
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34; font-weight: bold;    &#34;&gt;0.13&lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
dep_dist_compound
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.06
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.04
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.06
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.05&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.06
&lt;/td&gt;
&lt;td style=&#34;text-align:center;border-right:1px solid;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.03&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.06
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.1&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.06
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.07&lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
dep_dist_conj
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.04
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.03
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.04
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.04&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.04
&lt;/td&gt;
&lt;td style=&#34;text-align:center;border-right:1px solid;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.04&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.05
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.02&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.04
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.03&lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
ttr_form
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.08
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.03
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.08
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.05&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.08
&lt;/td&gt;
&lt;td style=&#34;text-align:center;border-right:1px solid;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.05&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.08
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.05&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.08
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.05&lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
dep_dist_det
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.06
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.03
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.06
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.02&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.06
&lt;/td&gt;
&lt;td style=&#34;text-align:center;border-right:1px solid;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.04&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.06
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.03&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.06
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.03&lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
dep_dist_aux
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.04
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.02
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.04
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.01&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.04
&lt;/td&gt;
&lt;td style=&#34;text-align:center;border-right:1px solid;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.01&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.04
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.06&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.04
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.04&lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
pos_dist_VBN
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.03
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.01
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.03
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.03
&lt;/td&gt;
&lt;td style=&#34;text-align:center;border-right:1px solid;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.03
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.01&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.03
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0&lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
xpos_dist_VBZ
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.04
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.01
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.04
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.01&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.04
&lt;/td&gt;
&lt;td style=&#34;text-align:center;border-right:1px solid;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.02&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.04
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.02&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.04
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.02&lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
ttr_lemma
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.09
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.01
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.09
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.06&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.09
&lt;/td&gt;
&lt;td style=&#34;text-align:center;border-right:1px solid;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.06&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.09
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.04&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.09
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.03&lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;As it can be observed, ALBERT’s last-layer sentence representations have relatively low knowledge of complexity-related probes, but their performances highly increase after fine-tuning. Specifically, a noticeable improvement was obtained on features that were already better encoded in base pretrained representation, i.e. sentence length and related, suggesting that fine-tuning possibly accentuates only properties already well-known by the model, regardless of the target task. To verify that this isn’t the case, the same probing tests were repeated on ALBERT models fine-tuned on the smallest length-binned subset (i.e. &lt;span class=&#34;math inline&#34;&gt;\(10\pm1\)&lt;/span&gt; tokens) presented in previous sections. The right side of Table &lt;a href=&#34;chap-ex1.html#tab:probes&#34;&gt;3.3&lt;/a&gt; presents the resulting scores. From the length-binned correlation analysis of Section &lt;a href=&#34;chap-ex1.html#fig:feat-bin-heatmap&#34;&gt;3.2&lt;/a&gt;, PC scores were observed to be mostly uncorrelated with length phenomena, while ET scores remain significantly affected despite our controlling of sequence size. This observation also holds for length-binned probing task results, where the PC model seems to neglect length-related properties in favor of task-specific ones that were also highlighted in our fine-grained correlation analysis (e.g. word length, numbers, explicit subjects). The ET-trained model follows the same behavior, retaining strong but lower performances for length-related features.&lt;/p&gt;
&lt;p&gt;In conclusion, although higher probing task performances after fine-tuning are not direct proof that the neural language model exploits newly-acquired morpho-syntactic and syntactic information, results suggest that training on tasks strongly connected with underlying linguistic structures triggers a change in model representations resulting in a better encoding of related linguistic properties.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;subchap:ex1-summary&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;3.5&lt;/span&gt; Summary&lt;/h2&gt;
&lt;p&gt;In this chapter, the connection between eye-tracking metrics and the offline perception of sentence complexity was investigated from an experimental standpoint. An in-depth correlation analysis was performed between complexity scores and sentence linguistic properties at different granularity levels, highlighting the strong relationship between metrics and length-affine properties and revealing different behaviors when controlling for sentence length. Models using explicit linguistic features and unsupervised word embeddings were evaluated on complexity prediction, showing comparable performances across metrics. Finally, the encoding of linguistic properties in a neural language model’s contextual representations was tested with probing tasks. This approach highlighted the natural emergence of task-related linguistic properties within the model’s representations after the fine-tuning process. Thus, it can be conjectured that a relation subsists between the model’s linguistic abilities during the training procedure and its downstream performances on morphosyntactically-related tasks and that linguistic probes may provide a reasonable estimate of the task-oriented quality of representations.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;h3&gt;References&lt;/h3&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-abnar-etal-2019-blackbox&#34;&gt;
&lt;p&gt;Abnar, Samira, Lisa Beinborn, Rochelle Choenni, and Willem Zuidema. 2019. “Blackbox Meets Blackbox: Representational Similarity &amp;amp; Stability Analysis of Neural Language Models and Brains.” In &lt;em&gt;Proceedings of the 2019 Acl Workshop Blackboxnlp: Analyzing and Interpreting Neural Networks for Nlp&lt;/em&gt;, 191–203. Florence, Italy: Association for Computational Linguistics. &lt;a href=&#34;https://doi.org/10.18653/v1/W19-4820&#34;&gt;https://doi.org/10.18653/v1/W19-4820&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-brunato-etal-2020-profiling&#34;&gt;
&lt;p&gt;Brunato, Dominique, Andrea Cimino, Felice Dell’Orletta, Giulia Venturi, and Simonetta Montemagni. 2020. “Profiling-UD: A Tool for Linguistic Profiling of Texts.” In &lt;em&gt;Proceedings of the 12th Language Resources and Evaluation Conference&lt;/em&gt;, 7145–51. Marseille, France: European Language Resources Association. &lt;a href=&#34;https://www.aclweb.org/anthology/2020.lrec-1.883&#34;&gt;https://www.aclweb.org/anthology/2020.lrec-1.883&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-brunato-etal-2018-sentence&#34;&gt;
&lt;p&gt;Brunato, Dominique, Lorenzo De Mattei, Felice Dell’Orletta, Benedetta Iavarone, and Giulia Venturi. 2018. “Is This Sentence Difficult? Do You Agree?” In &lt;em&gt;Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing&lt;/em&gt;, 2690–9. Brussels, Belgium: Association for Computational Linguistics. &lt;a href=&#34;https://doi.org/10.18653/v1/D18-1289&#34;&gt;https://doi.org/10.18653/v1/D18-1289&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-caruana-1997-multitask&#34;&gt;
&lt;p&gt;Caruana, Rich. 1997. “Multitask Learning.” &lt;em&gt;Machine Learning&lt;/em&gt; 28: 41–75. &lt;a href=&#34;https://www.cs.utexas.edu/~kuipers/readings/Caruana-mlj-97.pdf&#34;&gt;https://www.cs.utexas.edu/~kuipers/readings/Caruana-mlj-97.pdf&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-cop-etal-2017-presenting&#34;&gt;
&lt;p&gt;Cop, Uschi, Nicolas Dirix, Denis Drieghe, and Wouter Duyck. 2017. “Presenting Geco: An Eyetracking Corpus of Monolingual and Bilingual Sentence Reading.” &lt;em&gt;Behavior Research Methods&lt;/em&gt; 49 (2). Springer: 602–15.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-devlin-etal-2019-bert&#34;&gt;
&lt;p&gt;Devlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. “BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding.” In &lt;em&gt;Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)&lt;/em&gt;, 4171–86. Minneapolis, Minnesota: Association for Computational Linguistics. &lt;a href=&#34;https://doi.org/10.18653/v1/N19-1423&#34;&gt;https://doi.org/10.18653/v1/N19-1423&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-gauthier-levy-2019-linking&#34;&gt;
&lt;p&gt;Gauthier, Jon, and Roger Levy. 2019. “Linking Artificial and Human Neural Representations of Language.” In &lt;em&gt;Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (Emnlp-Ijcnlp)&lt;/em&gt;, 529–39. Hong Kong, China: Association for Computational Linguistics. &lt;a href=&#34;https://doi.org/10.18653/v1/D19-1050&#34;&gt;https://doi.org/10.18653/v1/D19-1050&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-hollenstein-etal-2019-cognival&#34;&gt;
&lt;p&gt;Hollenstein, Nora, Antonio de la Torre, Nicolas Langer, and Ce Zhang. 2019. “CogniVal: A Framework for Cognitive Word Embedding Evaluation.” In &lt;em&gt;Proceedings of the 23rd Conference on Computational Natural Language Learning (Conll)&lt;/em&gt;, 538–49. Hong Kong, China: Association for Computational Linguistics. &lt;a href=&#34;https://doi.org/10.18653/v1/K19-1050&#34;&gt;https://doi.org/10.18653/v1/K19-1050&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-lan-etal-2020-albert&#34;&gt;
&lt;p&gt;Lan, Zhenzhong, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. 2020. “ALBERT: A Lite BERT for Self-Supervised Learning of Language Representations.” In &lt;em&gt;International Conference on Learning Representations&lt;/em&gt;. &lt;a href=&#34;https://openreview.net/forum?id=H1eA7AEtvS&#34;&gt;https://openreview.net/forum?id=H1eA7AEtvS&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-nivre-etal-2016-universal&#34;&gt;
&lt;p&gt;Nivre, Joakim, Marie-Catherine de Marneffe, Filip Ginter, Yoav Goldberg, Jan Hajič, Christopher D. Manning, Ryan McDonald, et al. 2016. “Universal Dependencies V1: A Multilingual Treebank Collection.” In &lt;em&gt;Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC’16)&lt;/em&gt;, 1659–66. Portorož, Slovenia: European Language Resources Association (ELRA). &lt;a href=&#34;https://www.aclweb.org/anthology/L16-1262&#34;&gt;https://www.aclweb.org/anthology/L16-1262&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-rello-etal-2013-one&#34;&gt;
&lt;p&gt;Rello, Luz, Susana Bautista, Ricardo Baeza-Yates, Pablo Gervás, Raquel Hervás, and Horacio Saggion. 2013. “One Half or 50%? An Eye-Tracking Study of Number Representation Readability.” In &lt;em&gt;Human-Computer Interaction – Interact 2013&lt;/em&gt;, edited by Paula Kotzé, Gary Marsden, Gitte Lindgaard, Janet Wesson, and Marco Winckler, 229–45. Berlin, Heidelberg: Springer Berlin Heidelberg.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-sanguinetti-etal-2015-partut&#34;&gt;
&lt;p&gt;Sanguinetti, Manuela, and Cristina Bosco. 2015. “PartTUT: The Turin University Parallel Treebank.” In &lt;em&gt;Harmonization and Development of Resources and Tools for Italian Natural Language Processing Within the Parli Project&lt;/em&gt;, edited by Roberto Basili, Cristina Bosco, Rodolfo Delmonte, Alessandro Moschitti, and Maria Simi, 51–69. Cham: Springer International Publishing. &lt;a href=&#34;https://doi.org/10.1007/978-3-319-14206-7\_3&#34;&gt;https://doi.org/10.1007/978-3-319-14206-7\_3&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-silveira-etal-2014-gold&#34;&gt;
&lt;p&gt;Silveira, Natalia, Timothy Dozat, Marie-Catherine de Marneffe, Samuel Bowman, Miriam Connor, John Bauer, and Chris Manning. 2014. “A Gold Standard Dependency Corpus for English.” In &lt;em&gt;Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC’14)&lt;/em&gt;, 2897–2904. Reykjavik, Iceland: European Language Resources Association (ELRA). &lt;a href=&#34;http://www.lrec-conf.org/proceedings/lrec2014/pdf/1089_Paper.pdf&#34;&gt;http://www.lrec-conf.org/proceedings/lrec2014/pdf/1089_Paper.pdf&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-zeldes-2017-gum&#34;&gt;
&lt;p&gt;Zeldes, Amir. 2017. “The GUM Corpus: Creating Multilayer Resources in the Classroom.” &lt;em&gt;Language Resources and Evaluation&lt;/em&gt; 51: 581–612.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol start=&#34;16&#34;&gt;
&lt;li id=&#34;fn16&#34;&gt;&lt;p&gt;Code available at &lt;a href=&#34;https://github.com/gsarti/interpreting-complexity&#34;&gt;https://github.com/gsarti/interpreting-complexity&lt;/a&gt;&lt;a href=&#34;chap-ex1.html#fnref16&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn17&#34;&gt;&lt;p&gt;Training procedure and parameters are thoroughly described in Appendix &lt;a href=&#34;app-params.html#app:params&#34;&gt;F&lt;/a&gt;.&lt;a href=&#34;chap-ex1.html#fnref17&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
            &lt;/section&gt;

          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
&lt;a href=&#34;chap-models.html&#34; class=&#34;navigation navigation-prev &#34; aria-label=&#34;Previous page&#34;&gt;&lt;i class=&#34;fa fa-angle-left&#34;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&#34;chap-ex2.html&#34; class=&#34;navigation navigation-next &#34; aria-label=&#34;Next page&#34;&gt;&lt;i class=&#34;fa fa-angle-right&#34;&gt;&lt;/i&gt;&lt;/a&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/app.min.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/lunr.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/clipboard.min.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/plugin-search.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/plugin-sharing.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/plugin-fontsettings.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/plugin-bookdown.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/jquery.highlight.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/plugin-clipboard.js&#34;&gt;&lt;/script&gt;
&lt;script&gt;
gitbook.require([&#34;gitbook&#34;], function(gitbook) {
gitbook.start({
&#34;sharing&#34;: {
&#34;github&#34;: true,
&#34;facebook&#34;: true,
&#34;twitter&#34;: true,
&#34;linkedin&#34;: true,
&#34;weibo&#34;: false,
&#34;instapaper&#34;: false,
&#34;vk&#34;: false,
&#34;all&#34;: false
},
&#34;fontsettings&#34;: {
&#34;theme&#34;: &#34;white&#34;,
&#34;family&#34;: &#34;sans&#34;,
&#34;size&#34;: 2
},
&#34;edit&#34;: {
&#34;link&#34;: &#34;https://github.com/gsarti/master-thesis/tree/master/03-Linguistic-Phenomena.Rmd&#34;,
&#34;text&#34;: &#34;Edit&#34;
},
&#34;history&#34;: {
&#34;link&#34;: null,
&#34;text&#34;: null
},
&#34;view&#34;: {
&#34;link&#34;: null,
&#34;text&#34;: null
},
&#34;download&#34;: [[&#34;Sarti_2020_Interpreting_NLMs_for_LCA.pdf&#34;, &#34;PDF&#34;]],
&#34;toc&#34;: {
&#34;collapse&#34;: &#34;subsection&#34;,
&#34;scroll_highlight&#34;: true
},
&#34;info&#34;: false
});
});
&lt;/script&gt;

&lt;!-- dynamically load mathjax for compatibility with self-contained --&gt;
&lt;script&gt;
  (function () {
    var script = document.createElement(&#34;script&#34;);
    script.type = &#34;text/javascript&#34;;
    var src = &#34;true&#34;;
    if (src === &#34;&#34; || src === &#34;true&#34;) src = &#34;https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML&#34;;
    if (location.protocol !== &#34;file:&#34;)
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, &#39;&#39;);
    script.src = src;
    document.getElementsByTagName(&#34;head&#34;)[0].appendChild(script);
  })();
&lt;/script&gt;
&lt;/body&gt;

&lt;/html&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>http://localhost:1313/msc-thesis/chap-ex2/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/msc-thesis/chap-ex2/</guid>
      <description>&lt;!DOCTYPE html&gt;
&lt;html lang=&#34;&#34; xml:lang=&#34;&#34;&gt;
&lt;head&gt;

  &lt;meta charset=&#34;utf-8&#34; /&gt;
  &lt;meta http-equiv=&#34;X-UA-Compatible&#34; content=&#34;IE=edge&#34; /&gt;
  &lt;title&gt;4 Representational Similarity in Models of Complexity | Interpreting Neural Language Models for Linguistic Complexity Assessment&lt;/title&gt;
  &lt;meta name=&#34;description&#34; content=&#34;MSc Thesis at the University of Trieste and SISSA under the supervision of Prof. Davide Crepaldi and Dott. Felice dell&#39;Orletta&#34; /&gt;
  &lt;meta name=&#34;generator&#34; content=&#34;bookdown 0.20.6 and GitBook 2.6.7&#34; /&gt;

  &lt;meta property=&#34;og:title&#34; content=&#34;4 Representational Similarity in Models of Complexity | Interpreting Neural Language Models for Linguistic Complexity Assessment&#34; /&gt;
  &lt;meta property=&#34;og:type&#34; content=&#34;book&#34; /&gt;
  &lt;meta property=&#34;og:url&#34; content=&#34;https://gsarti.com/master-thesis&#34; /&gt;
  &lt;meta property=&#34;og:image&#34; content=&#34;https://gsarti.com/master-thesisfigures/cover.png&#34; /&gt;
  &lt;meta property=&#34;og:description&#34; content=&#34;MSc Thesis at the University of Trieste and SISSA under the supervision of Prof. Davide Crepaldi and Dott. Felice dell&#39;Orletta&#34; /&gt;
  &lt;meta name=&#34;github-repo&#34; content=&#34;gsarti/interpreting-complexity&#34; /&gt;

  &lt;meta name=&#34;twitter:card&#34; content=&#34;summary&#34; /&gt;
  &lt;meta name=&#34;twitter:title&#34; content=&#34;4 Representational Similarity in Models of Complexity | Interpreting Neural Language Models for Linguistic Complexity Assessment&#34; /&gt;
  
  &lt;meta name=&#34;twitter:description&#34; content=&#34;MSc Thesis at the University of Trieste and SISSA under the supervision of Prof. Davide Crepaldi and Dott. Felice dell&#39;Orletta&#34; /&gt;
  &lt;meta name=&#34;twitter:image&#34; content=&#34;https://gsarti.com/master-thesisfigures/cover.png&#34; /&gt;

&lt;meta name=&#34;author&#34; content=&#34;Gabriele Sarti&#34; /&gt;



  &lt;meta name=&#34;viewport&#34; content=&#34;width=device-width, initial-scale=1&#34; /&gt;
  &lt;meta name=&#34;apple-mobile-web-app-capable&#34; content=&#34;yes&#34; /&gt;
  &lt;meta name=&#34;apple-mobile-web-app-status-bar-style&#34; content=&#34;black&#34; /&gt;
  &lt;link rel=&#34;apple-touch-icon-precomposed&#34; sizes=&#34;152x152&#34; href=&#34;figures/icons/apple-icon.png&#34; /&gt;
  &lt;link rel=&#34;shortcut icon&#34; href=&#34;figures/icons/favicon.ico&#34; type=&#34;image/x-icon&#34; /&gt;
&lt;link rel=&#34;prev&#34; href=&#34;chap-ex1.html&#34;/&gt;
&lt;link rel=&#34;next&#34; href=&#34;chap-ex3.html&#34;/&gt;
&lt;style type=&#34;text/css&#34;&gt;
p.abstract{
  text-align: center;
  font-weight: bold;
}
div.abstract{
  margin: auto;
  width: 90%;
}
&lt;/style&gt;
&lt;script src=&#34;libs/jquery-2.2.3/jquery.min.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;libs/gitbook-2.6.7/css/style.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;link href=&#34;libs/gitbook-2.6.7/css/plugin-table.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;link href=&#34;libs/gitbook-2.6.7/css/plugin-bookdown.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;link href=&#34;libs/gitbook-2.6.7/css/plugin-highlight.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;link href=&#34;libs/gitbook-2.6.7/css/plugin-search.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;link href=&#34;libs/gitbook-2.6.7/css/plugin-fontsettings.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;link href=&#34;libs/gitbook-2.6.7/css/plugin-clipboard.css&#34; rel=&#34;stylesheet&#34; /&gt;









&lt;script src=&#34;libs/kePrint-0.0.1/kePrint.js&#34;&gt;&lt;/script&gt;



&lt;link rel=&#34;stylesheet&#34; href=&#34;templates/style.css&#34; type=&#34;text/css&#34; /&gt;
&lt;/head&gt;

&lt;body&gt;



  &lt;div class=&#34;book without-animation with-summary font-size-2 font-family-1&#34; data-basepath=&#34;.&#34;&gt;

    &lt;div class=&#34;book-summary&#34;&gt;
      &lt;nav role=&#34;navigation&#34;&gt;

&lt;ul class=&#34;summary&#34;&gt;
&lt;li&gt;&lt;a href=&#34;introduction.html#introduction&#34;&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1&lt;/b&gt; &lt;strong&gt;Linguistic Complexity&lt;/strong&gt;&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.1&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subchap:categorizing&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.1&lt;/b&gt; Categorizing Linguistic Complexity Measures&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.2&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subchap:intrinsic&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.2&lt;/b&gt; Intrinsic Perspective&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.2.1&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subsubchap:structural&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.2.1&lt;/b&gt; Structural Linguistic Complexity&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.2.2&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subsubchap:lm-surprisal&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.2.2&lt;/b&gt; Language Modeling Surprisal&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.3&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subchap:extrinsic&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.3&lt;/b&gt; Extrinsic Perspective&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.3.1&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subsubchap:readability&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.3.1&lt;/b&gt; Automatic Readability Assessment&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.3.2&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subsubchap:pc&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.3.2&lt;/b&gt; Perceived Complexity Prediction&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.3.3&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subsubchap:eye-tracking&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.3.3&lt;/b&gt; Gaze Metrics Prediction&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.4&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subchap:garden-path&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.4&lt;/b&gt; Garden-path Sentences&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2&lt;/b&gt; &lt;strong&gt;Models of Linguistic Complexity&lt;/strong&gt;&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2.1&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html#subchap:desiderata&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2.1&lt;/b&gt; Desiderata for Models of Linguistic Complexity&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2.2&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html#subchap:nlm&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2.2&lt;/b&gt; Neural Language Models: Unsupervised Multitask Learners&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2.2.1&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html#subsubchap:syntax-nlm&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2.2.1&lt;/b&gt; Emergent Linguistic Structures in Neural Language Models&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2.3&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html#subchap:analyzing-nlm&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2.3&lt;/b&gt; Analyzing Neural Models of Complexity&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2.3.1&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html#subsubchap:probe&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2.3.1&lt;/b&gt; Probing classifiers&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2.3.2&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html#subsubchap:rsa&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2.3.2&lt;/b&gt; Representational Similarity Analysis&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2.3.3&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html#subsubchap:pwcca&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2.3.3&lt;/b&gt; Projection-Weighted Canonical Correlation Analysis&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3&lt;/b&gt; &lt;strong&gt;Complexity Phenomena in Linguistic Annotations and Language Models&lt;/strong&gt;&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3.1&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html#subchap:ex1-data&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3.1&lt;/b&gt; Data and Preprocessing&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3.2&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html#subchap:ex1-analysis&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3.2&lt;/b&gt; Analysis of Linguistic Phenomena&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3.2.1&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html#subsubchap:ex1-analysis-bins&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3.2.1&lt;/b&gt; Linguistic Phenomena in Length-controlled Bins&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3.3&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html#subchap:ex1-modeling&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3.3&lt;/b&gt; Modeling Online and Offline Linguistic Complexity&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3.3.1&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html#subsubchap:ex1-modeling-bins&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3.3.1&lt;/b&gt; Modeling Complexity in Length-controlled Bins&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3.4&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html#subchap:ex1-probing&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3.4&lt;/b&gt; Probing Linguistic Phenomena in ALBERT Representations&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3.5&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html#subchap:ex1-summary&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3.5&lt;/b&gt; Summary&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;4&#34; data-path=&#34;chap-ex2.html&#34;&gt;&lt;a href=&#34;chap-ex2.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;4&lt;/b&gt; &lt;strong&gt;Representational Similarity in Models of Complexity&lt;/strong&gt;&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;4.1&#34; data-path=&#34;chap-ex2.html&#34;&gt;&lt;a href=&#34;chap-ex2.html#knowledge-driven-requirements-for-learning-models&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;4.1&lt;/b&gt; Knowledge-driven Requirements for Learning Models&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;4.2&#34; data-path=&#34;chap-ex2.html&#34;&gt;&lt;a href=&#34;chap-ex2.html#subchap:ex2-experiments&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;4.2&lt;/b&gt; Experimentsl Evaluation&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;4.2.1&#34; data-path=&#34;chap-ex2.html&#34;&gt;&lt;a href=&#34;chap-ex2.html#subsubchap:ex2-data&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;4.2.1&lt;/b&gt; Data&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;4.2.2&#34; data-path=&#34;chap-ex2.html&#34;&gt;&lt;a href=&#34;chap-ex2.html#subsubchap:ex2-inter&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;4.2.2&lt;/b&gt; Inter-model Representational Similarity&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;4.2.3&#34; data-path=&#34;chap-ex2.html&#34;&gt;&lt;a href=&#34;chap-ex2.html#subsubchap:ex2-intra&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;4.2.3&lt;/b&gt; Intra-model Representational Similarity&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;4.3&#34; data-path=&#34;chap-ex2.html&#34;&gt;&lt;a href=&#34;chap-ex2.html#subchap:ex2-summary&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;4.3&lt;/b&gt; Summary&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;5&#34; data-path=&#34;chap-ex3.html&#34;&gt;&lt;a href=&#34;chap-ex3.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;5&lt;/b&gt; &lt;strong&gt;Gaze-informed Models for Cognitive Processing Prediction&lt;/strong&gt;&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;5.1&#34; data-path=&#34;chap-ex3.html&#34;&gt;&lt;a href=&#34;chap-ex3.html#subchap:ex3-setup&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;5.1&lt;/b&gt; Experimental Setup&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;5.2&#34; data-path=&#34;chap-ex3.html&#34;&gt;&lt;a href=&#34;chap-ex3.html#subchap:ex3-experiments&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;5.2&lt;/b&gt; Experimental Evaluation&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;5.2.1&#34; data-path=&#34;chap-ex3.html&#34;&gt;&lt;a href=&#34;chap-ex3.html#subsubchap:ex3-magnitudes&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;5.2.1&lt;/b&gt; Estimating Magnitudes of Garden-path Delays&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;5.2.2&#34; data-path=&#34;chap-ex3.html&#34;&gt;&lt;a href=&#34;chap-ex3.html#subsubchap:ex3-predicting&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;5.2.2&lt;/b&gt; Predicting Delays with Surprisal and Gaze Metrics&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;5.3&#34; data-path=&#34;chap-ex3.html&#34;&gt;&lt;a href=&#34;chap-ex3.html#subchap:ex3-summary&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;5.3&lt;/b&gt; Summary&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;conclusion.html#conclusion&#34;&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;&#34; data-path=&#34;conclusion.html&#34;&gt;&lt;a href=&#34;conclusion.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;Broader Impact and Ethical Perspectives&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;&#34; data-path=&#34;conclusion.html&#34;&gt;&lt;a href=&#34;conclusion.html#future-directions&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;Future Directions&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;appendix&#34;&gt;&lt;span&gt;&lt;b&gt;Appendix&lt;/b&gt;&lt;/span&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;A&#34; data-path=&#34;app-ling-feats.html&#34;&gt;&lt;a href=&#34;app-ling-feats.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;A&lt;/b&gt; Linguistic Features&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;A.1&#34; data-path=&#34;app-ling-feats.html&#34;&gt;&lt;a href=&#34;app-ling-feats.html#raw-text-properties-and-lexical-variety&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;A.1&lt;/b&gt; Raw Text Properties and Lexical Variety&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;A.2&#34; data-path=&#34;app-ling-feats.html&#34;&gt;&lt;a href=&#34;app-ling-feats.html#morpho-syntacting-information&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;A.2&lt;/b&gt; Morpho-syntacting Information&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;A.3&#34; data-path=&#34;app-ling-feats.html&#34;&gt;&lt;a href=&#34;app-ling-feats.html#verbal-predicate-structure&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;A.3&lt;/b&gt; Verbal Predicate Structure&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;A.4&#34; data-path=&#34;app-ling-feats.html&#34;&gt;&lt;a href=&#34;app-ling-feats.html#global-and-local-parsed-tree-structures&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;A.4&lt;/b&gt; Global and Local Parsed Tree Structures&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;A.5&#34; data-path=&#34;app-ling-feats.html&#34;&gt;&lt;a href=&#34;app-ling-feats.html#syntactic-relations&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;A.5&lt;/b&gt; Syntactic Relations&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;A.6&#34; data-path=&#34;app-ling-feats.html&#34;&gt;&lt;a href=&#34;app-ling-feats.html#subordination-phenomena&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;A.6&lt;/b&gt; Subordination Phenomena&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;B&#34; data-path=&#34;app-et-metrics.html&#34;&gt;&lt;a href=&#34;app-et-metrics.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;B&lt;/b&gt; Precisions on Eye-tracking Metrics and Preprocessing&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;C&#34; data-path=&#34;app-et-modeling.html&#34;&gt;&lt;a href=&#34;app-et-modeling.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;C&lt;/b&gt; Multi-task Token-level Regression for Gaze Metrics Prediction&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;D&#34; data-path=&#34;app-intra-sim.html&#34;&gt;&lt;a href=&#34;app-intra-sim.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;D&lt;/b&gt; Intra-model Similarity for All Models&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;E&#34; data-path=&#34;app-garden-paths-et.html&#34;&gt;&lt;a href=&#34;app-garden-paths-et.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;E&lt;/b&gt; Gaze Metrics Predictions for Garden Path Sentences&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;F&#34; data-path=&#34;app-params.html&#34;&gt;&lt;a href=&#34;app-params.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;F&lt;/b&gt; Reproducibility and Environmental Impact&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;&#34; data-path=&#34;references.html&#34;&gt;&lt;a href=&#34;references.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;References&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;divider&#34;&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://gsarti.com&#34;&gt;Back to my website&lt;/a&gt;&lt;/li&gt;

&lt;/ul&gt;

      &lt;/nav&gt;
    &lt;/div&gt;

    &lt;div class=&#34;book-body&#34;&gt;
      &lt;div class=&#34;body-inner&#34;&gt;
        &lt;div class=&#34;book-header&#34; role=&#34;navigation&#34;&gt;
          &lt;h1&gt;
            &lt;i class=&#34;fa fa-circle-o-notch fa-spin&#34;&gt;&lt;/i&gt;&lt;a href=&#34;./&#34;&gt;Interpreting Neural Language Models&lt;br /&gt;
for Linguistic Complexity Assessment&lt;/a&gt;
          &lt;/h1&gt;
        &lt;/div&gt;

        &lt;div class=&#34;page-wrapper&#34; tabindex=&#34;-1&#34; role=&#34;main&#34;&gt;
          &lt;div class=&#34;page-inner&#34;&gt;

            &lt;section class=&#34;normal&#34; id=&#34;section-&#34;&gt;
&lt;div id=&#34;chap:ex2&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;4&lt;/span&gt; &lt;strong&gt;Representational Similarity in Models of Complexity&lt;/strong&gt;&lt;/h1&gt;
&lt;p&gt;&lt;!-- this will include a mini table of contents--&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;The experiments of this chapter aim to shed light on how the linguistic knowledge encoded in the contextual representations of complexity-trained neural language models varies across layers of abstraction and fine-tuning tasks. Two similarity approaches, Representational Similarity Analysis (RSA) and Projection-Weighted Canonical Correlation Analysis (PWCCA) are used to evaluate the relation subsisting between representations spanning different models and different layers of the same model. The outcomes are finally compared against a set of assumptions aimed at determining a model’s generalization capabilities across language phenomena. Results provide empirical evidence about the inability of state-of-the-art language modeling approaches to effectively represent an abstract hierarchy of linguistic complexity phenomena.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Chapter &lt;a href=&#34;chap-ex1.html#chap:ex1&#34;&gt;3&lt;/a&gt; highlighted how the relation between online and offline complexity perspectives and linguistic phenomena diverge when considering same-length sentences and how those properties of language are adequately captured by a neural language model fine-tuned on complexity metrics. This chapter adopts a complementary perspective on the model-driven study of complexity. Instead of connecting learned representations to the input’s structural properties, it explores how those representations change when the same model is exposed to different training objectives using similarity measures. This approach is used to gain insights on the underlying similarities across complexity metrics, using representations as proxies for the knowledge needed to correctly model various complexity phenomena under a minimal set of assumptions.&lt;/p&gt;
&lt;p&gt;The same ALBERT &lt;span class=&#34;citation&#34;&gt;(Lan et al. &lt;a href=&#34;#ref-lan-etal-2020-albert&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt; model introduced in Section &lt;a href=&#34;chap-models.html#subchap:nlm&#34;&gt;2.2&lt;/a&gt; and used for the last section’s probing task experiments is leveraged for this chapter’s experiments.&lt;a href=&#34;#fn18&#34; class=&#34;footnote-ref&#34; id=&#34;fnref18&#34;&gt;&lt;sup&gt;18&lt;/sup&gt;&lt;/a&gt; The model is first taken as-is in its pre-trained version without fine-tuning (referred to as &lt;strong&gt;Base&lt;/strong&gt;). Then, three instances of it are fine-tuned respectively on &lt;strong&gt;Automatic Readability Assessment&lt;/strong&gt; (RA, Section &lt;a href=&#34;chap-ling-comp.html#subsubchap:readability&#34;&gt;1.3.1&lt;/a&gt;), &lt;strong&gt;Perceived Complexity Prediction&lt;/strong&gt; (PC, Section &lt;a href=&#34;chap-ling-comp.html#subsubchap:pc&#34;&gt;1.3.2&lt;/a&gt;) and &lt;strong&gt;Eye-tracking Metrics Prediction&lt;/strong&gt; (ET, Section &lt;a href=&#34;chap-ling-comp.html#subsubchap:eye-tracking&#34;&gt;1.3.3&lt;/a&gt;) until convergence. The four models are evaluated in two settings: first, by comparing the similarity of same-layer representation across models (&lt;em&gt;inter-model similarity&lt;/em&gt;), and then comparing the similarity across different layers of the same model (&lt;em&gt;intra-model similarity&lt;/em&gt;). For each setting, two similarity metrics are used: Representational Similarity Analysis (RSA, Section &lt;a href=&#34;chap-models.html#subsubchap:rsa&#34;&gt;2.3.2&lt;/a&gt;) and Projection-Weighted Canonical Correlation Analysis (PWCCA, Section &lt;a href=&#34;chap-models.html#subsubchap:pwcca&#34;&gt;2.3.3&lt;/a&gt;). RSA and PWCCA were selected since they provide different perspectives over the similarity of representations: if, on the one hand, RSA naively evaluates the similarity across input representations through correlation, PWCCA factors in the importance of sparsity patterns that characterize overparametrized neural networks using a projection operation. Both token and sentence-level representations are evaluated to obtain a fine-grained overview of representational similarity.&lt;/p&gt;
&lt;p&gt;The models trained on perceived complexity and eye-tracking metrics are again the main subjects of this study, given the logical and empirical relation subsisting between the two complexity perspectives highlighted in previous chapters. The additional use of Base and readability-trained models allows us to verify whether ALBERT representations satisfy a minimal set of assumptions deemed necessary and sufficient for modeling an abstraction hierarchy of linguistic complexity phenomena in an interpretable fashion. Results produced by representational similarity experiments diverge significantly from the initial hypothesis, suggesting the prominence of surface structures and task setups over underlying general knowledge about the nature of the modeled phenomena in shaping representations during the training process.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;custompar&#34;&gt;Contributions&lt;/span&gt; While multiple works aimed at inspecting NLM representations by mean of similarity approaches already exist, this is the first work to the best of my knowledge that does so with the explicit purpose of evaluating the impact of linguistic complexity training. This work:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Highlights similarity and differences in the representations of models trained on different complexity-related tasks to understand how neural network parameters capture different perspectives over linguistic complexity after the training process;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Presents similarity and differences in the representations found at different layers of the same model to understand how knowledge is distributed hierarchically at various abstraction levels after training;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Provide evidence about the inability of state-of-the-art NLP approaches to learning to effectively represent an abstract hierarchy of linguistic complexity phenomena in an unsupervised manner, relying solely on complexity-related annotations.&lt;a href=&#34;#fn19&#34; class=&#34;footnote-ref&#34; id=&#34;fnref19&#34;&gt;&lt;sup&gt;19&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;knowledge-driven-requirements-for-learning-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;4.1&lt;/span&gt; Knowledge-driven Requirements for Learning Models&lt;/h2&gt;
&lt;p&gt;At the beginning of Chapter &lt;a href=&#34;chap-models.html#chap:models&#34;&gt;2&lt;/a&gt; two prerequisites to any model-driven study were defined: that available annotated corpora should be informative about the underlying phenomena we are trying to model, and that sufficiently elaborate models should be able to represent knowledge to solve phenomena-related tasks after being trained on those corpora effectively. This section formalizes the two assumptions and builds upon them to define a set of fundamental requirements that should be satisfied by models capable of generalizing over unseen linguistic structures after undergoing a learning process. Let:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathcal{C}^\phi_\alpha = \Big [ (x_1,\alpha_1)\dots(x_m,\alpha_m)\Big]\)&lt;/span&gt; be an annotated corpus containing some knowledge relative to an abstract phenomenon of interest &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt; encoded in its annotations &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;. &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; can represent any &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;-th linguistic structure or substructure (sentence, word, morpheme). This notation can be generalized to settings where annotations are not explicitly defined (e.g. in the context of language modeling, next structure &lt;span class=&#34;math inline&#34;&gt;\(x_i+1\)&lt;/span&gt; acts as an annotation for &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt;) or when multiple annotations are present (e.g. if &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{C}\)&lt;/span&gt; has two sets of annotations &lt;span class=&#34;math inline&#34;&gt;\(\alpha, \beta\)&lt;/span&gt; modeling the same phenomenon &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{K}\)&lt;/span&gt; is equivalent to two corpora &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{C}^\phi_\alpha, \mathcal{C}^\phi_\beta\)&lt;/span&gt; with shared &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;’s).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt; be a model that, after being trained on &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{C}^{\phi}_\alpha\)&lt;/span&gt;, learns representations (i.e. parameters) that allow him to map correctly linguistic structures to annotations&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathcal{K}^\phi\)&lt;/span&gt; be a set containing all empirical knowledge that is specifically relevant to phenomenon &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt;. &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{K}^\phi_\alpha\)&lt;/span&gt; represents all knowledge relative to &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt; contained in a corpus &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{C}^\phi_\alpha\)&lt;/span&gt;. Concretely, given a corpus &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{C}^\phi_\alpha\)&lt;/span&gt;, we can logically infer from it some estimate knowledge &lt;span class=&#34;math inline&#34;&gt;\(\tilde{\mathcal{K}}^\phi_\alpha\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(\tilde{\mathcal{K}}^\phi_\alpha \simeq \mathcal{K}^\phi_\alpha \subset \mathcal{K}^\phi\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\varsigma_{\alpha, \beta}^{\phi}(x)\)&lt;/span&gt; be an idealized similarity function reflecting the similarity between two sets of representations in performance-driven terms relative to phenomenon &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt;, i.e. measuring their invariance in relation to all knowledge sets &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{K}^\varphi\)&lt;/span&gt;, with &lt;span class=&#34;math inline&#34;&gt;\(\phi \neq \varphi\)&lt;/span&gt; that are irrelevant to phenomenon &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For example, taking linguistic complexity as &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt;, and the GECO corpus as &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{C}^\phi_\alpha\)&lt;/span&gt; (with &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; being e.g. the total fixation duration annotations), we may have &lt;span class=&#34;math inline&#34;&gt;\(\tilde{\mathcal{K}}^\phi_\alpha\)&lt;/span&gt; (i.e. our inferred knowledge about linguistic complexity) contains the observation &lt;span class=&#34;math inline&#34;&gt;\(o =\)&lt;/span&gt; “longer structures are more complex” because longer words have longer total fixation durations on average. Note that the relation &lt;span class=&#34;math inline&#34;&gt;\(o \in \mathcal{K}^\phi_\alpha\)&lt;/span&gt; can only be hypothesized whenever a corpus with different annotations &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{C}^\phi_\beta\)&lt;/span&gt; pertinent to the same phenomenon allows us to infer a &lt;span class=&#34;math inline&#34;&gt;\(\tilde{\mathcal{K}}^\phi_\beta\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(o \in \tilde{\mathcal{K}}^\phi_\alpha \cap \tilde{\mathcal{K}}^\phi_\beta\)&lt;/span&gt; (e.g. longer sentences are also deemed more complex on average in the perceived complexity corpus, so length is probably related to complexity in general).&lt;/p&gt;
&lt;p&gt;Chapter &lt;a href=&#34;chap-models.html#chap:models&#34;&gt;2&lt;/a&gt; assumptions can now be summarized in a single statement:&lt;/p&gt;

&lt;p&gt;&lt;span class=&#34;custompar&#34;&gt;Assumption 4.1&lt;/span&gt; (Learning-driven encodability) A learning process that trains a model &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt; on a corpus &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{C}^\phi_\alpha\)&lt;/span&gt; up to a reasonable accuracy is equivalent to an encoding function that maps &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt;-relevant knowledge contained in &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{C}^\phi_\alpha\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt;’s learned representations.&lt;/p&gt;

&lt;p&gt;If Assumption 4.1 is verified, then annotations must be informative, and the model must be able to encode all knowledge present in the corpora relevant to the phenomena. On top of that foundational assumption, three further requirements that are sufficient and necessary for building interpretable learning models able to represent knowledge in a generalizable manner are defined:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;custompar&#34;&gt;Assumption 4.2&lt;/span&gt; (Knowledge-similarity interrelation) Given two corpora &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{C}^\phi_\alpha, \mathcal{C}^\phi_\beta\)&lt;/span&gt; providing different and possibly complementary knowledge about the same phenomenon &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt; and representations &lt;span class=&#34;math inline&#34;&gt;\(R^{M}_{\alpha}, R^{M}_{\beta}\)&lt;/span&gt; learned by a model &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt; trained respectively on the two corpora, the more those representations are similar in relation to &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt;, the more &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt;-related shared knowledge is contained in the two corpora. When the two representations are perfectly &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt;-similar, the two corpora share the same &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt;-related knowledge.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;custompar&#34;&gt;Assumption 4.3&lt;/span&gt; (Pertinence-based preponderance) The amount of knowledge &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{K}^\phi_\alpha\)&lt;/span&gt; related to phenomenon &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt; contained in a corpus &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{C}^{\phi}_\alpha\)&lt;/span&gt; that explicitly encodes some knowledge about &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt; is always larger than the amount of knowledge relative to &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt; contained in any corpus &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{C}^{\phi&amp;#39;}_\beta\)&lt;/span&gt; which explicitly covers a different phenomenon &lt;span class=&#34;math inline&#34;&gt;\(\phi&amp;#39;\)&lt;/span&gt; by means of its annotations &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;custompar&#34;&gt;Assumption 4.4&lt;/span&gt; (Knowledge-similarity transitivity) Given three corpora &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{C}^\phi_\alpha, \mathcal{C}^\phi_\beta, \mathcal{C}^\phi_\gamma\)&lt;/span&gt; providing different views over the same phenomenon &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt; and representations &lt;span class=&#34;math inline&#34;&gt;\(R^{M}_{\alpha}, R^{M}_{\beta}, R^{M}_{\gamma}\)&lt;/span&gt; learned by a model &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt; trained on each one of them respectively, if a pair of those representations has higher &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt;-similarity than another, then the respective pair of corpora also have a larger amount of shared &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt;-related knowledge and vice versa.&lt;/p&gt;
&lt;p&gt;The experimental section of this chapter is aimed at testing whether those requirements are satisfied by ALBERT. Assumption 4.2 enables us to use representational similarity measures to evaluate our corpora’s latent knowledge related to linguistic complexity. In particular, RSA and PWCCA will be used respectively as naive and more advanced approximations of &lt;span class=&#34;math inline&#34;&gt;\(\varsigma\)&lt;/span&gt; that evaluate representations’ distance in the &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;-dimensional space across multiple linguistic structures.&lt;/p&gt;
&lt;p&gt;The first step in this verification process involves comparing representations learned by ALBERT models trained on PC, ET, and RA against those of Base. Since the base model was exposed to a general MLM pre-training, without having access to any complexity-related annotation, it can be hypothesized that &lt;em&gt;the three complexity-trained models had access to more complexity-related information during training&lt;/em&gt; (Assumptions 4.1 and 4.3), &lt;em&gt;and thus learned representations that are closer together in similarity terms than those of Base&lt;/em&gt; (Assumption 4.2). The other perspective involves evaluating how different views related to the same phenomenon are captured. While perceived complexity annotations and gaze metrics are at the antipodes of the processing spectrum (see Figure &lt;a href=&#34;chap-ling-comp.html#fig:compass&#34;&gt;1.1&lt;/a&gt;), they should logically contain more complexity-related shared information than readability categories since they are both related to the reader’s viewpoint, while RA captures the writer’s perspective. If Assumption 4.4 is verified, then it can be hypothesized that &lt;em&gt;ALBERT-PC and ALBERT-ET learned representations closer together in similarity terms than those of the ALBERT-RA model&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Before moving to the experiments, two crucial aspects should be highlighted. First, corpus size was abstracted away from the verification process despite being commonly known to be an essential factor in shaping neural network training effectiveness. In particular, we should be aware that the size imbalance across available corpora can be a significant source of error in the evaluation process. Secondly, sentence-level training objectives are used for PC and RA tasks, while ALBERT-ET is trained on token-level annotations.&lt;a href=&#34;#fn20&#34; class=&#34;footnote-ref&#34; id=&#34;fnref20&#34;&gt;&lt;sup&gt;20&lt;/sup&gt;&lt;/a&gt; If, on the one hand, this difference in training approaches can act as an additional confounder when evaluating requirements, from another perspective, it can provide us with some information relative to the generalization abilities of ALBERT beyond task setup.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;subchap:ex2-experiments&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;4.2&lt;/span&gt; Experimentsl Evaluation&lt;/h2&gt;
&lt;p&gt;This section describes the similarity experiments that have been carried out over model representations across multiple training setups. First, Section &lt;a href=&#34;chap-ex2.html#subsubchap:ex2-data&#34;&gt;4.2.1&lt;/a&gt; presents the data used to train ALBERT models and evaluate their representational similarity. Then, Section &lt;a href=&#34;chap-ex2.html#subsubchap:ex2-inter&#34;&gt;4.2.2&lt;/a&gt; focuses on validating the assumptions formulated at the beginning of this chapter by evaluating the intra-model similarity across all model pairs. Finally, Section &lt;a href=&#34;chap-ex2.html#subsubchap:ex2-intra&#34;&gt;4.2.3&lt;/a&gt; employs the same similarity approach in an intra-model setting, providing us with some evidence on how linguistic knowledge is encoded hierarchically across ALBERT layers during the training process.&lt;/p&gt;
&lt;div id=&#34;subsubchap:ex2-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;header-section-number&#34;&gt;4.2.1&lt;/span&gt; Data&lt;/h3&gt;
&lt;p&gt;The experiments of this chapter leverage all corpora that were presented in Sections &lt;a href=&#34;chap-ling-comp.html#subsubchap:readability&#34;&gt;1.3.1&lt;/a&gt;, &lt;a href=&#34;chap-ling-comp.html#subsubchap:pc&#34;&gt;1.3.2&lt;/a&gt; and &lt;a href=&#34;chap-ling-comp.html#subsubchap:eye-tracking&#34;&gt;1.3.3&lt;/a&gt; for fine-tuning the three complexity models whose representations were compared against each other and the Base pre-trained ALBERT. Specifically:&lt;/p&gt;

&lt;p&gt;&lt;span class=&#34;custompar&#34;&gt;Readability Assessment&lt;/span&gt; The OneStopEnglish corpus &lt;span class=&#34;citation&#34;&gt;(Vajjala and Lučić &lt;a href=&#34;#ref-vajjala-lucic-2018-onestopenglish&#34;&gt;2018&lt;/a&gt;)&lt;/span&gt; is leveraged by splitting each document into sentences and labeling those with the original reading level. A total of 7190 sentences equally distributed across the Elementary, Intermediate, and Advanced levels are used to fine-tune ALBERT-RA in a multiclass classification setting.&lt;/p&gt;

&lt;p&gt;&lt;span class=&#34;custompar&#34;&gt;Perceived Complexity&lt;/span&gt; The English portion of the corpus by &lt;span class=&#34;citation&#34;&gt;Brunato et al. (&lt;a href=&#34;#ref-brunato-etal-2018-sentence&#34;&gt;2018&lt;/a&gt;)&lt;/span&gt; was again used to fine-tune ALBERT-PC, following the same preprocessing steps detailed in Section &lt;a href=&#34;chap-ex1.html#subchap:ex1-data&#34;&gt;3.1&lt;/a&gt; of the previous chapter.&lt;/p&gt;

&lt;p&gt;&lt;span class=&#34;custompar&#34;&gt;Eye-tracking&lt;/span&gt; The GECO &lt;span class=&#34;citation&#34;&gt;(Cop et al. &lt;a href=&#34;#ref-cop-etal-2017-presenting&#34;&gt;2017&lt;/a&gt;)&lt;/span&gt;, Dundee &lt;span class=&#34;citation&#34;&gt;(Kennedy, Hill, and Pynte &lt;a href=&#34;#ref-kennedy-etal-2003-dundee&#34;&gt;2003&lt;/a&gt;)&lt;/span&gt;, ZuCo &lt;span class=&#34;citation&#34;&gt;(Hollenstein et al. &lt;a href=&#34;#ref-hollenstein-2018-zuco&#34;&gt;2018&lt;/a&gt;)&lt;/span&gt; and ZuCo 2.0 &lt;span class=&#34;citation&#34;&gt;(Hollenstein, Troendle, et al. &lt;a href=&#34;#ref-hollenstein-etal-2020-zuco&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt; corpora were merged (Total column of Table &lt;a href=&#34;chap-ling-comp.html#tab:et-corpora&#34;&gt;1.4&lt;/a&gt;) and used to train the ALBERT-ET model. As opposed to the previous section’s sentence-level approach, ALBERT-ET is trained to predict gaze metrics &lt;em&gt;at token-level&lt;/em&gt; to obtain a fine-grained perspective over the input’s complexity and fully exploit the information available through gaze recordings.&lt;a href=&#34;#fn21&#34; class=&#34;footnote-ref&#34; id=&#34;fnref21&#34;&gt;&lt;sup&gt;21&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;span class=&#34;custompar&#34;&gt;Evaluation&lt;/span&gt; All models are evaluated by measuring the similarity of their representations of the Stanford Sentiment Treebank (SST, &lt;span class=&#34;citation&#34;&gt;Socher et al. (&lt;a href=&#34;#ref-socher-etal-2013-recursive&#34;&gt;2013&lt;/a&gt;)&lt;/span&gt;). The version of the treebank leveraged for this study contained 11,855 sentences and was selected because the movie review genre is different from all textual genres encompassed by the available corpora (except ZuCo, which represent only a small fraction of the whole set of eye-tracking data used). Sentiment annotations were removed, and only sentences were considered.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;subsubchap:ex2-inter&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;header-section-number&#34;&gt;4.2.2&lt;/span&gt; Inter-model Representational Similarity&lt;/h3&gt;


&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:rsa-inter&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;figures/4_rsa_inter_cls.png&#34; alt=&#34;Inter-model RSA scores across layers for all ALBERT models’ combinations, using the [CLS] token (top-left), the all-token average (top-right), and all tokens (bottom) representations. Layer -1 corresponds to the last layer before prediction heads. Higher scores denote stronger inter-model similarity.&#34; width=&#34;50%&#34; /&gt;&lt;img src=&#34;figures/4_rsa_inter_mean.png&#34; alt=&#34;Inter-model RSA scores across layers for all ALBERT models’ combinations, using the [CLS] token (top-left), the all-token average (top-right), and all tokens (bottom) representations. Layer -1 corresponds to the last layer before prediction heads. Higher scores denote stronger inter-model similarity.&#34; width=&#34;50%&#34; /&gt;&lt;img src=&#34;figures/4_rsa_inter_tokens.png&#34; alt=&#34;Inter-model RSA scores across layers for all ALBERT models’ combinations, using the [CLS] token (top-left), the all-token average (top-right), and all tokens (bottom) representations. Layer -1 corresponds to the last layer before prediction heads. Higher scores denote stronger inter-model similarity.&#34; width=&#34;50%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 4.1: Inter-model RSA scores across layers for all ALBERT models’ combinations, using the [CLS] token (top-left), the all-token average (top-right), and all tokens (bottom) representations. Layer -1 corresponds to the last layer before prediction heads. Higher scores denote stronger inter-model similarity.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The inter-model similarity is evaluated by comparing layer-wise representations of models trained on different tasks using the same ALBERT architecture. Given the representations produced by two ALBERT models trained on different complexity-related annotations for all the sentences in the SST corpus, their similarity is evaluated using both RSA and PWCCA in three settings:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;[CLS] token&lt;/strong&gt;: Only the sentence-level &lt;code&gt;[CLS]&lt;/code&gt; initial embedding is considered when evaluating similarity at each layer for all sentences in the SST corpus.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Tokens’ average&lt;/strong&gt;: A sentence-level embedding obtained by averaging all the individual subword embeddings produced by ALBERT is considered when evaluating similarity at each layer for all sentences in the SST corpus.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;All tokens&lt;/strong&gt;: The subword embeddings produced by ALBERT for all SST sentences are considered when evaluating similarity at each layer, including &lt;code&gt;[CLS]&lt;/code&gt;, &lt;code&gt;[SEP]&lt;/code&gt; and regular token embeddings, for all sentences in the SST corpus. In practice, the number of considered embedding was set to a maximum of 50,000 to limit such an approach’s computational costs.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;
Figure &lt;a href=&#34;chap-ex2.html#fig:rsa-inter&#34;&gt;4.1&lt;/a&gt; presents inter-model RSA scores for all model combinations and layers, going from the input layer after initial embeddings (-12) to the last layer before prediction heads (-1).&lt;/p&gt;
&lt;p&gt;Given the RSA similarity metric has range &lt;span class=&#34;math inline&#34;&gt;\([0,1]\)&lt;/span&gt;, it can be observed that representational similarity varies greatly across layers, ranging from very high (&lt;span class=&#34;math inline&#34;&gt;\(\sim 0.9\)&lt;/span&gt;) across bottom layers of the models to very low (&lt;span class=&#34;math inline&#34;&gt;\(&amp;lt; 0.1\)&lt;/span&gt;) for top layers. This observation supports the widely accepted claim that layers closer to the input in NLMs are almost unaffected by task-specific fine-tuning since they encode low-level properties, while layers closer to prediction heads represent task-related abstract knowledge and tend to diverge rapidly during training.&lt;/p&gt;
&lt;p&gt;In settings involving the PC-trained model (yellow, red, and green lines in Figure &lt;a href=&#34;chap-ex2.html#fig:rsa-inter&#34;&gt;4.1&lt;/a&gt;) no sharp decrease in similarity is observed across the top layer for all three variations. Conversely, spikes of decreasing similarity are observed for top layers of all other model pairs. While in terms of &lt;code&gt;[CLS]&lt;/code&gt; all models behave comparably, there is a marked dissimilarity between PC and ET-trained models for top layers when considering all token representations, both with and without averaging (green line in Figures &lt;a href=&#34;chap-ex2.html#fig:rsa-inter&#34;&gt;4.1&lt;/a&gt; a,b). Conversely, RA’s &lt;code&gt;[CLS]&lt;/code&gt; representations behave similarly to the ones of other models, but token representations stay very similar to Base even for top layers, i.e. are slightly affected by fine-tuning (purple line in Figures &lt;a href=&#34;chap-ex2.html#fig:rsa-inter&#34;&gt;4.1&lt;/a&gt; b,c). It can be hypothesized that the RA-trained model cannot collect relevant token-level information since it misses the relative perspective that, as saw in Section &lt;a href=&#34;chap-ling-comp.html#subsubchap:readability&#34;&gt;1.3.1&lt;/a&gt;, plays a key role for readability assessment. In this case, PC and ET-trained models are the only ones building relevant complexity-related knowledge, but they still tend to diverge in terms of representational similarity.&lt;/p&gt;


&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:pwcca-inter&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;figures/4_pwcca_inter_cls.png&#34; alt=&#34;Inter-model PWCCA distances across layers for all ALBERT models’ combinations, using the [CLS] token (top-left), the all-token average (top-right), and all tokens (bottom) representations. Layer -1 corresponds to the last layer before prediction heads. Higher values denote weaker inter-model similarity.&#34; width=&#34;50%&#34; /&gt;&lt;img src=&#34;figures/4_pwcca_inter_mean.png&#34; alt=&#34;Inter-model PWCCA distances across layers for all ALBERT models’ combinations, using the [CLS] token (top-left), the all-token average (top-right), and all tokens (bottom) representations. Layer -1 corresponds to the last layer before prediction heads. Higher values denote weaker inter-model similarity.&#34; width=&#34;50%&#34; /&gt;&lt;img src=&#34;figures/4_pwcca_inter_tokens.png&#34; alt=&#34;Inter-model PWCCA distances across layers for all ALBERT models’ combinations, using the [CLS] token (top-left), the all-token average (top-right), and all tokens (bottom) representations. Layer -1 corresponds to the last layer before prediction heads. Higher values denote weaker inter-model similarity.&#34; width=&#34;50%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 4.2: Inter-model PWCCA distances across layers for all ALBERT models’ combinations, using the [CLS] token (top-left), the all-token average (top-right), and all tokens (bottom) representations. Layer -1 corresponds to the last layer before prediction heads. Higher values denote weaker inter-model similarity.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Figure &lt;a href=&#34;chap-ex2.html#fig:pwcca-inter&#34;&gt;4.2&lt;/a&gt; presents PWCCA scores in the exact same setup as Figure &lt;a href=&#34;chap-ex2.html#fig:rsa-inter&#34;&gt;4.1&lt;/a&gt;. It does not come as a surprise that scores, in this case, tend to increase while moving towards prediction heads since the PWCCA distance on the &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;-axis represents here a function of representational dissimilarity between different layers. Besides this difference, a sharp contrast in behavior is observed in relation to RSA scores, with generally smaller value ranges (&lt;span class=&#34;math inline&#34;&gt;\(\sim 0.0\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\(0.4\)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;In terms of &lt;code&gt;[CLS]&lt;/code&gt; representations, (PC, Base) and (RA, Base) are the two closest pairs, while (PC, ET) and (RA, ET) are furthest. This relation can be rationalized if considering that PC and RA-trained models are trained using the &lt;code&gt;[CLS]&lt;/code&gt; token representation for prediction and have relatively few annotations if compared to the token-level trained ET model. The contrast is even more pronounced when PWCCA distances are measured across token averages (Figure &lt;a href=&#34;chap-ex2.html#fig:pwcca-inter&#34;&gt;4.2&lt;/a&gt; b). Here, pairs containing the ET model quickly diverge from the common trend and settle to a shared PWCCA distance for top layers. Finally, the comparison of all individual token representation contradicts previous RSA trends by showing a remarkably consistent divergence from Base representations at all layers for all the three complexity-trained models.&lt;/p&gt;
&lt;p&gt;All in all, both RSA and PWCCA suggest an abstraction hierarchy where the closeness of a representation layer to prediction heads is proportional to the magnitude of changes in parameter values during the training process. While RSA similarity highlights a markedly different behavior for the readability-trained model, the more advanced PWCCA method indicates that representations of models trained with similar objectives stay close in parameter space throughout training, regardless of the conceptual proximity phenomena modeled by their loss functions.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;subsubchap:ex2-intra&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;header-section-number&#34;&gt;4.2.3&lt;/span&gt; Intra-model Representational Similarity&lt;/h3&gt;
&lt;p&gt;The intra-model similarity is evaluated in the same setting of the previous section. However, instead of comparing the same layer across two different models, the representations learned by all layer pairs inside the same model are compared using RSA and PWCCA. Again, the three perspectives of &lt;code&gt;[CLS]&lt;/code&gt;, token’s average, and all tokens introduced in the previous chapter are evaluated to understand the shift in representations across layers at different levels of granularity (two sentence-level and one token-level).&lt;/p&gt;


&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:rsa-intra-base&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;figures/4_rsa_intra_cls_base.png&#34; alt=&#34;Intra-model RSA scores across layers’ combinations for the pre-trained ALBERT model without fine-tuning (Base), using the [CLS] token (top-left), the all-token average (top-right), and all tokens (bottom) representations. Layer -1 corresponds to the last layer before prediction heads. Higher values denote stronger inter-layer similarity.&#34; width=&#34;50%&#34; /&gt;&lt;img src=&#34;figures/4_rsa_intra_mean_base.png&#34; alt=&#34;Intra-model RSA scores across layers’ combinations for the pre-trained ALBERT model without fine-tuning (Base), using the [CLS] token (top-left), the all-token average (top-right), and all tokens (bottom) representations. Layer -1 corresponds to the last layer before prediction heads. Higher values denote stronger inter-layer similarity.&#34; width=&#34;50%&#34; /&gt;&lt;img src=&#34;figures/4_rsa_intra_tokens_base.png&#34; alt=&#34;Intra-model RSA scores across layers’ combinations for the pre-trained ALBERT model without fine-tuning (Base), using the [CLS] token (top-left), the all-token average (top-right), and all tokens (bottom) representations. Layer -1 corresponds to the last layer before prediction heads. Higher values denote stronger inter-layer similarity.&#34; width=&#34;50%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 4.3: Intra-model RSA scores across layers’ combinations for the pre-trained ALBERT model without fine-tuning (&lt;strong&gt;Base&lt;/strong&gt;), using the [CLS] token (top-left), the all-token average (top-right), and all tokens (bottom) representations. Layer -1 corresponds to the last layer before prediction heads. Higher values denote stronger inter-layer similarity.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Figure &lt;a href=&#34;chap-ex2.html#fig:rsa-intra-base&#34;&gt;4.3&lt;/a&gt; presents intra-model RSA similarity scores for all layer pairs of the Base model, going from the input layer after initial embeddings (-12) to the last layer before prediction heads (-1). Only the Base model results are presented in this chapter since they are very similar to those produced by fine-tuned models. The latter can be found in Appendix &lt;a href=&#34;app-intra-sim.html#app:intra-sim&#34;&gt;D&lt;/a&gt;. The first insight relative to RSA intra-model results is that ALBERT layers tend to learn representations that are generally very similar to those of layers in their neighborhood, especially for layers found at the center and close to the input embeddings of the model. While in the case of &lt;code&gt;[CLS]&lt;/code&gt; similarity scores fall sharply beyond the preceding/following layer for each layer, suggesting a significant variation in the information encoded across the model structure, the high-similarity range is much broader for tokens’ average and all tokens representations. It is interesting to note that the top two layers (-1 and -2) are almost always very dissimilar in relation to the rest of the model, which is coherent with the spiking behavior around inter-model scores highlighted in the previous section. Another interesting observation is that, while &lt;code&gt;[CLS]&lt;/code&gt; and all tokens’ representations are consistently decreasing, the tokens’ average representation similarity follows an undulatory behavior across middle layers for all the tested models, with similarity scores dropping and raising while moving away from reference layer. This fact further supports the evidence that token’s sentence-level average may better integrate language information from lower layers into high-level representations, as highlighted by &lt;span class=&#34;citation&#34;&gt;Miaschi and Dell’Orletta (&lt;a href=&#34;#ref-miaschi-dellorletta-2020-contextual&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt; in the context of morphosyntactic knowledge.&lt;/p&gt;


&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:pwcca-intra-base&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;figures/4_pwcca_intra_cls_base.png&#34; alt=&#34;Intra-model PWCCA distances across layers’ combinations for the pre-trained ALBERT model without fine-tuning (Base), using the [CLS] token (top-left), the all-token average (top-right), and all tokens (bottom) representations. Layer -1 corresponds to the last layer before prediction heads. Higher values denote weaker inter-layer similarity.&#34; width=&#34;50%&#34; /&gt;&lt;img src=&#34;figures/4_pwcca_intra_mean_base.png&#34; alt=&#34;Intra-model PWCCA distances across layers’ combinations for the pre-trained ALBERT model without fine-tuning (Base), using the [CLS] token (top-left), the all-token average (top-right), and all tokens (bottom) representations. Layer -1 corresponds to the last layer before prediction heads. Higher values denote weaker inter-layer similarity.&#34; width=&#34;50%&#34; /&gt;&lt;img src=&#34;figures/4_pwcca_intra_tokens_base.png&#34; alt=&#34;Intra-model PWCCA distances across layers’ combinations for the pre-trained ALBERT model without fine-tuning (Base), using the [CLS] token (top-left), the all-token average (top-right), and all tokens (bottom) representations. Layer -1 corresponds to the last layer before prediction heads. Higher values denote weaker inter-layer similarity.&#34; width=&#34;50%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 4.4: Intra-model PWCCA distances across layers’ combinations for the pre-trained ALBERT model without fine-tuning (&lt;strong&gt;Base&lt;/strong&gt;), using the [CLS] token (top-left), the all-token average (top-right), and all tokens (bottom) representations. Layer -1 corresponds to the last layer before prediction heads. Higher values denote weaker inter-layer similarity.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Figure &lt;a href=&#34;chap-ex2.html#fig:pwcca-intra-base&#34;&gt;4.4&lt;/a&gt; presents PWCCA scores in the exact same setup as Figure &lt;a href=&#34;chap-ex2.html#fig:rsa-intra-base&#34;&gt;4.3&lt;/a&gt;. As in the previous section, the inverse trend in scores here is due to PWCCA being a dissimilarity measure, and the range of result scores is smaller than the one of RSA. Conversely to the previous setting, &lt;code&gt;[CLS]&lt;/code&gt; representations stay closer across layers when their similarity is measured using PWCCA, and there are no significant spikes in score values. The latter finding is coherent with the effect of cross-layer parameter sharing adopted by ALBERT authors. Quoting &lt;span class=&#34;citation&#34;&gt;Lan et al. (&lt;a href=&#34;#ref-lan-etal-2020-albert&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt;: “We observe that the transition from layer to layer [in terms of L2 distances and cosine similarity] are much smoother for ALBERT than for BERT. These results show that weight-sharing affects stabilizing network parameters”. In the context of &lt;code&gt;[CLS]&lt;/code&gt; representations, the lowest layer (-12) appears to be slightly closer to the top layers than the subsequent ones. This fact ultimately supports the intuition that ALBERT is heavily overparametrized, and first-level embeddings already capture much information.&lt;/p&gt;
&lt;p&gt;Again for intra-model similarity, PWCCA highlights an abstraction hierarchy inside ALBERT with smoother and generally more reasonable transitions than those showed by RSA. There is no reason to believe that ALBERT adapts its representation hierarchy as a function of its objective since intra-model similarity scores stay approximately the same before and after fine-tuning for all complexity corpora.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;subchap:ex2-summary&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;4.3&lt;/span&gt; Summary&lt;/h2&gt;
&lt;p&gt;In this chapter, the representations learned by a neural language model fine-tuned on multiple complexity-related tasks were compared using two widely-used representational similarity approaches. Token and sentence-level representations were compared both considering the same layer across models exposed to different training corpora and different layer pairs contained in the same model. In the first case, the absence of a preponderant similarity between complexity-trained models when compared to the pre-trained one suggests that those models learn their objective by overfitting annotations and without being able to recognize useful primitives that could be recycled throughout complexity tasks. This fact is highlighted in the comparison between perceived complexity and eye-tracking-trained models, where similarity scores of layers close to prediction heads are very different despite the close relationship between the two complexity perspectives. In conclusion, this work strongly supports the claim that representation learning in ALBERT and other neural language models is mainly driven by training biases like task granularity (token-level vs. sentence-level) that are unrelated to the nature of the task itself. This fact hinders their generalization performances, suggesting that much work still needs to be done beyond language modeling to drive generalizable, hierarchical, and compositional representation learning in models of language.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;h3&gt;References&lt;/h3&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-brunato-etal-2018-sentence&#34;&gt;
&lt;p&gt;Brunato, Dominique, Lorenzo De Mattei, Felice Dell’Orletta, Benedetta Iavarone, and Giulia Venturi. 2018. “Is This Sentence Difficult? Do You Agree?” In &lt;em&gt;Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing&lt;/em&gt;, 2690–9. Brussels, Belgium: Association for Computational Linguistics. &lt;a href=&#34;https://doi.org/10.18653/v1/D18-1289&#34;&gt;https://doi.org/10.18653/v1/D18-1289&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-cop-etal-2017-presenting&#34;&gt;
&lt;p&gt;Cop, Uschi, Nicolas Dirix, Denis Drieghe, and Wouter Duyck. 2017. “Presenting Geco: An Eyetracking Corpus of Monolingual and Bilingual Sentence Reading.” &lt;em&gt;Behavior Research Methods&lt;/em&gt; 49 (2). Springer: 602–15.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-hollenstein-2018-zuco&#34;&gt;
&lt;p&gt;Hollenstein, Nora, Jonathan Rotsztejn, Marius Troendle, Andreas Pedroni, Ce Zhang, and Nicolas Langer. 2018. “ZuCo, a Simultaneous Eeg and Eye-Tracking Resource for Natural Sentence Reading.” &lt;em&gt;Scientific Data&lt;/em&gt; 5 (1). Nature Publishing Group: 1–13.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-hollenstein-etal-2020-zuco&#34;&gt;
&lt;p&gt;Hollenstein, Nora, Marius Troendle, Ce Zhang, and Nicolas Langer. 2020. “ZuCo 2.0: A Dataset of Physiological Recordings During Natural Reading and Annotation.” In &lt;em&gt;Proceedings of the 12th Language Resources and Evaluation Conference&lt;/em&gt;, 138–46. Marseille, France: European Language Resources Association. &lt;a href=&#34;https://www.aclweb.org/anthology/2020.lrec-1.18&#34;&gt;https://www.aclweb.org/anthology/2020.lrec-1.18&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-kennedy-etal-2003-dundee&#34;&gt;
&lt;p&gt;Kennedy, Alan, Robin Hill, and Joël Pynte. 2003. “The Dundee Corpus.” In &lt;em&gt;Proceedings of the 12th European Conference on Eye Movement&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-lan-etal-2020-albert&#34;&gt;
&lt;p&gt;Lan, Zhenzhong, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. 2020. “ALBERT: A Lite BERT for Self-Supervised Learning of Language Representations.” In &lt;em&gt;International Conference on Learning Representations&lt;/em&gt;. &lt;a href=&#34;https://openreview.net/forum?id=H1eA7AEtvS&#34;&gt;https://openreview.net/forum?id=H1eA7AEtvS&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-miaschi-dellorletta-2020-contextual&#34;&gt;
&lt;p&gt;Miaschi, Alessio, and Felice Dell’Orletta. 2020. “Contextual and Non-Contextual Word Embeddings: An in-Depth Linguistic Investigation.” In &lt;em&gt;Proceedings of the 5th Workshop on Representation Learning for Nlp&lt;/em&gt;, 110–19. Online: Association for Computational Linguistics. &lt;a href=&#34;https://www.aclweb.org/anthology/2020.repl4nlp-1.15&#34;&gt;https://www.aclweb.org/anthology/2020.repl4nlp-1.15&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-socher-etal-2013-recursive&#34;&gt;
&lt;p&gt;Socher, Richard, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. 2013. “Recursive Deep Models for Semantic Compositionality over a Sentiment Treebank.” In &lt;em&gt;Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing&lt;/em&gt;, 1631–42. Seattle, Washington, USA: Association for Computational Linguistics. &lt;a href=&#34;https://www.aclweb.org/anthology/D13-1170&#34;&gt;https://www.aclweb.org/anthology/D13-1170&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-vajjala-lucic-2018-onestopenglish&#34;&gt;
&lt;p&gt;Vajjala, Sowmya, and Ivana Lučić. 2018. “OneStopEnglish Corpus: A New Corpus for Automatic Readability Assessment and Text Simplification.” In &lt;em&gt;Proceedings of the Thirteenth Workshop on Innovative Use of NLP for Building Educational Applications&lt;/em&gt;, 297–304. New Orleans, Louisiana: Association for Computational Linguistics. &lt;a href=&#34;https://doi.org/10.18653/v1/W18-0535&#34;&gt;https://doi.org/10.18653/v1/W18-0535&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol start=&#34;18&#34;&gt;
&lt;li id=&#34;fn18&#34;&gt;&lt;p&gt;The &lt;code&gt;albert-base-v2&lt;/code&gt; checkpoint from 🤗 &lt;code&gt;transformers&lt;/code&gt; &lt;span class=&#34;citation&#34;&gt;(Wolf et al. &lt;a href=&#34;#ref-wolf-etal-2020-huggingface&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt; is used.&lt;a href=&#34;chap-ex2.html#fnref18&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn19&#34;&gt;&lt;p&gt;Code available at &lt;a href=&#34;https://github.com/gsarti/interpreting-complexity&#34;&gt;https://github.com/gsarti/interpreting-complexity&lt;/a&gt;&lt;a href=&#34;chap-ex2.html#fnref19&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn20&#34;&gt;&lt;p&gt;More details on this procedure are provided in Appendix &lt;a href=&#34;app-et-modeling.html#app:et-modeling&#34;&gt;C&lt;/a&gt;.&lt;a href=&#34;chap-ex2.html#fnref20&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn21&#34;&gt;&lt;p&gt;See Appendix &lt;a href=&#34;app-et-metrics.html#app:et-metrics&#34;&gt;B&lt;/a&gt; for additional details on the preprocessing and merging of eye-tracking corpora.&lt;a href=&#34;chap-ex2.html#fnref21&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
            &lt;/section&gt;

          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
&lt;a href=&#34;chap-ex1.html&#34; class=&#34;navigation navigation-prev &#34; aria-label=&#34;Previous page&#34;&gt;&lt;i class=&#34;fa fa-angle-left&#34;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&#34;chap-ex3.html&#34; class=&#34;navigation navigation-next &#34; aria-label=&#34;Next page&#34;&gt;&lt;i class=&#34;fa fa-angle-right&#34;&gt;&lt;/i&gt;&lt;/a&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/app.min.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/lunr.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/clipboard.min.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/plugin-search.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/plugin-sharing.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/plugin-fontsettings.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/plugin-bookdown.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/jquery.highlight.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/plugin-clipboard.js&#34;&gt;&lt;/script&gt;
&lt;script&gt;
gitbook.require([&#34;gitbook&#34;], function(gitbook) {
gitbook.start({
&#34;sharing&#34;: {
&#34;github&#34;: true,
&#34;facebook&#34;: true,
&#34;twitter&#34;: true,
&#34;linkedin&#34;: true,
&#34;weibo&#34;: false,
&#34;instapaper&#34;: false,
&#34;vk&#34;: false,
&#34;all&#34;: false
},
&#34;fontsettings&#34;: {
&#34;theme&#34;: &#34;white&#34;,
&#34;family&#34;: &#34;sans&#34;,
&#34;size&#34;: 2
},
&#34;edit&#34;: {
&#34;link&#34;: &#34;https://github.com/gsarti/master-thesis/tree/master/04-Representation-Similarity.Rmd&#34;,
&#34;text&#34;: &#34;Edit&#34;
},
&#34;history&#34;: {
&#34;link&#34;: null,
&#34;text&#34;: null
},
&#34;view&#34;: {
&#34;link&#34;: null,
&#34;text&#34;: null
},
&#34;download&#34;: [[&#34;Sarti_2020_Interpreting_NLMs_for_LCA.pdf&#34;, &#34;PDF&#34;]],
&#34;toc&#34;: {
&#34;collapse&#34;: &#34;subsection&#34;,
&#34;scroll_highlight&#34;: true
},
&#34;info&#34;: false
});
});
&lt;/script&gt;

&lt;!-- dynamically load mathjax for compatibility with self-contained --&gt;
&lt;script&gt;
  (function () {
    var script = document.createElement(&#34;script&#34;);
    script.type = &#34;text/javascript&#34;;
    var src = &#34;true&#34;;
    if (src === &#34;&#34; || src === &#34;true&#34;) src = &#34;https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML&#34;;
    if (location.protocol !== &#34;file:&#34;)
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, &#39;&#39;);
    script.src = src;
    document.getElementsByTagName(&#34;head&#34;)[0].appendChild(script);
  })();
&lt;/script&gt;
&lt;/body&gt;

&lt;/html&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>http://localhost:1313/msc-thesis/chap-ex3/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/msc-thesis/chap-ex3/</guid>
      <description>&lt;!DOCTYPE html&gt;
&lt;html lang=&#34;&#34; xml:lang=&#34;&#34;&gt;
&lt;head&gt;

  &lt;meta charset=&#34;utf-8&#34; /&gt;
  &lt;meta http-equiv=&#34;X-UA-Compatible&#34; content=&#34;IE=edge&#34; /&gt;
  &lt;title&gt;5 Gaze-informed Models for Cognitive Processing Prediction | Interpreting Neural Language Models for Linguistic Complexity Assessment&lt;/title&gt;
  &lt;meta name=&#34;description&#34; content=&#34;MSc Thesis at the University of Trieste and SISSA under the supervision of Prof. Davide Crepaldi and Dott. Felice dell&#39;Orletta&#34; /&gt;
  &lt;meta name=&#34;generator&#34; content=&#34;bookdown 0.20.6 and GitBook 2.6.7&#34; /&gt;

  &lt;meta property=&#34;og:title&#34; content=&#34;5 Gaze-informed Models for Cognitive Processing Prediction | Interpreting Neural Language Models for Linguistic Complexity Assessment&#34; /&gt;
  &lt;meta property=&#34;og:type&#34; content=&#34;book&#34; /&gt;
  &lt;meta property=&#34;og:url&#34; content=&#34;https://gsarti.com/master-thesis&#34; /&gt;
  &lt;meta property=&#34;og:image&#34; content=&#34;https://gsarti.com/master-thesisfigures/cover.png&#34; /&gt;
  &lt;meta property=&#34;og:description&#34; content=&#34;MSc Thesis at the University of Trieste and SISSA under the supervision of Prof. Davide Crepaldi and Dott. Felice dell&#39;Orletta&#34; /&gt;
  &lt;meta name=&#34;github-repo&#34; content=&#34;gsarti/interpreting-complexity&#34; /&gt;

  &lt;meta name=&#34;twitter:card&#34; content=&#34;summary&#34; /&gt;
  &lt;meta name=&#34;twitter:title&#34; content=&#34;5 Gaze-informed Models for Cognitive Processing Prediction | Interpreting Neural Language Models for Linguistic Complexity Assessment&#34; /&gt;
  
  &lt;meta name=&#34;twitter:description&#34; content=&#34;MSc Thesis at the University of Trieste and SISSA under the supervision of Prof. Davide Crepaldi and Dott. Felice dell&#39;Orletta&#34; /&gt;
  &lt;meta name=&#34;twitter:image&#34; content=&#34;https://gsarti.com/master-thesisfigures/cover.png&#34; /&gt;

&lt;meta name=&#34;author&#34; content=&#34;Gabriele Sarti&#34; /&gt;



  &lt;meta name=&#34;viewport&#34; content=&#34;width=device-width, initial-scale=1&#34; /&gt;
  &lt;meta name=&#34;apple-mobile-web-app-capable&#34; content=&#34;yes&#34; /&gt;
  &lt;meta name=&#34;apple-mobile-web-app-status-bar-style&#34; content=&#34;black&#34; /&gt;
  &lt;link rel=&#34;apple-touch-icon-precomposed&#34; sizes=&#34;152x152&#34; href=&#34;figures/icons/apple-icon.png&#34; /&gt;
  &lt;link rel=&#34;shortcut icon&#34; href=&#34;figures/icons/favicon.ico&#34; type=&#34;image/x-icon&#34; /&gt;
&lt;link rel=&#34;prev&#34; href=&#34;chap-ex2.html&#34;/&gt;
&lt;link rel=&#34;next&#34; href=&#34;conclusion.html&#34;/&gt;
&lt;style type=&#34;text/css&#34;&gt;
p.abstract{
  text-align: center;
  font-weight: bold;
}
div.abstract{
  margin: auto;
  width: 90%;
}
&lt;/style&gt;
&lt;script src=&#34;libs/jquery-2.2.3/jquery.min.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;libs/gitbook-2.6.7/css/style.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;link href=&#34;libs/gitbook-2.6.7/css/plugin-table.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;link href=&#34;libs/gitbook-2.6.7/css/plugin-bookdown.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;link href=&#34;libs/gitbook-2.6.7/css/plugin-highlight.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;link href=&#34;libs/gitbook-2.6.7/css/plugin-search.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;link href=&#34;libs/gitbook-2.6.7/css/plugin-fontsettings.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;link href=&#34;libs/gitbook-2.6.7/css/plugin-clipboard.css&#34; rel=&#34;stylesheet&#34; /&gt;









&lt;script src=&#34;libs/kePrint-0.0.1/kePrint.js&#34;&gt;&lt;/script&gt;



&lt;link rel=&#34;stylesheet&#34; href=&#34;templates/style.css&#34; type=&#34;text/css&#34; /&gt;
&lt;/head&gt;

&lt;body&gt;



  &lt;div class=&#34;book without-animation with-summary font-size-2 font-family-1&#34; data-basepath=&#34;.&#34;&gt;

    &lt;div class=&#34;book-summary&#34;&gt;
      &lt;nav role=&#34;navigation&#34;&gt;

&lt;ul class=&#34;summary&#34;&gt;
&lt;li&gt;&lt;a href=&#34;introduction.html#introduction&#34;&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1&lt;/b&gt; &lt;strong&gt;Linguistic Complexity&lt;/strong&gt;&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.1&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subchap:categorizing&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.1&lt;/b&gt; Categorizing Linguistic Complexity Measures&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.2&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subchap:intrinsic&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.2&lt;/b&gt; Intrinsic Perspective&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.2.1&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subsubchap:structural&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.2.1&lt;/b&gt; Structural Linguistic Complexity&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.2.2&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subsubchap:lm-surprisal&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.2.2&lt;/b&gt; Language Modeling Surprisal&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.3&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subchap:extrinsic&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.3&lt;/b&gt; Extrinsic Perspective&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.3.1&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subsubchap:readability&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.3.1&lt;/b&gt; Automatic Readability Assessment&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.3.2&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subsubchap:pc&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.3.2&lt;/b&gt; Perceived Complexity Prediction&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.3.3&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subsubchap:eye-tracking&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.3.3&lt;/b&gt; Gaze Metrics Prediction&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.4&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subchap:garden-path&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.4&lt;/b&gt; Garden-path Sentences&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2&lt;/b&gt; &lt;strong&gt;Models of Linguistic Complexity&lt;/strong&gt;&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2.1&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html#subchap:desiderata&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2.1&lt;/b&gt; Desiderata for Models of Linguistic Complexity&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2.2&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html#subchap:nlm&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2.2&lt;/b&gt; Neural Language Models: Unsupervised Multitask Learners&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2.2.1&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html#subsubchap:syntax-nlm&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2.2.1&lt;/b&gt; Emergent Linguistic Structures in Neural Language Models&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2.3&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html#subchap:analyzing-nlm&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2.3&lt;/b&gt; Analyzing Neural Models of Complexity&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2.3.1&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html#subsubchap:probe&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2.3.1&lt;/b&gt; Probing classifiers&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2.3.2&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html#subsubchap:rsa&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2.3.2&lt;/b&gt; Representational Similarity Analysis&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2.3.3&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html#subsubchap:pwcca&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2.3.3&lt;/b&gt; Projection-Weighted Canonical Correlation Analysis&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3&lt;/b&gt; &lt;strong&gt;Complexity Phenomena in Linguistic Annotations and Language Models&lt;/strong&gt;&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3.1&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html#subchap:ex1-data&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3.1&lt;/b&gt; Data and Preprocessing&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3.2&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html#subchap:ex1-analysis&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3.2&lt;/b&gt; Analysis of Linguistic Phenomena&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3.2.1&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html#subsubchap:ex1-analysis-bins&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3.2.1&lt;/b&gt; Linguistic Phenomena in Length-controlled Bins&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3.3&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html#subchap:ex1-modeling&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3.3&lt;/b&gt; Modeling Online and Offline Linguistic Complexity&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3.3.1&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html#subsubchap:ex1-modeling-bins&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3.3.1&lt;/b&gt; Modeling Complexity in Length-controlled Bins&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3.4&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html#subchap:ex1-probing&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3.4&lt;/b&gt; Probing Linguistic Phenomena in ALBERT Representations&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3.5&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html#subchap:ex1-summary&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3.5&lt;/b&gt; Summary&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;4&#34; data-path=&#34;chap-ex2.html&#34;&gt;&lt;a href=&#34;chap-ex2.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;4&lt;/b&gt; &lt;strong&gt;Representational Similarity in Models of Complexity&lt;/strong&gt;&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;4.1&#34; data-path=&#34;chap-ex2.html&#34;&gt;&lt;a href=&#34;chap-ex2.html#knowledge-driven-requirements-for-learning-models&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;4.1&lt;/b&gt; Knowledge-driven Requirements for Learning Models&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;4.2&#34; data-path=&#34;chap-ex2.html&#34;&gt;&lt;a href=&#34;chap-ex2.html#subchap:ex2-experiments&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;4.2&lt;/b&gt; Experimentsl Evaluation&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;4.2.1&#34; data-path=&#34;chap-ex2.html&#34;&gt;&lt;a href=&#34;chap-ex2.html#subsubchap:ex2-data&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;4.2.1&lt;/b&gt; Data&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;4.2.2&#34; data-path=&#34;chap-ex2.html&#34;&gt;&lt;a href=&#34;chap-ex2.html#subsubchap:ex2-inter&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;4.2.2&lt;/b&gt; Inter-model Representational Similarity&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;4.2.3&#34; data-path=&#34;chap-ex2.html&#34;&gt;&lt;a href=&#34;chap-ex2.html#subsubchap:ex2-intra&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;4.2.3&lt;/b&gt; Intra-model Representational Similarity&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;4.3&#34; data-path=&#34;chap-ex2.html&#34;&gt;&lt;a href=&#34;chap-ex2.html#subchap:ex2-summary&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;4.3&lt;/b&gt; Summary&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;5&#34; data-path=&#34;chap-ex3.html&#34;&gt;&lt;a href=&#34;chap-ex3.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;5&lt;/b&gt; &lt;strong&gt;Gaze-informed Models for Cognitive Processing Prediction&lt;/strong&gt;&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;5.1&#34; data-path=&#34;chap-ex3.html&#34;&gt;&lt;a href=&#34;chap-ex3.html#subchap:ex3-setup&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;5.1&lt;/b&gt; Experimental Setup&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;5.2&#34; data-path=&#34;chap-ex3.html&#34;&gt;&lt;a href=&#34;chap-ex3.html#subchap:ex3-experiments&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;5.2&lt;/b&gt; Experimental Evaluation&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;5.2.1&#34; data-path=&#34;chap-ex3.html&#34;&gt;&lt;a href=&#34;chap-ex3.html#subsubchap:ex3-magnitudes&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;5.2.1&lt;/b&gt; Estimating Magnitudes of Garden-path Delays&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;5.2.2&#34; data-path=&#34;chap-ex3.html&#34;&gt;&lt;a href=&#34;chap-ex3.html#subsubchap:ex3-predicting&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;5.2.2&lt;/b&gt; Predicting Delays with Surprisal and Gaze Metrics&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;5.3&#34; data-path=&#34;chap-ex3.html&#34;&gt;&lt;a href=&#34;chap-ex3.html#subchap:ex3-summary&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;5.3&lt;/b&gt; Summary&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;conclusion.html#conclusion&#34;&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;&#34; data-path=&#34;conclusion.html&#34;&gt;&lt;a href=&#34;conclusion.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;Broader Impact and Ethical Perspectives&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;&#34; data-path=&#34;conclusion.html&#34;&gt;&lt;a href=&#34;conclusion.html#future-directions&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;Future Directions&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;appendix&#34;&gt;&lt;span&gt;&lt;b&gt;Appendix&lt;/b&gt;&lt;/span&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;A&#34; data-path=&#34;app-ling-feats.html&#34;&gt;&lt;a href=&#34;app-ling-feats.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;A&lt;/b&gt; Linguistic Features&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;A.1&#34; data-path=&#34;app-ling-feats.html&#34;&gt;&lt;a href=&#34;app-ling-feats.html#raw-text-properties-and-lexical-variety&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;A.1&lt;/b&gt; Raw Text Properties and Lexical Variety&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;A.2&#34; data-path=&#34;app-ling-feats.html&#34;&gt;&lt;a href=&#34;app-ling-feats.html#morpho-syntacting-information&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;A.2&lt;/b&gt; Morpho-syntacting Information&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;A.3&#34; data-path=&#34;app-ling-feats.html&#34;&gt;&lt;a href=&#34;app-ling-feats.html#verbal-predicate-structure&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;A.3&lt;/b&gt; Verbal Predicate Structure&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;A.4&#34; data-path=&#34;app-ling-feats.html&#34;&gt;&lt;a href=&#34;app-ling-feats.html#global-and-local-parsed-tree-structures&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;A.4&lt;/b&gt; Global and Local Parsed Tree Structures&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;A.5&#34; data-path=&#34;app-ling-feats.html&#34;&gt;&lt;a href=&#34;app-ling-feats.html#syntactic-relations&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;A.5&lt;/b&gt; Syntactic Relations&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;A.6&#34; data-path=&#34;app-ling-feats.html&#34;&gt;&lt;a href=&#34;app-ling-feats.html#subordination-phenomena&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;A.6&lt;/b&gt; Subordination Phenomena&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;B&#34; data-path=&#34;app-et-metrics.html&#34;&gt;&lt;a href=&#34;app-et-metrics.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;B&lt;/b&gt; Precisions on Eye-tracking Metrics and Preprocessing&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;C&#34; data-path=&#34;app-et-modeling.html&#34;&gt;&lt;a href=&#34;app-et-modeling.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;C&lt;/b&gt; Multi-task Token-level Regression for Gaze Metrics Prediction&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;D&#34; data-path=&#34;app-intra-sim.html&#34;&gt;&lt;a href=&#34;app-intra-sim.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;D&lt;/b&gt; Intra-model Similarity for All Models&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;E&#34; data-path=&#34;app-garden-paths-et.html&#34;&gt;&lt;a href=&#34;app-garden-paths-et.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;E&lt;/b&gt; Gaze Metrics Predictions for Garden Path Sentences&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;F&#34; data-path=&#34;app-params.html&#34;&gt;&lt;a href=&#34;app-params.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;F&lt;/b&gt; Reproducibility and Environmental Impact&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;&#34; data-path=&#34;references.html&#34;&gt;&lt;a href=&#34;references.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;References&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;divider&#34;&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://gsarti.com&#34;&gt;Back to my website&lt;/a&gt;&lt;/li&gt;

&lt;/ul&gt;

      &lt;/nav&gt;
    &lt;/div&gt;

    &lt;div class=&#34;book-body&#34;&gt;
      &lt;div class=&#34;body-inner&#34;&gt;
        &lt;div class=&#34;book-header&#34; role=&#34;navigation&#34;&gt;
          &lt;h1&gt;
            &lt;i class=&#34;fa fa-circle-o-notch fa-spin&#34;&gt;&lt;/i&gt;&lt;a href=&#34;./&#34;&gt;Interpreting Neural Language Models&lt;br /&gt;
for Linguistic Complexity Assessment&lt;/a&gt;
          &lt;/h1&gt;
        &lt;/div&gt;

        &lt;div class=&#34;page-wrapper&#34; tabindex=&#34;-1&#34; role=&#34;main&#34;&gt;
          &lt;div class=&#34;page-inner&#34;&gt;

            &lt;section class=&#34;normal&#34; id=&#34;section-&#34;&gt;
&lt;div id=&#34;chap:ex3&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;5&lt;/span&gt; &lt;strong&gt;Gaze-informed Models for Cognitive Processing Prediction&lt;/strong&gt;&lt;/h1&gt;
&lt;p&gt;&lt;!-- this will include a mini table of contents--&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;This final experimental chapter aims to study the syntactic generalization capabilities of neural language models by evaluating their performances over atypical linguistic constructions. In particular, architectures pre-trained with masked and causal language modeling are evaluated in their ability to predict garden-path effects on three test suites taken from the SyntaxGym psycholinguistic benchmark. First, the results of previous studies using GPT-2 surprisal to predict garden-path effects are reproduced, and a conversion coefficient is used to evaluate GPT-2 surprisal in terms of human reading times delays. Two neural language models are fine-tuned over gaze metrics from multiple eye-tracking corpora in a multitask token-level setting. Gaze metric predictions on garden-path sentences are evaluated to see whether gaze data fine-tuning can improve garden-path effects prediction. Results highlight how GPT-2 surprisals overestimate the magnitude of MV/RR and NP/Z garden-path effects, and fine-tuning procedures on gaze metrics prediction over typical linguistic structures do not benefit the generalization capabilities of neural language models on out-of-distribution cases like garden-path sentences.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Human behavioral data collected during naturalistic reading can provide useful insights into the primary sources of processing difficulties during reading comprehension. Multiple cognitive processing theories were formulated to account for the sources of such difficulties (see Section &lt;a href=&#34;chap-ling-comp.html#subchap:garden-path&#34;&gt;1.4&lt;/a&gt;). Notably, &lt;strong&gt;surprisal theory&lt;/strong&gt; &lt;span class=&#34;citation&#34;&gt;(Hale &lt;a href=&#34;#ref-hale-2001-probabilistic&#34;&gt;2001&lt;/a&gt;; Levy &lt;a href=&#34;#ref-levy-2008-expectation&#34;&gt;2008&lt;/a&gt;)&lt;/span&gt; suggests that processing during reading is the direct result of a single mechanism, that is, the shift in readers’ probability distribution over all possible parses. To evaluate whether this perspective holds empirically, language models defining a probability distribution over a vocabulary given previous context (RNNs in &lt;span class=&#34;citation&#34;&gt;Elman (&lt;a href=&#34;#ref-elman-1991-distributed&#34;&gt;1991&lt;/a&gt;)&lt;/span&gt; and &lt;span class=&#34;citation&#34;&gt;Mikolov et al. (&lt;a href=&#34;#ref-mikolov-etal-2010-recurrent&#34;&gt;2010&lt;/a&gt;)&lt;/span&gt;, recently Transformers in &lt;span class=&#34;citation&#34;&gt;Hu et al. (&lt;a href=&#34;#ref-hu-etal-2020-systematic&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt;) are commonly used to obtain accurate predictability estimates that can directly be compared to behavioral recordings (e.g. gaze metrics) acting as proxies of human cognitive processing.&lt;/p&gt;
&lt;p&gt;A computational model that consistently mimics human processing behaviors would provide strong evidence of cognitive processing’s underlying probabilistic-driven nature. For this reason, many studies in the fields of syntax and psycholinguistics have focused on probing the abilities of language models to highlight phenomena related to reading difficulties &lt;span class=&#34;citation&#34;&gt;(Linzen, Dupoux, and Goldberg &lt;a href=&#34;#ref-linzen-etal-2016-assessing&#34;&gt;2016&lt;/a&gt;; Gulordava et al. &lt;a href=&#34;#ref-gulordava-etal-2018-colorless&#34;&gt;2018&lt;/a&gt;; Futrell et al. &lt;a href=&#34;#ref-futrell-etal-2019-neural&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt;. Peculiar constructions like garden-path sentences are often used in this context to evaluate the generalization capabilities of language models for two main reasons. First, garden-path sentences are rare in naturally-occurring text. As such, they represent out-of-distribution examples for any language model trained on conventional data and can be used to test the latter’s generalization capabilities. Secondly, researchers nowadays have access to reasonably-sized literature describing the impact of garden-path effects on cognitive processing proxies such as gaze recordings, with articles being often released alongside publicly-available resources for reproducible evaluation &lt;span class=&#34;citation&#34;&gt;(Prasad and Linzen &lt;a href=&#34;#ref-prasad-linzen-2019-self&#34;&gt;2019&lt;/a&gt;&lt;a href=&#34;#ref-prasad-linzen-2019-self&#34;&gt;a&lt;/a&gt;, &lt;a href=&#34;#ref-prasad-linzen-2019-much&#34;&gt;2019&lt;/a&gt;&lt;a href=&#34;#ref-prasad-linzen-2019-much&#34;&gt;b&lt;/a&gt;)&lt;/span&gt; and recently even ad-hoc benchmarks &lt;span class=&#34;citation&#34;&gt;(Gauthier et al. &lt;a href=&#34;#ref-gauthier-etal-2020-syntaxgym&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;This final experimental chapter evaluates the ability of neural language models in predicting garden-path effects observed on human subjects, using language modeling surprisal and eye-tracking metrics elicited respectively before and after multitask token-level eye-tracking fine-tuning for garden-path effects prediction. Specifically, an autoregressive (GPT-2, &lt;span class=&#34;citation&#34;&gt;Radford et al. (&lt;a href=&#34;#ref-radford-etal-2019-language&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt;) and a masked language model (ALBERT, &lt;span class=&#34;citation&#34;&gt;Lan et al. (&lt;a href=&#34;#ref-lan-etal-2020-albert&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt;) are first tested over three garden-path test suites that are part of the SyntaxGym benchmark to evaluate whether their language modeling surprisal before and after eye-tracking fine-tuning (ET) can be used to predict the presence and the magnitude of garden-path effects over disambiguating regions. In particular, GPT-2 and GPT-2 XL results presented in &lt;span class=&#34;citation&#34;&gt;Hu et al. (&lt;a href=&#34;#ref-hu-etal-2020-systematic&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt; are reproduced. Finally, the same procedure is repeated using predicted eye-tracking scores predicted by models after fine-tuning instead of language modeling surprisal, following the intuition that an accurate model of gaze measurements should predict such phenomena correctly.&lt;/p&gt;
&lt;p&gt;While the usage of surprisal is a common practice for garden-path effect prediction, leveraging eye-tracking scores predicted by a neural language model trained for this purpose is a novel research direction that is deemed interesting as a way to combine the predictive power of modern language models and the strong connection between cognitive processing and gaze metrics. While predicted gaze metrics for garden-path evaluation were used in concurrent studies &lt;span class=&#34;citation&#34;&gt;(Schijndel and Linzen &lt;a href=&#34;#ref-schjindel-linzen-2020-single&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt;, the approach adopted by this work can be regarded as complementary evidence since eye-tracking metrics predictions are produced as results of an end-to-end supervised fine-tuning procedure involving a neural language model rather than being derived from surprisal values through a conversion coefficient. Findings suggest that, while surprisal scores from autoregressive models accurately reflect garden-path structures both before and after fine-tuning, gaze metrics predictions produced by fine-tuned models do not account for the temporary syntactic ambiguity that characterizes such sentences and makes them difficult to process.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;custompar&#34;&gt;Contributions&lt;/span&gt; This study validates the performances of standard and gaze-informed Transformed-based neural language models for garden-path effects prediction. In particular:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;It reproduces the GPT-2 performances on garden-path test suites reported by &lt;span class=&#34;citation&#34;&gt;Gauthier et al. (&lt;a href=&#34;#ref-gauthier-etal-2020-syntaxgym&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt; and highlights how GPT-2 overestimates reading delays caused by garden-path effects on MV/RR and NP/Z constructions.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;It highlights masked language models’ inability to consistently predict garden-path effects, using language modeling surprisal and gaze metrics predictions.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;It introduces a novel gaze metrics multitask token-level fine-tuning approach that, despite being accurate for predicting eye-tracking scores on standard constructions, does not improve models’ performances on garden-path effects predictions.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;subchap:ex3-setup&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;5.1&lt;/span&gt; Experimental Setup&lt;/h2&gt;
&lt;p&gt;&lt;span class=&#34;custompar&#34;&gt;Fine-tuning data&lt;/span&gt; As for the gaze metrics model presented in the previous chapter, all eye-tracking datasets presented in Section &lt;a href=&#34;chap-ling-comp.html#subsubchap:eye-tracking&#34;&gt;1.3.3&lt;/a&gt; were merged and used to fine-tune neural language models using the multitask token-level approach described in Appendix &lt;a href=&#34;app-et-modeling.html#app:et-modeling&#34;&gt;C&lt;/a&gt;. Only the training variant without embedding concatenation (referred to as “surprisal” in the appendix) was evaluated on garden-path test suites given comparable modeling performances.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;custompar&#34;&gt;Models&lt;/span&gt; Two variants of GPT-2 having respectively 117 million and 1.5 billion parameters are evaluated in terms of surprisal-driven predictability, alongside an ALBERT model with 11 million parameters.&lt;a href=&#34;#fn22&#34; class=&#34;footnote-ref&#34; id=&#34;fnref22&#34;&gt;&lt;sup&gt;22&lt;/sup&gt;&lt;/a&gt; Only the small GPT-2 model and the ALBERT model were fine-tuned for gaze metric predictions due to limited computational resources.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;custompar&#34;&gt;Evaluation data&lt;/span&gt; SyntaxGym &lt;span class=&#34;citation&#34;&gt;(Gauthier et al. &lt;a href=&#34;#ref-gauthier-etal-2020-syntaxgym&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt; is a recently introduced online platform designed to make the targeted evaluation of language models on psycholinguistic test suites both accessible and reproducible. The MV/RR and NP/Z test suites containing garden paths from &lt;span class=&#34;citation&#34;&gt;Futrell et al. (&lt;a href=&#34;#ref-futrell-etal-2019-neural&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt; are used in the context of this work. The MV/RR test suite consists of 28 groups containing a sentence with a main verb/reduced relative ambiguity and its non-ambiguous rewritings. In comparison, the NP/Z test suites consist of 24 groups containing a sentence with a nominal/zero predicate ambiguity, produced either by a misinterpreted transitive use of a verb (Verb Transitivity) or the absence of an object for the main verb (Overt Object). Examples (3), (4), and (5) from Section &lt;a href=&#34;chap-ling-comp.html#subchap:garden-path&#34;&gt;1.4&lt;/a&gt; follow the format used in the three SyntaxGym test suites used in this work.&lt;/p&gt;


&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:gpt2-surprisal&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;figures/5_gpt2_surprisal_npz_ambig.png&#34; alt=&#34;Average GPT-2 surprisal predictions and examples for the NP/Z Ambiguity with Verb Transitivity (top), the NP/Z Ambiguity with Overt Object (middle), and the MV/RR Ambiguity (bottom) SyntaxGym test suites used in this study. Star marks the garden-path disambiguator (bold in examples), and bars show 95% confidence intervals.&#34; width=&#34;100%&#34; /&gt;&lt;img src=&#34;figures/5_gpt2_surprisal_npz_obj.png&#34; alt=&#34;Average GPT-2 surprisal predictions and examples for the NP/Z Ambiguity with Verb Transitivity (top), the NP/Z Ambiguity with Overt Object (middle), and the MV/RR Ambiguity (bottom) SyntaxGym test suites used in this study. Star marks the garden-path disambiguator (bold in examples), and bars show 95% confidence intervals.&#34; width=&#34;100%&#34; /&gt;&lt;img src=&#34;figures/5_gpt2_surprisal_mvrr.png&#34; alt=&#34;Average GPT-2 surprisal predictions and examples for the NP/Z Ambiguity with Verb Transitivity (top), the NP/Z Ambiguity with Overt Object (middle), and the MV/RR Ambiguity (bottom) SyntaxGym test suites used in this study. Star marks the garden-path disambiguator (bold in examples), and bars show 95% confidence intervals.&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 5.1: Average GPT-2 surprisal predictions and examples for the NP/Z Ambiguity with Verb Transitivity (top), the NP/Z Ambiguity with Overt Object (middle), and the MV/RR Ambiguity (bottom) SyntaxGym test suites used in this study. Star marks the garden-path disambiguator (bold in examples), and bars show 95% confidence intervals.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;subchap:ex3-experiments&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;5.2&lt;/span&gt; Experimental Evaluation&lt;/h2&gt;
&lt;p&gt;For the first part of the experiments, the smallest version of the model GPT-2 is used. Figure &lt;a href=&#34;chap-ex3.html#fig:gpt2-surprisal&#34;&gt;5.1&lt;/a&gt; reproduces the original setting tested by &lt;span class=&#34;citation&#34;&gt;Hu et al. (&lt;a href=&#34;#ref-hu-etal-2020-systematic&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt;, showing how predictability estimates produced by the model correctly individuate the presence of garden-path effects.&lt;a href=&#34;#fn23&#34; class=&#34;footnote-ref&#34; id=&#34;fnref23&#34;&gt;&lt;sup&gt;23&lt;/sup&gt;&lt;/a&gt; Surprisal values are computed using a pre-trained GPT-2 for all tokens in all sentences of the three test suites. Then, those values are aggregated by summing them across all tokens composing a sentence region. For example, for the NP/Z Ambiguity test suite entry shown in example (a) the region “Start” will be associated with the sum of surprisal estimates for all subword tokens in the sequence &lt;em&gt;While the students&lt;/em&gt;. It is important to note that the four variants of the same sentence have only minimal variations, but only one of those (the underlined one in all examples) is a garden-path sentence. After computing GPT-2 surprisal scores for all regions of all sentences in the test sets, those are averaged region-wise across sentences belonging to the same test set to obtain the three plots presented in Figure &lt;a href=&#34;chap-ex3.html#fig:gpt2-surprisal&#34;&gt;5.1&lt;/a&gt;. The star symbol is used to mark the disambiguating region of garden-path sentences, making evident how predictability estimates are significantly lower (i.e., higher surprisal values) for those and correctly predict the presence of a garden-path effect in most settings and for all the three garden-path variants.&lt;/p&gt;
&lt;div id=&#34;subsubchap:ex3-magnitudes&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;header-section-number&#34;&gt;5.2.1&lt;/span&gt; Estimating Magnitudes of Garden-path Delays&lt;/h3&gt;
&lt;p&gt;An important part of evaluating model predictions over garden-path sentences is determining whether the increase in surprisal scores correctly captures the effect’s magnitude. &lt;span class=&#34;citation&#34;&gt;Schijndel and Linzen (&lt;a href=&#34;#ref-schjindel-linzen-2020-single&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt; perform this evaluation on RNN language models, finding that they vastly underestimate garden-path effects for MV/RR and NP/Z ambiguities. In their approach, &lt;span class=&#34;citation&#34;&gt;Schijndel and Linzen (&lt;a href=&#34;#ref-schjindel-linzen-2020-single&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt; estimate the surprisal-to-reading-times conversion rate at 2ms per surprisal bit by fitting a linear mixed-effect model on relevant factors (surprisal, entropy, word length, among others) relative to a word and its three preceding words to account for spillover effects. The approach adopted in this work is different in that it stems from the empirical relation between surprisal scores produced by GPT-2 and reading times produced by eye-tracking experiments’ participants. Figure &lt;a href=&#34;chap-ex3.html#fig:surprisal-ratios&#34;&gt;5.2&lt;/a&gt; presents the median values over words for the ratio between gaze metrics recorded by participants and GPT-2 surprisal estimates, with the red cross indicating the average median surprisal-to-metric ratio &lt;span class=&#34;math inline&#34;&gt;\(C_{\text{corpus}}^{\text{metric}}\)&lt;/span&gt; computed across all participants of a corpus. The following formula is used to produce the surprisal-to-reading-times conversion coefficient:
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
C_{S\rightarrow RT} = w_1 \cdot C_{\text{GECO}}^{\text{FPD}} + w_2 \cdot C_{\text{Dundee}}^{\text{FPD}} + w_3 \cdot C_{\text{ZuCo NR}}^{\text{FPD}} + w_4 \cdot C_{\text{ZuCo SR}}^{\text{FPD}} + w_5 \cdot C_{\text{ZuCo 2.0}}^{\text{FPD}}
\end{equation}\]&lt;/span&gt;
with &lt;span class=&#34;math inline&#34;&gt;\(w = [.4, .45, .05, .05, .05]\)&lt;/span&gt; being the weighting coefficients representing the proportion of each corpus’ tokens over the total amount of available gaze-annotated tokens.&lt;/p&gt;

&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:surprisal-ratios&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;figures/5_surprisal_ratios.png&#34; alt=&#34;Median scores for the ratio between gaze metrics units and GPT-2 surprisal estimates across all participants of all eye-tracking datasets used in this study. The red cross shows the average across participants of a single dataset. Units are in ms for durations, % for FXP, and raw counts for FXC.&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 5.2: Median scores for the ratio between gaze metrics units and GPT-2 surprisal estimates across all participants of all eye-tracking datasets used in this study. The red cross shows the average across participants of a single dataset. Units are in ms for durations, % for FXP, and raw counts for FXC.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The resulting value for the conversion coefficient is &lt;span class=&#34;math inline&#34;&gt;\(27.7\)&lt;/span&gt;, i.e., &lt;em&gt;each surprisal bit predicted by GPT-2 accounts for roughly 27.7 milliseconds in first pass duration&lt;/em&gt; (30.3ms using TFD). When applied to the average effects predicted by GPT-2 in Figure &lt;a href=&#34;chap-ex3.html#fig:gpt2-surprisal&#34;&gt;5.1&lt;/a&gt;, it leads to an estimated delay of roughly 64ms for the MV/RR setting and 166ms and 194ms for the NP/Z Ambiguity and NP/Z Overt Object settings, respectively. These computed delays overestimate the literature’s effects: &lt;span class=&#34;citation&#34;&gt;Prasad and Linzen (&lt;a href=&#34;#ref-prasad-linzen-2019-self&#34;&gt;2019&lt;/a&gt;&lt;a href=&#34;#ref-prasad-linzen-2019-self&#34;&gt;a&lt;/a&gt;)&lt;/span&gt; and &lt;span class=&#34;citation&#34;&gt;Prasad and Linzen (&lt;a href=&#34;#ref-prasad-linzen-2019-much&#34;&gt;2019&lt;/a&gt;&lt;a href=&#34;#ref-prasad-linzen-2019-much&#34;&gt;b&lt;/a&gt;)&lt;/span&gt;, for example, report an average garden-path effect of 22ms and 27ms for MV/RR and NP/Z variants, respectively. However, it should be mentioned that precedent studies found higher delays for NP/Z structures: &lt;span class=&#34;citation&#34;&gt;Grodner et al. (&lt;a href=&#34;#ref-grodner-etal-2003-against&#34;&gt;2003&lt;/a&gt;)&lt;/span&gt; find a 64ms delay on disambiguating words, and &lt;span class=&#34;citation&#34;&gt;Sturt, Pickering, and Crocker (&lt;a href=&#34;#ref-sturt-etal-1999-structural&#34;&gt;1999&lt;/a&gt;)&lt;/span&gt;‘s delays of 152ms per word are close to the estimates produced by GPT-2 surprisal predictions. Overall, using models’ surprisal on gaze-annotated sentences to directly compute a conversion coefficient produces values that correctly identify delays on disambiguating regions and overestimate the magnitude of garden-path effects conversely to what was found by &lt;span class=&#34;citation&#34;&gt;Schijndel and Linzen (&lt;a href=&#34;#ref-schjindel-linzen-2020-single&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt;. Even with an adjustment of the conversion coefficient to match MV/RR estimates with &lt;span class=&#34;citation&#34;&gt;Prasad and Linzen (&lt;a href=&#34;#ref-prasad-linzen-2019-self&#34;&gt;2019&lt;/a&gt;&lt;a href=&#34;#ref-prasad-linzen-2019-self&#34;&gt;a&lt;/a&gt;)&lt;/span&gt; findings, the NP/Z effect prediction would still be much larger than the empirically-observed values collected in comparable settings.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;subsubchap:ex3-predicting&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;header-section-number&#34;&gt;5.2.2&lt;/span&gt; Predicting Delays with Surprisal and Gaze Metrics&lt;/h3&gt;
&lt;p&gt;The other perspective explored in this study is evaluating whether gaze metric predicted by models fine-tuned on eye-tracking corpora annotations can correctly estimate the presence and magnitude of garden-path effects and how they compare to surprisal-driven approaches. Table &lt;a href=&#34;chap-ex3.html#tab:gp-results&#34;&gt;5.1&lt;/a&gt; presents the accuracy of multiple pre-trained Transformer-based language models in respecting a set of three conditions taken from &lt;span class=&#34;citation&#34;&gt;Hu et al. (&lt;a href=&#34;#ref-hu-etal-2020-systematic&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt; for each SyntaxGym test suite, namely:
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
V_d(b) &amp;lt; V_d(a);\qquad V_d(c) &amp;lt; V_d(a);\qquad V_d(c)-V_d(d) &amp;lt; V_d(a)-V_d(b)
\end{equation}\]&lt;/span&gt;
Where &lt;span class=&#34;math inline&#34;&gt;\(V_d(a)\)&lt;/span&gt; corresponds to the value, either in terms of surprisal or gaze metrics, assigned by a model to the disambiguating region &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; of sentence &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(a,b,c,d\)&lt;/span&gt; are the same sentence’s variants for each test suite presented in examples (3),(4) and (5) of Section &lt;a href=&#34;chap-ling-comp.html#subchap:garden-path&#34;&gt;1.4&lt;/a&gt;. Accuracy is computed as the proportion of items in the test suite on which the language model’s predictions conform to the respective criterion. The first three models (GPT-2, GPT-2 XL, and ALBERT) are the pre-trained variants of the three models presented in Table &lt;a href=&#34;chap-ex3.html#subchap:ex3-setup&#34;&gt;5.1&lt;/a&gt; without additional fine-tuning. Instead, the GPT-2 ET and ALBERT ET models correspond to the same GPT-2 and ALBERT models as before after a multitask token-level fine-tuning on gaze metrics for all the aggregated corpora. The top part of Table &lt;a href=&#34;chap-ex3.html#tab:gp-results&#34;&gt;5.1&lt;/a&gt; shows the five models’ performances while using region-aggregated surprisals as predictors. Focusing on the GPT-2 variants, it can be observed that they all achieve considerably high scores on all evaluated conditions. Conversely, ALBERT masked language models poorly fit the specified criteria. This fact can be intuitively explained by accounting for the different training and evaluation setup used for the two architectures. GPT-2 models are likely to produce high surprisal estimates for garden-path sentences since, processing the input autoregressively and having access only to previous tokens, they incur in the same syntactic ambiguities faced by human readers.&lt;/p&gt;
&lt;table class=&#34;table&#34; style=&#34;font-size: 11px; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;caption style=&#34;font-size: initial !important;&#34;&gt;
&lt;span id=&#34;tab:gp-results&#34;&gt;Table 5.1: &lt;/span&gt;Results of experiments using surprisal and gaze metrics as predictors for garden-path effects on the three SyntaxGym test suites.
&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;border-bottom:hidden&#34; colspan=&#34;1&#34;&gt;
&lt;/th&gt;
&lt;th style=&#34;border-bottom:hidden&#34; colspan=&#34;1&#34;&gt;
&lt;/th&gt;
&lt;th style=&#34;border-bottom:hidden&#34; colspan=&#34;1&#34;&gt;
&lt;/th&gt;
&lt;th style=&#34;border-bottom:hidden; padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; font-weight: bold; &#34; colspan=&#34;3&#34;&gt;
&lt;div style=&#34;border-bottom: 1px solid #ddd; padding-bottom: 5px; &#34;&gt;
NP/Z Verb Transitivity
&lt;/div&gt;
&lt;/th&gt;
&lt;th style=&#34;border-bottom:hidden; padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; font-weight: bold; &#34; colspan=&#34;3&#34;&gt;
&lt;div style=&#34;border-bottom: 1px solid #ddd; padding-bottom: 5px; &#34;&gt;
NP/Z Overt Object
&lt;/div&gt;
&lt;/th&gt;
&lt;th style=&#34;border-bottom:hidden; padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; font-weight: bold; &#34; colspan=&#34;3&#34;&gt;
&lt;div style=&#34;border-bottom: 1px solid #ddd; padding-bottom: 5px; &#34;&gt;
MV/RR Ambiguity
&lt;/div&gt;
&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
Cond. 1 &lt;sup&gt;1&lt;/sup&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
Cond. 2 &lt;sup&gt;2&lt;/sup&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
Cond. 3 &lt;sup&gt;3&lt;/sup&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
Cond. 1 &lt;sup&gt;a&lt;/sup&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
Cond. 2 &lt;sup&gt;b&lt;/sup&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
Cond. 3 &lt;sup&gt;c&lt;/sup&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
Cond. 1 &lt;sup&gt;*&lt;/sup&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
Cond. 2 &lt;sup&gt;†&lt;/sup&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
Cond. 3 &lt;sup&gt;‡&lt;/sup&gt;
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;vertical-align: middle !important;&#34; rowspan=&#34;5&#34;&gt;
Surprisal
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
GPT-2
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.96&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.92&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;border-right:1px solid;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.88&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34; font-weight: bold;    &#34;&gt;0.96&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34; font-weight: bold;    &#34;&gt;1&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;border-right:1px solid;&#34;&gt;
&lt;span style=&#34; font-weight: bold;    &#34;&gt;1&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34; font-weight: bold;    &#34;&gt;1&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34; font-weight: bold;    &#34;&gt;0.89&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34; font-weight: bold;    &#34;&gt;0.82&lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
GPT-2 XL
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34; font-weight: bold;    &#34;&gt;1&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34; font-weight: bold;    &#34;&gt;0.96&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;border-right:1px solid;&#34;&gt;
&lt;span style=&#34; font-weight: bold;    &#34;&gt;1&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34; font-weight: bold;    &#34;&gt;0.96&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34; font-weight: bold;    &#34;&gt;1&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;border-right:1px solid;&#34;&gt;
&lt;span style=&#34; font-weight: bold;    &#34;&gt;1&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.93&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.75&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.75&lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
ALBERT
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.21&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.63&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;border-right:1px solid;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.58&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.21&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.54&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;border-right:1px solid;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.46&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.61&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.54&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.38&lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
GPT-2 ET
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.96&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.88&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;border-right:1px solid;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.79&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34; font-weight: bold;    &#34;&gt;0.96&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34; font-weight: bold;    &#34;&gt;1&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;border-right:1px solid;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.96&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.96&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.79&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34; font-weight: bold;    &#34;&gt;0.82&lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
ALBERT ET
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.42&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.42&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;border-right:1px solid;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.58&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.42&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.75&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;border-right:1px solid;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.62&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.5&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.64&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.64&lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;vertical-align: middle !important;&#34; rowspan=&#34;12&#34;&gt;
Eye-tracking metrics
&lt;/td&gt;
&lt;td style=&#34;text-align:left;vertical-align: middle !important;&#34; rowspan=&#34;6&#34;&gt;
GPT-2 ET
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
FFD
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.29&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.38&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;border-right:1px solid;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.46&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.29&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.54&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;border-right:1px solid;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.42&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.86&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.57&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.5&lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
FPD
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.13&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.46&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;border-right:1px solid;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.67&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.13&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.5&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;border-right:1px solid;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.46&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.86&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.54&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.36&lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
FXP
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.38&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.5&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;border-right:1px solid;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.42&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.42&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.41&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;border-right:1px solid;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.42&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.71&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.43&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.57&lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
FXC
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.75&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.5&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;border-right:1px solid;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.42&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.75&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.63&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;border-right:1px solid;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.46&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.92&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.46&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.54&lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
TFD
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.5&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.33&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;border-right:1px solid;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.46&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.5&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.58&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;border-right:1px solid;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.75&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.79&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.43&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.39&lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
TRD
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.67&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.46&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;border-right:1px solid;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.54&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.63&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.25&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;border-right:1px solid;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.54&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.29&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.39&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.5&lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;vertical-align: middle !important;&#34; rowspan=&#34;6&#34;&gt;
ALBERT ET
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
FFD
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.67&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.33&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;border-right:1px solid;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.42&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.42&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.83&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;border-right:1px solid;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.67&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.68&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.61&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.5&lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
FPD
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.54&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.41&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;border-right:1px solid;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.33&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.38&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.79&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;border-right:1px solid;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.75&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.75&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.57&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.46&lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
FXP
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.28&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.46&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;border-right:1px solid;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.29&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.54&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.38&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;border-right:1px solid;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.63&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.29&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.5&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.43&lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
FXC
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.63&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.46&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;border-right:1px solid;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.5&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.38&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.67&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;border-right:1px solid;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.71&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.86&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.43&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.39&lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
TFD
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.75&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.38&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;border-right:1px solid;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.29&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.5&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.88&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;border-right:1px solid;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.83&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.79&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.61&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.54&lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
TRD
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.96&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.42&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;border-right:1px solid;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.42&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.63&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.75&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;border-right:1px solid;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.5&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.79&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.5&lt;/span&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;span style=&#34;     &#34;&gt;0.57&lt;/span&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;tfoot&gt;
&lt;tr&gt;
&lt;td style=&#34;padding: 0; border: 0;&#34; colspan=&#34;100%&#34;&gt;
&lt;sup&gt;&lt;/sup&gt; Description of the evaluated conditions
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;padding: 0; border: 0;&#34; colspan=&#34;100%&#34;&gt;
&lt;span style=&#34;font-style: italic;text-decoration: underline;&#34;&gt;NP/Z Verb Trans.: &lt;/span&gt; &lt;sup&gt;1&lt;/sup&gt; [Ambig. No Comma] &amp;gt; [Ambig. Comma]; &lt;sup&gt;2&lt;/sup&gt; [Ambig. No Comma] &amp;gt; [Unambig. No Comma]; &lt;sup&gt;3&lt;/sup&gt; [Ambig. No Comma] - [Ambig. Comma] &amp;gt; [Unambig. No Comma] - [Unambig. Comma]
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;padding: 0; border: 0;&#34; colspan=&#34;100%&#34;&gt;
&lt;span style=&#34;font-style: italic;text-decoration: underline;&#34;&gt;NP/Z Overt Obj.: &lt;/span&gt; &lt;sup&gt;a&lt;/sup&gt; [No Obj. No Comma] &amp;gt; [No Obj. Comma]; &lt;sup&gt;b&lt;/sup&gt; [No Obj. No Comma] &amp;gt; [Obj. No Comma]; &lt;sup&gt;c&lt;/sup&gt; [No Obj. No Comma] - [No Obj. Comma] &amp;gt; [Obj. No Comma] - [Obj. Comma]
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;padding: 0; border: 0;&#34; colspan=&#34;100%&#34;&gt;
&lt;span style=&#34;font-style: italic;text-decoration: underline;&#34;&gt;MV/RR Ambig.: &lt;/span&gt; &lt;sup&gt;*&lt;/sup&gt; [Reduced Ambig.] &amp;gt; [Unred. Ambig.]; &lt;sup&gt;†&lt;/sup&gt; [Reduced Ambig.] &amp;gt; [Reduced Unambig.]; &lt;sup&gt;‡&lt;/sup&gt; [Reduced Ambig.] - [Unred. Ambig.] &amp;gt; [Reduced Unambig.] - [Unred. Unambig.]
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tfoot&gt;
&lt;/table&gt;
&lt;p&gt;Conversely, ALBERT-like masked language models have access to bidirectional contexts and are not exposed to the ambiguity. It is interesting to observe that while the eye-tracking fine-tuning procedure appears to hamper GPT-2 surprisal performances, it generally improves the ALBERT model’s accuracy. This phenomenon may be due to the sequential nature of reading that is being captured by gaze metrics and transferred to the bidirectional ALBERT model as a useful bias for sequential processing. The same procedure performs suboptimally, instead, when associated with an inherently autoregressive model like the GPT-2 decoder&lt;/p&gt;
&lt;p&gt;The bottom part of Table &lt;a href=&#34;chap-ex3.html#tab:gp-results&#34;&gt;5.1&lt;/a&gt; presents the two ET-trained models’ accuracy in matching criteria using predicted gaze metrics. For both GPT-2 and ALBERT, it can be observed that gaze metrics vastly underperform in accuracy terms. We can conclude that, despite the conceptual relation between gaze metrics and predictability observed in humans, the predictions of fine-tuned model cannot generalize to unseen settings, and as such &lt;em&gt;eye-tracking predictions obtained after a fine-tuning on standard constructions do not appear useful to individuate or estimate the magnitude of garden-path effects&lt;/em&gt;. This observation suggests that fine-tuned models stick to predicting gaze metric values that are the most likely for each specific token, regardless of the surrounding context’s ambiguities. Plots in Appendix &lt;a href=&#34;app-garden-paths-et.html#app:garden-paths-et&#34;&gt;E&lt;/a&gt; present the region-aggregated average scores for all metrics predicted by GPT-2 ET in the same format as before and show how predictions on the disambiguator regions are unaffected by the presence of previous ambiguities.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;subchap:ex3-summary&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;5.3&lt;/span&gt; Summary&lt;/h2&gt;
&lt;p&gt;This chapter focused on two perspectives related to the evaluation of neural language models for garden-path effects prediction. First, promising results from previous studies using GPT-2 surprisal to evaluate predictability are reproduced, and language modeling surprisal estimates are converted to reading times using a conversion coefficient. Resulting predictions vastly overestimate the magnitude of garden-path effects in all settings, suggesting the presence of additional mechanisms besides predictability in shaping cognitive processing in the presence of ambiguous constructions like garden-path sentences. This evidence is further supported by the second experimental perspective, in which reading times for garden-path sentences are predicted by models fine-tuned on eye-tracking annotations on corpora containing standard constructions. Results suggest that predicted gaze metrics poorly estimate the presence of garden-path effects over disambiguating regions, suggesting that fine-tuned models are once again incapable of out-of-the-box generalization beyond training settings.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;h3&gt;References&lt;/h3&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-elman-1991-distributed&#34;&gt;
&lt;p&gt;Elman, Jeffrey L. 1991. “Distributed Representations, Simple Recurrent Networks, and Grammatical Structure.” &lt;em&gt;Machine Learning&lt;/em&gt; 7 (2-3). Springer: 195–225.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-futrell-etal-2019-neural&#34;&gt;
&lt;p&gt;Futrell, Richard, Ethan Wilcox, Takashi Morita, Peng Qian, Miguel Ballesteros, and Roger Levy. 2019. “Neural Language Models as Psycholinguistic Subjects: Representations of Syntactic State.” In &lt;em&gt;Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)&lt;/em&gt;, 32–42. Minneapolis, Minnesota: Association for Computational Linguistics. &lt;a href=&#34;https://doi.org/10.18653/v1/N19-1004&#34;&gt;https://doi.org/10.18653/v1/N19-1004&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-gauthier-etal-2020-syntaxgym&#34;&gt;
&lt;p&gt;Gauthier, Jon, Jennifer Hu, Ethan Wilcox, Peng Qian, and Roger Levy. 2020. “SyntaxGym: An Online Platform for Targeted Evaluation of Language Models.” In &lt;em&gt;Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations&lt;/em&gt;, 70–76. Online: Association for Computational Linguistics. &lt;a href=&#34;https://doi.org/10.18653/v1/2020.acl-demos.10&#34;&gt;https://doi.org/10.18653/v1/2020.acl-demos.10&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-grodner-etal-2003-against&#34;&gt;
&lt;p&gt;Grodner, Daniel, Edward Gibson, Vered Argaman, and Maria Babyonyshev. 2003. “Against Repair-Based Reanalysis in Sentence Comprehension.” &lt;em&gt;Journal of Psycholinguistic Research&lt;/em&gt; 32 (2). Springer: 141–66.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-gulordava-etal-2018-colorless&#34;&gt;
&lt;p&gt;Gulordava, Kristina, Piotr Bojanowski, Edouard Grave, Tal Linzen, and Marco Baroni. 2018. “Colorless Green Recurrent Networks Dream Hierarchically.” In &lt;em&gt;Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)&lt;/em&gt;, 1195–1205. New Orleans, Louisiana: Association for Computational Linguistics. &lt;a href=&#34;https://doi.org/10.18653/v1/N18-1108&#34;&gt;https://doi.org/10.18653/v1/N18-1108&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-hale-2001-probabilistic&#34;&gt;
&lt;p&gt;Hale, John. 2001. “A Probabilistic Earley Parser as a Psycholinguistic Model.” In &lt;em&gt;Second Meeting of the North American Chapter of the Association for Computational Linguistics&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-hu-etal-2020-systematic&#34;&gt;
&lt;p&gt;Hu, Jennifer, Jon Gauthier, Peng Qian, Ethan Wilcox, and Roger Levy. 2020. “A Systematic Assessment of Syntactic Generalization in Neural Language Models.” In &lt;em&gt;Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics&lt;/em&gt;, 1725–44. Online: Association for Computational Linguistics. &lt;a href=&#34;https://doi.org/10.18653/v1/2020.acl-main.158&#34;&gt;https://doi.org/10.18653/v1/2020.acl-main.158&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-lan-etal-2020-albert&#34;&gt;
&lt;p&gt;Lan, Zhenzhong, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. 2020. “ALBERT: A Lite BERT for Self-Supervised Learning of Language Representations.” In &lt;em&gt;International Conference on Learning Representations&lt;/em&gt;. &lt;a href=&#34;https://openreview.net/forum?id=H1eA7AEtvS&#34;&gt;https://openreview.net/forum?id=H1eA7AEtvS&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-levy-2008-expectation&#34;&gt;
&lt;p&gt;Levy, Roger. 2008. “Expectation-Based Syntactic Comprehension.” &lt;em&gt;Cognition&lt;/em&gt; 106 (3). Elsevier: 1126–77.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-linzen-etal-2016-assessing&#34;&gt;
&lt;p&gt;Linzen, Tal, Emmanuel Dupoux, and Yoav Goldberg. 2016. “Assessing the Ability of Lstms to Learn Syntax-Sensitive Dependencies.” &lt;em&gt;Transactions of the Association for Computational Linguistics&lt;/em&gt; 4. MIT Press: 521–35.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-mikolov-etal-2010-recurrent&#34;&gt;
&lt;p&gt;Mikolov, Tomas, M. Karafiát, L. Burget, J. Cernocký, and S. Khudanpur. 2010. “Recurrent Neural Network Based Language Model.” In &lt;em&gt;INTERSPEECH&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-prasad-linzen-2019-self&#34;&gt;
&lt;p&gt;Prasad, Grusha, and Tal Linzen. 2019a. “Do Self-Paced Reading Studies Provide Evidence for Rapid Syntactic Adaptation?” &lt;em&gt;PsyArXiv Pre-Print&lt;/em&gt;. &lt;a href=&#34;https://tallinzen.net/media/papers/prasad_linzen_2019_adaptation.pdf&#34;&gt;https://tallinzen.net/media/papers/prasad_linzen_2019_adaptation.pdf&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-prasad-linzen-2019-much&#34;&gt;
&lt;p&gt;Prasad, Grusha, and Tal Linzen. 2019b. “How Much Harder Are Hard Garden-Path Sentences Than Easy Ones?” &lt;em&gt;OSF Preprint&lt;/em&gt; syh3j. &lt;a href=&#34;https://osf.io/syh3j/&#34;&gt;https://osf.io/syh3j/&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-radford-etal-2019-language&#34;&gt;
&lt;p&gt;Radford, A., Jeffrey Wu, R. Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. “Language Models Are Unsupervised Multitask Learners.” &lt;em&gt;OpenAI Blog&lt;/em&gt;. OpenAI.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-schjindel-linzen-2020-single&#34;&gt;
&lt;p&gt;Schijndel, Marten van, and Tal Linzen. 2020. “Single-Stage Prediction Models Do Not Explain the Magnitude of Syntactic Disambiguation Difficulty.” &lt;em&gt;PsyArXiv Pre-Print&lt;/em&gt; sgbqy. &lt;a href=&#34;https://psyarxiv.com/sgbqy/&#34;&gt;https://psyarxiv.com/sgbqy/&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-sturt-etal-1999-structural&#34;&gt;
&lt;p&gt;Sturt, Patrick, Martin J Pickering, and Matthew W Crocker. 1999. “Structural Change and Reanalysis Difficulty in Language Comprehension.” &lt;em&gt;Journal of Memory and Language&lt;/em&gt; 40 (1). Elsevier: 136–50.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol start=&#34;22&#34;&gt;
&lt;li id=&#34;fn22&#34;&gt;&lt;p&gt;The &lt;code&gt;gpt2&lt;/code&gt;, &lt;code&gt;gpt2-xl&lt;/code&gt; and &lt;code&gt;albert-base-v2&lt;/code&gt; pre-trained models from 🤗 &lt;code&gt;transformers&lt;/code&gt; &lt;span class=&#34;citation&#34;&gt;(Wolf et al. &lt;a href=&#34;#ref-wolf-etal-2020-huggingface&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt;.&lt;a href=&#34;chap-ex3.html#fnref22&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn23&#34;&gt;&lt;p&gt;Similar plots are available on the SyntaxGym website: &lt;a href=&#34;http://syntaxgym.org/viz/individual&#34; class=&#34;uri&#34;&gt;http://syntaxgym.org/viz/individual&lt;/a&gt;&lt;a href=&#34;chap-ex3.html#fnref23&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
            &lt;/section&gt;

          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
&lt;a href=&#34;chap-ex2.html&#34; class=&#34;navigation navigation-prev &#34; aria-label=&#34;Previous page&#34;&gt;&lt;i class=&#34;fa fa-angle-left&#34;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&#34;conclusion.html&#34; class=&#34;navigation navigation-next &#34; aria-label=&#34;Next page&#34;&gt;&lt;i class=&#34;fa fa-angle-right&#34;&gt;&lt;/i&gt;&lt;/a&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/app.min.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/lunr.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/clipboard.min.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/plugin-search.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/plugin-sharing.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/plugin-fontsettings.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/plugin-bookdown.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/jquery.highlight.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/plugin-clipboard.js&#34;&gt;&lt;/script&gt;
&lt;script&gt;
gitbook.require([&#34;gitbook&#34;], function(gitbook) {
gitbook.start({
&#34;sharing&#34;: {
&#34;github&#34;: true,
&#34;facebook&#34;: true,
&#34;twitter&#34;: true,
&#34;linkedin&#34;: true,
&#34;weibo&#34;: false,
&#34;instapaper&#34;: false,
&#34;vk&#34;: false,
&#34;all&#34;: false
},
&#34;fontsettings&#34;: {
&#34;theme&#34;: &#34;white&#34;,
&#34;family&#34;: &#34;sans&#34;,
&#34;size&#34;: 2
},
&#34;edit&#34;: {
&#34;link&#34;: &#34;https://github.com/gsarti/master-thesis/tree/master/05-Cognitive-Phenomena.Rmd&#34;,
&#34;text&#34;: &#34;Edit&#34;
},
&#34;history&#34;: {
&#34;link&#34;: null,
&#34;text&#34;: null
},
&#34;view&#34;: {
&#34;link&#34;: null,
&#34;text&#34;: null
},
&#34;download&#34;: [[&#34;Sarti_2020_Interpreting_NLMs_for_LCA.pdf&#34;, &#34;PDF&#34;]],
&#34;toc&#34;: {
&#34;collapse&#34;: &#34;subsection&#34;,
&#34;scroll_highlight&#34;: true
},
&#34;info&#34;: false
});
});
&lt;/script&gt;

&lt;!-- dynamically load mathjax for compatibility with self-contained --&gt;
&lt;script&gt;
  (function () {
    var script = document.createElement(&#34;script&#34;);
    script.type = &#34;text/javascript&#34;;
    var src = &#34;true&#34;;
    if (src === &#34;&#34; || src === &#34;true&#34;) src = &#34;https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML&#34;;
    if (location.protocol !== &#34;file:&#34;)
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, &#39;&#39;);
    script.src = src;
    document.getElementsByTagName(&#34;head&#34;)[0].appendChild(script);
  })();
&lt;/script&gt;
&lt;/body&gt;

&lt;/html&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>http://localhost:1313/msc-thesis/chap-ling-comp/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/msc-thesis/chap-ling-comp/</guid>
      <description>&lt;!DOCTYPE html&gt;
&lt;html lang=&#34;&#34; xml:lang=&#34;&#34;&gt;
&lt;head&gt;

  &lt;meta charset=&#34;utf-8&#34; /&gt;
  &lt;meta http-equiv=&#34;X-UA-Compatible&#34; content=&#34;IE=edge&#34; /&gt;
  &lt;title&gt;1 Linguistic Complexity | Interpreting Neural Language Models for Linguistic Complexity Assessment&lt;/title&gt;
  &lt;meta name=&#34;description&#34; content=&#34;MSc Thesis at the University of Trieste and SISSA under the supervision of Prof. Davide Crepaldi and Dott. Felice dell&#39;Orletta&#34; /&gt;
  &lt;meta name=&#34;generator&#34; content=&#34;bookdown 0.20.6 and GitBook 2.6.7&#34; /&gt;

  &lt;meta property=&#34;og:title&#34; content=&#34;1 Linguistic Complexity | Interpreting Neural Language Models for Linguistic Complexity Assessment&#34; /&gt;
  &lt;meta property=&#34;og:type&#34; content=&#34;book&#34; /&gt;
  &lt;meta property=&#34;og:url&#34; content=&#34;https://gsarti.com/master-thesis&#34; /&gt;
  &lt;meta property=&#34;og:image&#34; content=&#34;https://gsarti.com/master-thesisfigures/cover.png&#34; /&gt;
  &lt;meta property=&#34;og:description&#34; content=&#34;MSc Thesis at the University of Trieste and SISSA under the supervision of Prof. Davide Crepaldi and Dott. Felice dell&#39;Orletta&#34; /&gt;
  &lt;meta name=&#34;github-repo&#34; content=&#34;gsarti/interpreting-complexity&#34; /&gt;

  &lt;meta name=&#34;twitter:card&#34; content=&#34;summary&#34; /&gt;
  &lt;meta name=&#34;twitter:title&#34; content=&#34;1 Linguistic Complexity | Interpreting Neural Language Models for Linguistic Complexity Assessment&#34; /&gt;
  
  &lt;meta name=&#34;twitter:description&#34; content=&#34;MSc Thesis at the University of Trieste and SISSA under the supervision of Prof. Davide Crepaldi and Dott. Felice dell&#39;Orletta&#34; /&gt;
  &lt;meta name=&#34;twitter:image&#34; content=&#34;https://gsarti.com/master-thesisfigures/cover.png&#34; /&gt;

&lt;meta name=&#34;author&#34; content=&#34;Gabriele Sarti&#34; /&gt;



  &lt;meta name=&#34;viewport&#34; content=&#34;width=device-width, initial-scale=1&#34; /&gt;
  &lt;meta name=&#34;apple-mobile-web-app-capable&#34; content=&#34;yes&#34; /&gt;
  &lt;meta name=&#34;apple-mobile-web-app-status-bar-style&#34; content=&#34;black&#34; /&gt;
  &lt;link rel=&#34;apple-touch-icon-precomposed&#34; sizes=&#34;152x152&#34; href=&#34;figures/icons/apple-icon.png&#34; /&gt;
  &lt;link rel=&#34;shortcut icon&#34; href=&#34;figures/icons/favicon.ico&#34; type=&#34;image/x-icon&#34; /&gt;
&lt;link rel=&#34;prev&#34; href=&#34;introduction.html&#34;/&gt;
&lt;link rel=&#34;next&#34; href=&#34;chap-models.html&#34;/&gt;
&lt;style type=&#34;text/css&#34;&gt;
p.abstract{
  text-align: center;
  font-weight: bold;
}
div.abstract{
  margin: auto;
  width: 90%;
}
&lt;/style&gt;
&lt;script src=&#34;libs/jquery-2.2.3/jquery.min.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;libs/gitbook-2.6.7/css/style.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;link href=&#34;libs/gitbook-2.6.7/css/plugin-table.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;link href=&#34;libs/gitbook-2.6.7/css/plugin-bookdown.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;link href=&#34;libs/gitbook-2.6.7/css/plugin-highlight.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;link href=&#34;libs/gitbook-2.6.7/css/plugin-search.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;link href=&#34;libs/gitbook-2.6.7/css/plugin-fontsettings.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;link href=&#34;libs/gitbook-2.6.7/css/plugin-clipboard.css&#34; rel=&#34;stylesheet&#34; /&gt;









&lt;script src=&#34;libs/kePrint-0.0.1/kePrint.js&#34;&gt;&lt;/script&gt;



&lt;link rel=&#34;stylesheet&#34; href=&#34;templates/style.css&#34; type=&#34;text/css&#34; /&gt;
&lt;/head&gt;

&lt;body&gt;



  &lt;div class=&#34;book without-animation with-summary font-size-2 font-family-1&#34; data-basepath=&#34;.&#34;&gt;

    &lt;div class=&#34;book-summary&#34;&gt;
      &lt;nav role=&#34;navigation&#34;&gt;

&lt;ul class=&#34;summary&#34;&gt;
&lt;li&gt;&lt;a href=&#34;introduction.html#introduction&#34;&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1&lt;/b&gt; &lt;strong&gt;Linguistic Complexity&lt;/strong&gt;&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.1&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subchap:categorizing&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.1&lt;/b&gt; Categorizing Linguistic Complexity Measures&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.2&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subchap:intrinsic&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.2&lt;/b&gt; Intrinsic Perspective&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.2.1&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subsubchap:structural&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.2.1&lt;/b&gt; Structural Linguistic Complexity&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.2.2&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subsubchap:lm-surprisal&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.2.2&lt;/b&gt; Language Modeling Surprisal&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.3&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subchap:extrinsic&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.3&lt;/b&gt; Extrinsic Perspective&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.3.1&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subsubchap:readability&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.3.1&lt;/b&gt; Automatic Readability Assessment&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.3.2&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subsubchap:pc&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.3.2&lt;/b&gt; Perceived Complexity Prediction&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.3.3&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subsubchap:eye-tracking&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.3.3&lt;/b&gt; Gaze Metrics Prediction&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.4&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subchap:garden-path&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.4&lt;/b&gt; Garden-path Sentences&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2&lt;/b&gt; &lt;strong&gt;Models of Linguistic Complexity&lt;/strong&gt;&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2.1&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html#subchap:desiderata&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2.1&lt;/b&gt; Desiderata for Models of Linguistic Complexity&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2.2&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html#subchap:nlm&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2.2&lt;/b&gt; Neural Language Models: Unsupervised Multitask Learners&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2.2.1&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html#subsubchap:syntax-nlm&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2.2.1&lt;/b&gt; Emergent Linguistic Structures in Neural Language Models&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2.3&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html#subchap:analyzing-nlm&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2.3&lt;/b&gt; Analyzing Neural Models of Complexity&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2.3.1&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html#subsubchap:probe&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2.3.1&lt;/b&gt; Probing classifiers&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2.3.2&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html#subsubchap:rsa&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2.3.2&lt;/b&gt; Representational Similarity Analysis&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2.3.3&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html#subsubchap:pwcca&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2.3.3&lt;/b&gt; Projection-Weighted Canonical Correlation Analysis&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3&lt;/b&gt; &lt;strong&gt;Complexity Phenomena in Linguistic Annotations and Language Models&lt;/strong&gt;&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3.1&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html#subchap:ex1-data&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3.1&lt;/b&gt; Data and Preprocessing&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3.2&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html#subchap:ex1-analysis&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3.2&lt;/b&gt; Analysis of Linguistic Phenomena&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3.2.1&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html#subsubchap:ex1-analysis-bins&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3.2.1&lt;/b&gt; Linguistic Phenomena in Length-controlled Bins&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3.3&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html#subchap:ex1-modeling&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3.3&lt;/b&gt; Modeling Online and Offline Linguistic Complexity&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3.3.1&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html#subsubchap:ex1-modeling-bins&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3.3.1&lt;/b&gt; Modeling Complexity in Length-controlled Bins&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3.4&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html#subchap:ex1-probing&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3.4&lt;/b&gt; Probing Linguistic Phenomena in ALBERT Representations&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3.5&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html#subchap:ex1-summary&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3.5&lt;/b&gt; Summary&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;4&#34; data-path=&#34;chap-ex2.html&#34;&gt;&lt;a href=&#34;chap-ex2.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;4&lt;/b&gt; &lt;strong&gt;Representational Similarity in Models of Complexity&lt;/strong&gt;&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;4.1&#34; data-path=&#34;chap-ex2.html&#34;&gt;&lt;a href=&#34;chap-ex2.html#knowledge-driven-requirements-for-learning-models&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;4.1&lt;/b&gt; Knowledge-driven Requirements for Learning Models&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;4.2&#34; data-path=&#34;chap-ex2.html&#34;&gt;&lt;a href=&#34;chap-ex2.html#subchap:ex2-experiments&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;4.2&lt;/b&gt; Experimentsl Evaluation&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;4.2.1&#34; data-path=&#34;chap-ex2.html&#34;&gt;&lt;a href=&#34;chap-ex2.html#subsubchap:ex2-data&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;4.2.1&lt;/b&gt; Data&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;4.2.2&#34; data-path=&#34;chap-ex2.html&#34;&gt;&lt;a href=&#34;chap-ex2.html#subsubchap:ex2-inter&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;4.2.2&lt;/b&gt; Inter-model Representational Similarity&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;4.2.3&#34; data-path=&#34;chap-ex2.html&#34;&gt;&lt;a href=&#34;chap-ex2.html#subsubchap:ex2-intra&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;4.2.3&lt;/b&gt; Intra-model Representational Similarity&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;4.3&#34; data-path=&#34;chap-ex2.html&#34;&gt;&lt;a href=&#34;chap-ex2.html#subchap:ex2-summary&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;4.3&lt;/b&gt; Summary&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;5&#34; data-path=&#34;chap-ex3.html&#34;&gt;&lt;a href=&#34;chap-ex3.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;5&lt;/b&gt; &lt;strong&gt;Gaze-informed Models for Cognitive Processing Prediction&lt;/strong&gt;&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;5.1&#34; data-path=&#34;chap-ex3.html&#34;&gt;&lt;a href=&#34;chap-ex3.html#subchap:ex3-setup&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;5.1&lt;/b&gt; Experimental Setup&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;5.2&#34; data-path=&#34;chap-ex3.html&#34;&gt;&lt;a href=&#34;chap-ex3.html#subchap:ex3-experiments&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;5.2&lt;/b&gt; Experimental Evaluation&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;5.2.1&#34; data-path=&#34;chap-ex3.html&#34;&gt;&lt;a href=&#34;chap-ex3.html#subsubchap:ex3-magnitudes&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;5.2.1&lt;/b&gt; Estimating Magnitudes of Garden-path Delays&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;5.2.2&#34; data-path=&#34;chap-ex3.html&#34;&gt;&lt;a href=&#34;chap-ex3.html#subsubchap:ex3-predicting&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;5.2.2&lt;/b&gt; Predicting Delays with Surprisal and Gaze Metrics&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;5.3&#34; data-path=&#34;chap-ex3.html&#34;&gt;&lt;a href=&#34;chap-ex3.html#subchap:ex3-summary&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;5.3&lt;/b&gt; Summary&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;conclusion.html#conclusion&#34;&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;&#34; data-path=&#34;conclusion.html&#34;&gt;&lt;a href=&#34;conclusion.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;Broader Impact and Ethical Perspectives&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;&#34; data-path=&#34;conclusion.html&#34;&gt;&lt;a href=&#34;conclusion.html#future-directions&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;Future Directions&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;appendix&#34;&gt;&lt;span&gt;&lt;b&gt;Appendix&lt;/b&gt;&lt;/span&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;A&#34; data-path=&#34;app-ling-feats.html&#34;&gt;&lt;a href=&#34;app-ling-feats.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;A&lt;/b&gt; Linguistic Features&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;A.1&#34; data-path=&#34;app-ling-feats.html&#34;&gt;&lt;a href=&#34;app-ling-feats.html#raw-text-properties-and-lexical-variety&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;A.1&lt;/b&gt; Raw Text Properties and Lexical Variety&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;A.2&#34; data-path=&#34;app-ling-feats.html&#34;&gt;&lt;a href=&#34;app-ling-feats.html#morpho-syntacting-information&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;A.2&lt;/b&gt; Morpho-syntacting Information&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;A.3&#34; data-path=&#34;app-ling-feats.html&#34;&gt;&lt;a href=&#34;app-ling-feats.html#verbal-predicate-structure&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;A.3&lt;/b&gt; Verbal Predicate Structure&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;A.4&#34; data-path=&#34;app-ling-feats.html&#34;&gt;&lt;a href=&#34;app-ling-feats.html#global-and-local-parsed-tree-structures&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;A.4&lt;/b&gt; Global and Local Parsed Tree Structures&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;A.5&#34; data-path=&#34;app-ling-feats.html&#34;&gt;&lt;a href=&#34;app-ling-feats.html#syntactic-relations&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;A.5&lt;/b&gt; Syntactic Relations&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;A.6&#34; data-path=&#34;app-ling-feats.html&#34;&gt;&lt;a href=&#34;app-ling-feats.html#subordination-phenomena&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;A.6&lt;/b&gt; Subordination Phenomena&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;B&#34; data-path=&#34;app-et-metrics.html&#34;&gt;&lt;a href=&#34;app-et-metrics.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;B&lt;/b&gt; Precisions on Eye-tracking Metrics and Preprocessing&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;C&#34; data-path=&#34;app-et-modeling.html&#34;&gt;&lt;a href=&#34;app-et-modeling.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;C&lt;/b&gt; Multi-task Token-level Regression for Gaze Metrics Prediction&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;D&#34; data-path=&#34;app-intra-sim.html&#34;&gt;&lt;a href=&#34;app-intra-sim.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;D&lt;/b&gt; Intra-model Similarity for All Models&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;E&#34; data-path=&#34;app-garden-paths-et.html&#34;&gt;&lt;a href=&#34;app-garden-paths-et.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;E&lt;/b&gt; Gaze Metrics Predictions for Garden Path Sentences&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;F&#34; data-path=&#34;app-params.html&#34;&gt;&lt;a href=&#34;app-params.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;F&lt;/b&gt; Reproducibility and Environmental Impact&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;&#34; data-path=&#34;references.html&#34;&gt;&lt;a href=&#34;references.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;References&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;divider&#34;&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://gsarti.com&#34;&gt;Back to my website&lt;/a&gt;&lt;/li&gt;

&lt;/ul&gt;

      &lt;/nav&gt;
    &lt;/div&gt;

    &lt;div class=&#34;book-body&#34;&gt;
      &lt;div class=&#34;body-inner&#34;&gt;
        &lt;div class=&#34;book-header&#34; role=&#34;navigation&#34;&gt;
          &lt;h1&gt;
            &lt;i class=&#34;fa fa-circle-o-notch fa-spin&#34;&gt;&lt;/i&gt;&lt;a href=&#34;./&#34;&gt;Interpreting Neural Language Models&lt;br /&gt;
for Linguistic Complexity Assessment&lt;/a&gt;
          &lt;/h1&gt;
        &lt;/div&gt;

        &lt;div class=&#34;page-wrapper&#34; tabindex=&#34;-1&#34; role=&#34;main&#34;&gt;
          &lt;div class=&#34;page-inner&#34;&gt;

            &lt;section class=&#34;normal&#34; id=&#34;section-&#34;&gt;
&lt;div id=&#34;chap:ling-comp&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;1&lt;/span&gt; &lt;strong&gt;Linguistic Complexity&lt;/strong&gt;&lt;/h1&gt;
&lt;p&gt;&lt;!--this will include a mini table of contents--&gt;&lt;/p&gt;

&lt;p&gt;Defining linguistic complexity in a univocal way is challenging, despite the subjective intuition that every individual may have about what should be deemed complex in written or spoken language. Indeed, if the faculty of language allows us to produce a possibly infinite set of sentences from a finite vocabulary, there are infinitely many ways in which a sentence may appear difficult to a reader’s eyes. An accurate definition is still debated in research fields like cognitive science, psycholinguistics, and computational linguistics. Nonetheless, it is indisputable that the concept of natural language complexity is closely related to difficulties in knowledge acquisition. This property stands both for human language learners and for computational models learning the distributional behavior of words in a corpus.&lt;/p&gt;
&lt;p&gt;This introductory chapter begins with a categorization of linguistic complexity annotations following taxonomical definitions found in the literature. Various complexity metrics are then introduced alongside corpora and resources that were used throughout this study. Finally, the focus will be put on garden-path sentences, peculiar syntactically-ambiguous constructs studied in the experiments of Chapter &lt;a href=&#34;chap-ex3.html#chap:ex3&#34;&gt;5&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;subchap:categorizing&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.1&lt;/span&gt; Categorizing Linguistic Complexity Measures&lt;/h2&gt;
&lt;p&gt;In modern literature about linguistic complexity, two positions, each trying to define the nature of linguistic complexity phenomena, can be identified. In &lt;span class=&#34;citation&#34;&gt;Kusters (&lt;a href=&#34;#ref-kusters-2008-complexity&#34;&gt;2008&lt;/a&gt;)&lt;/span&gt; words:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;On the one hand, complexity is used as a theory-internal concept, or linguistic tool, that refers only indirectly, by way of the theory, to language reality. On the other hand, complexity is defined as an empirical phenomenon, not part of, but to be explained by a theory.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;
These definitions are coherent with the &lt;strong&gt;absolute&lt;/strong&gt; and &lt;strong&gt;relative complexity&lt;/strong&gt; terminology coined by &lt;span class=&#34;citation&#34;&gt;Miestamo (&lt;a href=&#34;#ref-miestamo-2004-feasibility&#34;&gt;2004&lt;/a&gt;)&lt;/span&gt;, where relative complexity is seen as a factor characterizing the perceptual experience of specific language users. In contrast, absolute complexity is structurally-defined by language constructs and independent from user evaluation. While these two perspectives seem to identify two opposite viewpoints over linguistic complexity, the distinction between the two becomes blurred when we consider that linguistic theories underlying absolute complexity evaluation are developed by linguists, who still have a subjective perspective despite their competence &lt;span class=&#34;citation&#34;&gt;(Kusters &lt;a href=&#34;#ref-kusters-2003-linguistic&#34;&gt;2003&lt;/a&gt;)&lt;/span&gt;. Two definitions are now introduced to operationalize absolute and relative complexity in the context of complexity measurements:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;custompar&#34;&gt;Intrinsic Perspective&lt;/span&gt; The intrinsic perspective on linguistic complexity is closely related to the notion of absolute complexity. From the intrinsic viewpoint, language productions are evaluated using their distributional and structural properties, without any complexity annotation derived by language users. The linguistic system is characterized by a set of elementary components (lexicon, morphology, syntax &lt;em&gt;inter alia&lt;/em&gt;) that interact hierarchically &lt;span class=&#34;citation&#34;&gt;(Cangelosi and Turner &lt;a href=&#34;#ref-cangelosi-turner-2002-emergere&#34;&gt;2002&lt;/a&gt;)&lt;/span&gt;, and their interactions can be measured in terms of complexity by fixing a set of rules and descriptions. The focus is on objectivity and automatic evaluation based on the intrinsic properties of language systems.&lt;/p&gt;

&lt;p&gt;&lt;span class=&#34;custompar&#34;&gt;Extrinsic Perspective&lt;/span&gt; The extrinsic perspective connects to the concept of relative complexity and takes into account the individual perspective of users. Complexity judgments are collected during or after the processing of linguistic productions and are then evaluated in terms of cognitive effort required by language users for comprehension. The extrinsic viewpoint is partaken by cognitive processing theories in psycholinguistics such as the Dependency Locality Theory &lt;span class=&#34;citation&#34;&gt;(Gibson &lt;a href=&#34;#ref-gibson-1998-linguistic&#34;&gt;1998&lt;/a&gt;, &lt;a href=&#34;#ref-gibson-2000-dependency&#34;&gt;2000&lt;/a&gt;)&lt;/span&gt;, the Surprisal Theory &lt;span class=&#34;citation&#34;&gt;(Hale &lt;a href=&#34;#ref-hale-2001-probabilistic&#34;&gt;2001&lt;/a&gt;, &lt;a href=&#34;#ref-hale-2016-information&#34;&gt;2016&lt;/a&gt;; Levy &lt;a href=&#34;#ref-levy-2008-expectation&#34;&gt;2008&lt;/a&gt;)&lt;/span&gt;, and the more recent Lossy-context Surprisal Theory &lt;span class=&#34;citation&#34;&gt;(Futrell, Gibson, and Levy &lt;a href=&#34;#ref-futrell-2020-lossy&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt;, aiming to disentangle the source of processing difficulties in sentence comprehension. The focus, in this case, is on the subjectivity of language users and their judgments.&lt;/p&gt;

&lt;p&gt;Despite being different under many aspects, the two perspectives are highly interdependent: a user’s perception of complexity will be strongly influenced by the distributional and structural properties of utterances, and some of those properties will be considered complex in relation to the type of judgments they typically elicit in language users. Provided that the strength of human influence in complexity measurements can vary widely depending on data collection procedures, the two perspectives can be seen as the two ends of a spectrum. A visual representation is provided by the horizontal axis of the complexity measures compass in Figure &lt;a href=&#34;chap-ling-comp.html#fig:compass&#34;&gt;1.1&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:compass&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;figures/1_complexity_compass.png&#34; alt=&#34;Complexity measures&#39; compass.&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1.1: Complexity measures’ compass.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;An additional dimension for categorizing linguistic complexity metrics can be introduced by considering the time at which measures are obtained, relative to the incremental processing paradigm that characterizes natural reading in human subjects. In this context, &lt;em&gt;processing&lt;/em&gt; is defined as any act aimed at extracting information from linguistic forms and structures, either by employing reasoning (in humans) or through computation (in automatic systems). Again, we can identify the two ends of a spectrum concerning processing modalities, related to the concepts of &lt;strong&gt;local&lt;/strong&gt; and &lt;strong&gt;global complexity&lt;/strong&gt; found in linguistic literature &lt;span class=&#34;citation&#34;&gt;(Edmonds &lt;a href=&#34;#ref-edmonds-1999-syntactic&#34;&gt;1999&lt;/a&gt;; Miestamo &lt;a href=&#34;#ref-miestamo-2004-feasibility&#34;&gt;2004&lt;/a&gt;, &lt;a href=&#34;#ref-miestamo-2008-grammatical&#34;&gt;2008&lt;/a&gt;)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;custompar&#34;&gt;Online processing&lt;/span&gt; Online complexity judgments are collected while a language user, be it a human subject or a computational system, is sequentially processing a text. Online processing is widely explored in the cognitive science literature, where behavioral metrics such are fMRI data and gaze recordings are collected from subjects exposed to locally and temporally-immediate inputs and tasks that require fast processing &lt;span class=&#34;citation&#34;&gt;(Iverson and Thelen &lt;a href=&#34;#ref-iverson-thelen-1999-hand&#34;&gt;1999&lt;/a&gt;)&lt;/span&gt;. The act of reading is predominantly performed by online cognition &lt;span class=&#34;citation&#34;&gt;(Meyer and Rice &lt;a href=&#34;#ref-meyer-rice-1992-prose&#34;&gt;1992&lt;/a&gt;)&lt;/span&gt;, making online measures especially suitable for complexity evaluation for natural reading.&lt;/p&gt;

&lt;p&gt;&lt;span class=&#34;custompar&#34;&gt;Offline processing&lt;/span&gt; Offline complexity judgments are collected at a later time when the language user has a complete and contextual view of the text in its entirety. Again, offline complexity is related to the offline cognition paradigm &lt;span class=&#34;citation&#34;&gt;(Day &lt;a href=&#34;#ref-day-2004-religion&#34;&gt;2004&lt;/a&gt;)&lt;/span&gt; typically used in re-evaluations and future planning. In practice, offline evaluation accounts for contextual and cultural factors closely related to individual subjectivity and is poorly captured by immediate online metrics.&lt;/p&gt;

&lt;p&gt;Figure &lt;a href=&#34;chap-ling-comp.html#fig:compass&#34;&gt;1.1&lt;/a&gt; situates various linguistic complexity metrics in terms of processing modalities and analyzed perspective by including the processing spectrum on the vertical axis. In the next sections, all these measures will be introduced and their use will be motivated in light of this categorization.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;subchap:intrinsic&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.2&lt;/span&gt; Intrinsic Perspective&lt;/h2&gt;
&lt;p&gt;Complexity studies where the intrinsic point of view is adopted rely on annotations describing linguistic phenomena and structures in sentences and aim to map those to complexity levels or ratings, often resorting to formulas parametrized through empirical observation. Given the scarcity of experienced human annotators and the cost of a manual annotation process, computational systems have been primarily employed to extract linguistic information from raw text in an automated yet precise way.&lt;/p&gt;
&lt;p&gt;Another intrinsic viewpoint is based on the intuition that frequent constructs should be deemed as less complex than infrequent ones. In this case, terms’ co-occurrences are extracted from large corpora, and complexity judgments are derived from their probabilistic likelihood of appearance in a given context. Given the infeasibility of tracking co-occurrences for long sequences in large, typologically-varied corpora, &lt;strong&gt;computational language models&lt;/strong&gt; are usually employed to learn approximations of co-occurrence likelihoods for specific constructs.&lt;/p&gt;
&lt;p&gt;While this thesis work only partially addresses the use of these approaches, they will be briefly introduced to provide additional context for understanding extrinsic perspectives and their experimental evaluation.&lt;/p&gt;
&lt;div id=&#34;subsubchap:structural&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.2.1&lt;/span&gt; Structural Linguistic Complexity&lt;/h3&gt;
&lt;p&gt;Language systems can be seen as hierarchies of rules and processes governing various aspects of utterances production and use. For each of those levels, it is possible to identify characteristics leading to higher complexity from a structural standpoint &lt;span class=&#34;citation&#34;&gt;(Sinnemäki &lt;a href=&#34;#ref-sinnemaki-2011-language&#34;&gt;2011&lt;/a&gt;)&lt;/span&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;A greater number of parts in a specific language level leads to a greater &lt;strong&gt;syntagmatic complexity&lt;/strong&gt; (also known as &lt;em&gt;constitutional complexity&lt;/em&gt;). This mode is related to the &lt;em&gt;lexical&lt;/em&gt; and “superficial” properties of language, such as the length of words and sentences.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;A greater variety of parts in a specific language level leads to a greater &lt;strong&gt;paradigmatic complexity&lt;/strong&gt; (also known as &lt;em&gt;taxonomic complexity&lt;/em&gt;). This mode characterizes, in particular, the &lt;em&gt;phonological&lt;/em&gt; level, where the presence of an elaborated tonal system makes a language more complex &lt;span class=&#34;citation&#34;&gt;(McWhorter &lt;a href=&#34;#ref-mcwhorter-2001-world&#34;&gt;2001&lt;/a&gt;)&lt;/span&gt;, the &lt;em&gt;morphologic&lt;/em&gt; level, where inflectional morphology is usually associated to a higher degree of complexity &lt;span class=&#34;citation&#34;&gt;(McWhorter &lt;a href=&#34;#ref-mcwhorter-2001-world&#34;&gt;2001&lt;/a&gt;; Kusters &lt;a href=&#34;#ref-kusters-2003-linguistic&#34;&gt;2003&lt;/a&gt;)&lt;/span&gt; when compared to the regularity of derivational rules, and the &lt;em&gt;semantic&lt;/em&gt; level, where polysemic words are generally considered more complex than monosemic ones &lt;span class=&#34;citation&#34;&gt;(Voghera &lt;a href=&#34;#ref-voghera-2001-riflessioni&#34;&gt;2001&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;A greater variety of interrelation modalities and hierarchical structures leads to greater &lt;strong&gt;organizational and hierarchical complexities&lt;/strong&gt;. Those complexity modes are mainly related to the &lt;em&gt;syntactic level&lt;/em&gt;, where recursive and nested constructs are deemed more complex and possibly determinant in distinguishing human language from animal communication &lt;span class=&#34;citation&#34;&gt;(Hauser, Chomsky, and Fitch &lt;a href=&#34;#ref-hauser-2002-faculty&#34;&gt;2002&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Focusing on the syntactic level, we can find multiple factors accounting for greater complexity &lt;span class=&#34;citation&#34;&gt;(Berruto and Cerruti &lt;a href=&#34;#ref-berruto-2011-linguistica&#34;&gt;2011&lt;/a&gt;)&lt;/span&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Subordinate clauses preceding the main clause, as in &lt;em&gt;&lt;span class=&#34;underline&#34;&gt;If you need help&lt;/span&gt;, let me know&amp;quot;&lt;/em&gt; as opposed to &lt;em&gt;“Let me know &lt;span class=&#34;underline&#34;&gt;if you need help&lt;/span&gt;”&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Presence of long-range syntactic dependencies between non-contiguous elements, as in &lt;em&gt;“&lt;span class=&#34;underline&#34;&gt;The dog&lt;/span&gt; that the cat chased for days &lt;span class=&#34;underline&#34;&gt;ran away&lt;/span&gt;”&lt;/em&gt; where the subject referent (&lt;em&gt;dog&lt;/em&gt;) and its verb (&lt;em&gt;ran&lt;/em&gt;) are far apart in the sentence.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;A high degree of nesting between elements and substructures, as in &lt;em&gt;“The mouse &lt;strong&gt;that the cat&lt;/strong&gt; &lt;span class=&#34;underline&#34;&gt;that the dog bit&lt;/span&gt; &lt;strong&gt;ate&lt;/strong&gt; was bought at the fair”&lt;/em&gt; where two nested subordinate clauses introduced by the preposition &lt;em&gt;that&lt;/em&gt; are present.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Repeated applications of recursive principles to build utterances with different meanings through the compositionality principle, as in &lt;em&gt;“I am a huge fan &lt;span class=&#34;underline&#34;&gt;of fans&lt;/span&gt; of fans of … of recursion”&lt;/em&gt;, where the number of recursions defines the final meaning of the sentence.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;While all those properties are relevant when evaluating an utterance’s complexity, only some can be easily extracted from corpora using automatic approaches. In the specific context of this work, the analysis of complexity-related features in Chapter &lt;a href=&#34;chap-ex1.html#chap:ex1&#34;&gt;3&lt;/a&gt; makes use of the Profiling–UD tool&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; &lt;span class=&#34;citation&#34;&gt;(Brunato et al. &lt;a href=&#34;#ref-brunato-etal-2020-profiling&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt;, implementing a two-stage process: first, the linguistic annotation process is automatically performed by UDPipe &lt;span class=&#34;citation&#34;&gt;(Straka, Hajič, and Straková &lt;a href=&#34;#ref-straka-etal-2016-udpipe&#34;&gt;2016&lt;/a&gt;)&lt;/span&gt;, a multilingual pipeline leveraging neural parsers and taggers included in the Universal Dependencies initiative &lt;span class=&#34;citation&#34;&gt;(Nivre et al. &lt;a href=&#34;#ref-nivre-etal-2016-universal&#34;&gt;2016&lt;/a&gt;)&lt;/span&gt;. During this step, sentences are tokenized, lemmatized, POS-tagged (i.e., words are assigned lexical categories such as “Noun” and “Verb”) and parsed (i.e., the hierarchical structure of syntactic dependencies is inferred). Then, a set of about 130 linguistic features representing underlying linguistic properties of sentences is extracted from various levels of annotation. Those features account for multiple morphological, syntactic, and “superficial” properties related to linguistic complexity. A relevant subset of those features is presented in detail in Appendix &lt;a href=&#34;app-ling-feats.html#app:ling-feats&#34;&gt;A&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;After deriving linguistic properties from sentences, either automatically as in this study or by manual annotations, two approaches are viable to determine their complexity while maintaining an intrinsic perspective (no human processing data involved):&lt;/p&gt;

&lt;p&gt;&lt;span class=&#34;custompar&#34;&gt;Formula-based Approach&lt;/span&gt; This approach treats linguistic properties of input texts as components of a formula used to determine levels or readability grades. Traditional readability formulas consider multiple factors, such as word length, sentence length, and word frequency. Parameters in those formulas are carefully hand-tuned to match human intuition and correlate well with human-graded readability levels.&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;span class=&#34;custompar&#34;&gt;Learning-based Approach&lt;/span&gt; This approach casts the complexity prediction problem in the supervised machine learning framework. More specifically, linguistic parsers are used to predict linguistic properties, and their accuracy on a set of gold-labeled instances is taken as an indicator of complexity. In the case of dependency parsers (i.e., models trained to extract the syntactic structure of a sentence), two evaluation metrics can be used: the &lt;em&gt;Unlabeled and Labeled Attachment Scores&lt;/em&gt; (UAS and LAS), where the UAS is the percentage of words assigned to the right dependency head and LAS also consider if the dependency relation was labeled correctly.&lt;/p&gt;
&lt;p&gt;Both approaches are represented in Figure &lt;a href=&#34;chap-ling-comp.html#fig:compass&#34;&gt;1.1&lt;/a&gt; under the label “Property-based Automatic LCA” and are considered offline since the text is generally not processed incrementally but instead taken as a whole.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;subsubchap:lm-surprisal&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.2.2&lt;/span&gt; Language Modeling Surprisal&lt;/h3&gt;
&lt;p&gt;The information-theoretic concept of &lt;strong&gt;surprisal&lt;/strong&gt;, also known as &lt;em&gt;information content&lt;/em&gt; of an event, can be seen as a quantification of the level of surprise caused by a specific outcome: an event that is certain yields no information, while the less probable an event is, the more surprising it gets. Formally, an event &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; with probability &lt;span class=&#34;math inline&#34;&gt;\(p(x)\)&lt;/span&gt; has a surprisal value equal to:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
I(x) = - \log[p(x)]
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The idea that probabilistic expectations in the context of language reading are related to greater complexity in terms of cognitive processing was formalized by &lt;em&gt;surprisal theory&lt;/em&gt; &lt;span class=&#34;citation&#34;&gt;(Hale &lt;a href=&#34;#ref-hale-2001-probabilistic&#34;&gt;2001&lt;/a&gt;, &lt;a href=&#34;#ref-hale-2016-information&#34;&gt;2016&lt;/a&gt;)&lt;/span&gt;. Surprisal theory defines processing difficulties &lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt; (which can be considered as proxies of complexity) as directly proportional to the surprisal produced in readers by a word &lt;span class=&#34;math inline&#34;&gt;\(w\)&lt;/span&gt; given its previous context &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; (i.e., preceding words in the sentence):&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
D(w_i|c) \propto -\log p(w_i|c) = -\log p(w_i|w_i-1, w_i-2,\dots, w_0)
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;While processing difficulties imply human subjects’ presence, &lt;strong&gt;language models&lt;/strong&gt; (LM) can be used to estimate the conceptually similar information-theoretic surprisal without the need of human annotations by learning word occurrences and co-occurrences probabilities from large quantities of text. Concretely, a language model is a probabilistic classifier that learns to predict a probability distribution over words of a vocabulary &lt;span class=&#34;math inline&#34;&gt;\(V\)&lt;/span&gt; given a large number of contexts &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; in which those words occur &lt;span class=&#34;citation&#34;&gt;(Goodman &lt;a href=&#34;#ref-goodman-2001-bit&#34;&gt;2001&lt;/a&gt;)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
p(w_i|c) \quad \forall\ w_i\in V
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;After the training procedure it is possible to estimate the probability &lt;span class=&#34;math inline&#34;&gt;\(p(s)\)&lt;/span&gt; of a sentence &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt; having length &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; as the product of the conditional probabilities assigned to individual words by the language model, given its context:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34; id=&#34;eq:sent-surprisal&#34;&gt;\[\begin{equation}
p(s) = p(w_1, \dots, w_m) = \prod_{i=1}^m p(w_i \ |c)
\tag{1.1}
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We can consider the surprisal &lt;span class=&#34;math inline&#34;&gt;\(I(s) = -\log p(s)\)&lt;/span&gt; as an &lt;em&gt;intrinsic measure&lt;/em&gt; of linguistic complexity since it is a function of the co-occurrence relations derived by the training corpora. Thus, it describes how likely a construct can be observed in a structurally-sound manner, without relying on human processing data. However, automatic surprisal estimation using language models cannot be considered purely intrinsic since it is highly dependent on a multitude of factors that are arguably “less objective” than the linguistic categories of the previous section, such as the type and dimension of the considered context and the corpora employed by the LM to learn words’ distributional behavior.&lt;/p&gt;
&lt;p&gt;We can categorize modern language models in two broad categories: &lt;strong&gt;sequential&lt;/strong&gt; models (also known as &lt;em&gt;autoregressive&lt;/em&gt; or &lt;em&gt;causal&lt;/em&gt; LMs) consider as context only preceding words, while &lt;strong&gt;bidirectional&lt;/strong&gt; models (also known as &lt;em&gt;masked&lt;/em&gt; LMs) consider both preceding and following words when estimating occurrence probabilities, much like the well-established &lt;em&gt;cloze test&lt;/em&gt; &lt;span class=&#34;citation&#34;&gt;(Taylor &lt;a href=&#34;#ref-taylor-1953-cloze&#34;&gt;1953&lt;/a&gt;)&lt;/span&gt; in psycholinguistics. Equations &lt;a href=&#34;chap-ling-comp.html#eq:sent-surprisal-cases&#34;&gt;(1.2)&lt;/a&gt; show how the sentence surprisal equation &lt;a href=&#34;chap-ling-comp.html#eq:sent-surprisal&#34;&gt;(1.1)&lt;/a&gt; is adapted in both cases, using the product rule for logarithms:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34; id=&#34;eq:sent-surprisal-cases&#34;&gt;\[\begin{equation}
\begin{split}
I_{\mathrm{sequential}}(s) &amp;amp; = - \sum_{i=1}^m \log p(w_i \ | w_1, w_2, \dots, w_{i-1})\\
I_{\mathrm{bidirectional}}(s) &amp;amp; = - \sum_{i=1}^m \log p(w_i \ | w_1, \dots, w_{i-1}, w_{i+1}, \dots, w_m)
\end{split}
\tag{1.2}
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;If the LM used to estimate surprisal was sequential, then surprisal estimation could be considered part of the &lt;em&gt;online processing paradigm&lt;/em&gt; despite the absence of a human subject.&lt;a href=&#34;#fn4&#34; class=&#34;footnote-ref&#34; id=&#34;fnref4&#34;&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt; In the bidirectional case, the estimation of surprisals from the whole context can be assimilated with offline processing practices.&lt;/p&gt;
&lt;p&gt;The relation between co-occurrence frequencies estimated by a language model and perception of complexity is one of the aspects that make language models especially suitable for predicting extrinsic complexity metrics, as it will be discussed in Chapter &lt;a href=&#34;chap-models.html#chap:models&#34;&gt;2&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;subchap:extrinsic&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.3&lt;/span&gt; Extrinsic Perspective&lt;/h2&gt;
&lt;p&gt;Extrinsic complexity measures elicited from human-produced signals and annotations are the main focus of this thesis work. In this section, three different viewpoints on linguistic complexity assessment from a human perspective are introduced:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The &lt;strong&gt;readability&lt;/strong&gt; point-of-view, as intended in the context of the &lt;em&gt;automatic readability assessment&lt;/em&gt; (ARA) task, is concerned with collocating similar textual inputs into difficulty levels that are often predetermined by writers and given a clear semantic interpretation (e.g., easy, medium, hard).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The &lt;strong&gt;perceptual&lt;/strong&gt; point-of-view, represented by the &lt;em&gt;perceived complexity prediction&lt;/em&gt; (PCP) task, is based on human annotations of complexity on a numeric scale, taking into account disparate textual inputs presented sequentially to obtain more generalizable complexity annotations. Unlike ARA, PCP annotations are produced by readers after sentence comprehension.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The &lt;strong&gt;cognitive&lt;/strong&gt; point-of-view, employing cognitive signals collected by specialized machinery (e.g., electrodes, MRI scanners, eye-trackers) as proxies for the linguistic complexity experienced by users. In this work, the focus will be on the &lt;em&gt;gaze metrics prediction&lt;/em&gt; task, using gaze data collected from subjects during natural reading.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;All three complexity-related tasks will be introduced alongside recent results in the literature. The corpora on which each task relies upon will also be presented in their respective sections.&lt;/p&gt;
&lt;div id=&#34;subsubchap:readability&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.3.1&lt;/span&gt; Automatic Readability Assessment&lt;/h3&gt;
&lt;p&gt;While the term &lt;em&gt;readability assessment&lt;/em&gt; is often broadly employed to denote the task of predicting the general reading difficulty of a text, here it is used to describe the typical approach in ARA, relying on corpora categorized by the writer’s perception of what is difficult for readers.&lt;/p&gt;
&lt;p&gt;We can take as an example the OneStopEnglish (OSE) corpus &lt;span class=&#34;citation&#34;&gt;(Vajjala and Lučić &lt;a href=&#34;#ref-vajjala-lucic-2018-onestopenglish&#34;&gt;2018&lt;/a&gt;)&lt;/span&gt;, which will be used later to study the ARA relation with other complexity tasks in Chapter &lt;a href=&#34;chap-ex2.html#chap:ex2&#34;&gt;4&lt;/a&gt;. OSE contains 567 weekly articles from The Guardian newspaper rewritten by language teachers to suit three adult English learners’ levels. Each text can be divided into passages spanning one or multiple sentences, each labeled with a readability level (“Elementary”, “Intermediate” or “Advanced”) based on the original writers’ judgment. An example of the same passage at different reading levels is provided in Table &lt;a href=&#34;chap-ling-comp.html#tab:ose-example&#34;&gt;1.1&lt;/a&gt;.&lt;/p&gt;
&lt;table class=&#34;table&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;caption&gt;
&lt;span id=&#34;tab:ose-example&#34;&gt;Table 1.1: &lt;/span&gt;An OSE Corpus passage at different reading levels.
&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;font-weight: bold;&#34;&gt;
Reading Level
&lt;/th&gt;
&lt;th style=&#34;text-align:left;font-weight: bold;&#34;&gt;
Example
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Advanced (Adv)
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 29em; &#34;&gt;
Amsterdam still looks liberal to tourists, who were recently assured by the Labour Mayor that the city’s marijuana-selling coffee shops would stay open despite a new national law tackling drug tourism. But the Dutch capital may lose its reputation for tolerance over plans to dispatch nuisance neighbours
to scum villages made from shipping containers.
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Intermediate (Int)
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 29em; &#34;&gt;
To tourists, Amsterdam still seems very liberal. Recently the city’s Mayor assured them that the city’s marijuana-selling coffee shops would stay open despite a new national law to prevent drug tourism. But the Dutch capitals plans to send nuisance neighbours to scum villages made from shipping containers may damage its reputation for tolerance.
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Elementary (Ele)
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 29em; &#34;&gt;
To tourists, Amsterdam still seems very liberal. Recently the city’s Mayor told them that the coffee shops that sell marijuana would stay open, although there is a new national law to stop drug tourism. But the Dutch capital has a plan to send antisocial neighbours to scum villages made from shipping containers, and so maybe now people wont think it is a liberal city any more.
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;From Table &lt;a href=&#34;chap-ling-comp.html#tab:ose-example&#34;&gt;1.1&lt;/a&gt; example, it is evident that the reading level of a specific text should be interpreted only in relation to its other versions, i.e., elementary passages are not necessarily straightforward in absolute terms, but rather &lt;em&gt;less complicated than their intermediate and advanced counterparts&lt;/em&gt;. This affirmation holds for the OSE corpus and other widely-used readability corpora such as the Newsela corpus &lt;span class=&#34;citation&#34;&gt;(Xu, Callison-Burch, and Napoles &lt;a href=&#34;#ref-xu-etal-2015-problems&#34;&gt;2015&lt;/a&gt;)&lt;/span&gt;, which contains newspaper articles rewritten by experts to match eleven school grade reading levels. For this reason, and because of its writer-centric perspective relying only on readability judgments formulated by the same writers who composed the passages, readability assessment is fundamentally different from the other extrinsic approaches.&lt;a href=&#34;#fn5&#34; class=&#34;footnote-ref&#34; id=&#34;fnref5&#34;&gt;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt; ARA can be framed as a machine learning task in which a computational model &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; is trained to predict the readability level &lt;span class=&#34;math inline&#34;&gt;\(y \in \mathcal{Y}\)&lt;/span&gt; over a set of labeled examples &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{S} = (s_1, s_2, \dots, s_n)\)&lt;/span&gt; in two possible ways:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;A simple multiclass classification setting, where the model predicts the level of a single sentence &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt;. In this case, the model outputs a prediction &lt;span class=&#34;math inline&#34;&gt;\(m(s) = \hat y \in \mathcal{Y}\)&lt;/span&gt;. We can then minimize the categorical cross-entropy &lt;span class=&#34;math inline&#34;&gt;\(H(y, \hat y)\)&lt;/span&gt; between gold and predicted labels during the training process and evaluate the model’s performances with standard classification metrics such as precision and recall. This approach is similar to the ones used for other extrinsic metrics but does not account for readability levels’ relative nature.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;A multiple-choice scenario, where the model is provided with two semantically equivalent sentences &lt;span class=&#34;math inline&#34;&gt;\(s_1, s_2\)&lt;/span&gt; at different readability levels (&lt;span class=&#34;math inline&#34;&gt;\(s_1 \equiv s_2, y_1 \neq y_2\)&lt;/span&gt;) and needs to predict which of the sentences has the highest readability level. In this case, which is more coherent with the relative nature of readability judgments, the model is trained to minimize the binary cross-entropy between gold and predicted labels &lt;span class=&#34;math inline&#34;&gt;\(y, \hat y \in \mathcal{Y}_{bin} = \{0,1\}\)&lt;/span&gt; corresponding to the position of the more complex sentence in the pair.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Expert annotations’ effectiveness in determining readers’ comprehension was recently questioned, as automatic readability scoring did not show a significant correlation to comprehension scores of participants, at least for the OSE Corpus &lt;span class=&#34;citation&#34;&gt;(Vajjala and Lucic &lt;a href=&#34;#ref-vajjala-lucic-2019-understanding&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt;. However, measuring if this observation holds for other corpora and extrinsic approaches is beyond this thesis’s scope.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;subsubchap:pc&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.3.2&lt;/span&gt; Perceived Complexity Prediction&lt;/h3&gt;
&lt;p&gt;While ARA measures linguistic complexity in a context-relative and writer-centric sense, the &lt;em&gt;perceived complexity prediction&lt;/em&gt; (PCP) approach focuses on eliciting absolute complexity judgments directly from target readers, aiming at evaluating difficulties in comprehension rather than production. This approach was pioneered by &lt;span class=&#34;citation&#34;&gt;Brunato et al. (&lt;a href=&#34;#ref-brunato-etal-2018-sentence&#34;&gt;2018&lt;/a&gt;)&lt;/span&gt;, who collected crowdsourced complexity ratings from native speakers for Italian and English sentences and evaluated how different structural linguistic properties contribute to human complexity perception. The use of annotators recruited on a crowdsourcing platform was intended to better grasp the layman’s perspective on linguistic complexity, as opposed to ARA expert writers. If collected properly, crowdsourced annotations were shown to be highly reliable for linguistics and computational linguistics research by the survey of &lt;span class=&#34;citation&#34;&gt;Munro et al. (&lt;a href=&#34;#ref-munro-etal-2010-crowdsourcing&#34;&gt;2010&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;citation&#34;&gt;Brunato et al. (&lt;a href=&#34;#ref-brunato-etal-2018-sentence&#34;&gt;2018&lt;/a&gt;)&lt;/span&gt; extracted 1200 sentences from both the newspaper sections of the Italian Universal Dependency Treebank (IUDT) &lt;span class=&#34;citation&#34;&gt;(Simi, Bosco, and Montemagni &lt;a href=&#34;#ref-simi-etal-2014-less&#34;&gt;2014&lt;/a&gt;)&lt;/span&gt; and the Penn Treebank &lt;span class=&#34;citation&#34;&gt;(McDonald et al. &lt;a href=&#34;#ref-mcdonald-etal-2013-universal&#34;&gt;2013&lt;/a&gt;)&lt;/span&gt;, such that those are equally distributed in term of length. To collect human complexity judgments, twenty native speakers were recruited for each language on a crowdsourcing platform. Annotators had to rate each sentence’s difficulty on a Likert 7-point scale, with 1 meaning “very simple” and 7 “very complex”. Sentences were randomly shuffled and presented in groups of five per web page, with annotators being given a minimum of ten seconds to complete each page to prevent skimming. The quality of annotations was measured using the Krippendorff alpha reliability, obtaining 26% and 24% for Italian and English. Table &lt;a href=&#34;chap-ling-comp.html#tab:pc-example&#34;&gt;1.2&lt;/a&gt; presents an example of English sentences labeled with multiple annotators’ perceived complexity judgments.&lt;/p&gt;
&lt;table class=&#34;table&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;caption&gt;
&lt;span id=&#34;tab:pc-example&#34;&gt;Table 1.2: &lt;/span&gt;Sample of sentences taken from the English portion of the Perceived Complexity (PC) Corpus with complexity scores from crowdsourced annotators.
&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;font-weight: bold;&#34;&gt;
Sentence
&lt;/th&gt;
&lt;th style=&#34;text-align:center;font-weight: bold;&#34;&gt;
A1
&lt;/th&gt;
&lt;th style=&#34;text-align:center;font-weight: bold;&#34;&gt;
A2
&lt;/th&gt;
&lt;th style=&#34;text-align:center;font-weight: bold;&#34;&gt;
A3
&lt;/th&gt;
&lt;th style=&#34;text-align:center;font-weight: bold;&#34;&gt;
…
&lt;/th&gt;
&lt;th style=&#34;text-align:center;font-weight: bold;&#34;&gt;
A20
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;width: 25em; &#34;&gt;
In other European markets, share prices closed sharply higher in Frankfurt and Zurich and posted moderate rises in Stockholm, Amsterdam and Milan.
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
…
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;width: 25em; &#34;&gt;
The pound strengthened to $ 1.5795 from $ 1.5765.
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
…
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;width: 25em; &#34;&gt;
In Connecticut, however, most state judges are appointed by the governor and approved by the state legislature.
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
…
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
5
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;width: 25em; &#34;&gt;
When the market stabilized, he added, the firm sold the bonds and quickly paid the loans back.
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
…
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
3
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;width: 25em; &#34;&gt;
Paribas already holds about 18.7 % of Navigation Mixte, and the acquisition of the additional 48 % would cost it about 11 billion francs under its current bid.
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
…
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
6
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;As can be expected, PC judgments show significant variability across participants since they cannot be easily framed in a relative setting. Since this work’s focus is related to a general notion of complexity, PC judgments are averaged and filtered to obtain a score reflecting the mean perception of complexity of all participants in experimental chapters. The averaged score is later treated as the gold label in a regression task, with machine learning models trained to minimize the &lt;em&gt;mean square error&lt;/em&gt; between their predictions and gold average annotations. Another possibility, which is not explored in this thesis work, would be to consider only single participants’ judgments to model their linguistic complexity perception.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;subsubchap:eye-tracking&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.3.3&lt;/span&gt; Gaze Metrics Prediction&lt;/h3&gt;
&lt;p&gt;Gaze data collected from human subjects during reading can provide us with useful insights from an online extrinsic complexity perspective. Patterns found in both &lt;em&gt;saccades&lt;/em&gt;, i.e., eye movements from one location to another, and &lt;em&gt;fixations&lt;/em&gt;, where eyes are relatively stable while fixating a specific region, were shown to be reliably linked to a multitude of linguistic factors &lt;span class=&#34;citation&#34;&gt;(Demberg and Keller &lt;a href=&#34;#ref-demberg-keller-2008-data&#34;&gt;2008&lt;/a&gt;)&lt;/span&gt;. Because of this, a linking assumption between overt attention and mental processing can be reasonably established, and gaze metrics can be considered as proxies of cognitive effort, and thus of complexity, at various processing levels.&lt;a href=&#34;#fn6&#34; class=&#34;footnote-ref&#34; id=&#34;fnref6&#34;&gt;&lt;sup&gt;6&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Gaze metrics are widely employed in cognitive processing research because of their multiple benefits: optical eye-tracking systems are non-invasive and relatively inexpensive compared to other approaches that directly measure brain activity, such as electroencephalography (EEG) and all magnetic resonance imaging (MRI) variants. Moreover, gaze data generally have high spatial and temporal precision, limited only by sampling rates, which are generally in the order of few milliseconds. This aspect is crucial for reading research since it allows us to directly associate gaze measures to specific &lt;em&gt;areas of interest&lt;/em&gt; (AOI, also called region), i.e., small portions of the visual input provided to participants.&lt;/p&gt;

&lt;p&gt;&lt;span class=&#34;custompar&#34;&gt;Gaze data for NLP&lt;/span&gt; Eye-tracking data and other cognitive signals were effectively used in many NLP applications such as POS tagging &lt;span class=&#34;citation&#34;&gt;(Barrett et al. &lt;a href=&#34;#ref-barrett-etal-2016-weakly&#34;&gt;2016&lt;/a&gt;)&lt;/span&gt;, sentiment analysis &lt;span class=&#34;citation&#34;&gt;(Mishra, Dey, and Bhattacharyya &lt;a href=&#34;#ref-mishra-etal-2017-learning&#34;&gt;2017&lt;/a&gt;)&lt;/span&gt;, native language identification &lt;span class=&#34;citation&#34;&gt;(Berzak, Katz, and Levy &lt;a href=&#34;#ref-berzak-etal-2018-assessing&#34;&gt;2018&lt;/a&gt;)&lt;/span&gt;, and dependency parsing &lt;span class=&#34;citation&#34;&gt;(Strzyz, Vilares, and Gómez-Rodríguez &lt;a href=&#34;#ref-strzyz-etal-2019-towards&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt; &lt;em&gt;inter alia&lt;/em&gt;, often providing modest yet consistent improvements across models and tasks through the combination of gaze features and linguistic features or distributed representations.&lt;a href=&#34;#fn7&#34; class=&#34;footnote-ref&#34; id=&#34;fnref7&#34;&gt;&lt;sup&gt;7&lt;/sup&gt;&lt;/a&gt; In the context of linguistic complexity assessment, eye-tracking data were applied to the ARA task for both monolingual and bilingual participants, obtaining meaningful results for sentence-level classification in easy and hard-to-read categories &lt;span class=&#34;citation&#34;&gt;(Vasishth, Malsburg, and Engelmann &lt;a href=&#34;#ref-vasishth-etal-2013-what&#34;&gt;2013&lt;/a&gt;; Ambati, Reddy, and Steedman &lt;a href=&#34;#ref-ambati-etal-2016-assessing&#34;&gt;2016&lt;/a&gt;)&lt;/span&gt;. For example, &lt;span class=&#34;citation&#34;&gt;Singh et al. (&lt;a href=&#34;#ref-singh-etal-2016-quantifying&#34;&gt;2016&lt;/a&gt;)&lt;/span&gt; first use a set of linguistic features to learn a reading times model from a set of gaze-annotated sentences and then use models’ predicted times over a second set of sentences to perform multiple-choice ARA. &lt;span class=&#34;citation&#34;&gt;González-Garduño and Søgaard (&lt;a href=&#34;#ref-gonzalez-garduno-sogaard-2018-learning&#34;&gt;2018&lt;/a&gt;)&lt;/span&gt; extend this approach in a multitask learning setting &lt;span class=&#34;citation&#34;&gt;(Caruana &lt;a href=&#34;#ref-caruana-1997-multitask&#34;&gt;1997&lt;/a&gt;; Ruder &lt;a href=&#34;#ref-ruder-2017-overview&#34;&gt;2017&lt;/a&gt;)&lt;/span&gt;, using eye-movement prediction tasks to produce models able to predict readability levels both from a native speaker and foreign language learner perspective.&lt;/p&gt;

&lt;p&gt;&lt;span class=&#34;custompar&#34;&gt;Collecting Eye-tracking Data&lt;/span&gt; A typical procedure to collect gaze data for reading research, as described by &lt;span class=&#34;citation&#34;&gt;Schotter (&lt;a href=&#34;#ref-schotter-2020-eyetracking&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt;, usually includes the following steps:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Textual inputs are selected and split by experiment designers, first in areas of interest directly mapped to pixels (for natural reading, usually word boundaries), then over multiple rows, and finally in screens presented to participants. This step should take into account calibration errors to determine the correct level of tolerance for off-word fixations.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;A participant is placed in a room with a display computer used to present visual inputs and a host computer used to record data from the eye-tracker setup. Optical eye-trackers use infrared light beams, which are reflected differently by different parts of the eye, to measure pupil and corneal reflection and track gaze movements at each timestep. The setup is calibrated and validated for each participant to ensure the quality of results.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Each participant follows the on-screen instructions to complete a reading task trial while remaining at a fixed distance from the screen. A &lt;em&gt;fixation report&lt;/em&gt; containing events (saccades, fixations, blinks) is produced for each individual on the host computer.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Finally, a data preprocessing step is taken for each trial to identify and remove artifacts and possibly decide to reject the trial. Some examples of standard practices are the merge of fixations below 80ms due to eye jittering, the exclusion of fixations caused by track loss after blinks, and vertical drift correction &lt;span class=&#34;citation&#34;&gt;(Carr et al. &lt;a href=&#34;#ref-carr-etal-2020-algorithms&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt;. An &lt;em&gt;AOI report&lt;/em&gt; containing gaze metrics grouped at AOI level can be produced.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;custompar&#34;&gt;Eye-tracking Metrics&lt;/span&gt; Metrics derived from the AOI report contain information about the processing phases in which subjects incur during sentence comprehension. &lt;em&gt;Early gaze measures&lt;/em&gt; capture information about lexical access and early processing of syntactic structures, while &lt;em&gt;late measures&lt;/em&gt; are more likely to reflect comprehension and both syntactic and semantic disambiguation &lt;span class=&#34;citation&#34;&gt;(Demberg and Keller &lt;a href=&#34;#ref-demberg-keller-2008-data&#34;&gt;2008&lt;/a&gt;)&lt;/span&gt;. The third kind of measures, referred to as &lt;em&gt;contextual&lt;/em&gt; following the categorization in &lt;span class=&#34;citation&#34;&gt;Hollenstein and Zhang (&lt;a href=&#34;#ref-hollenstein-zhang-2019-entity&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt;, capture information from surrounding content. Table &lt;a href=&#34;chap-ling-comp.html#tab:et-metrics&#34;&gt;1.3&lt;/a&gt; presents a subset of metrics, spanning the three categories, that will be used in the experimental section.&lt;a href=&#34;#fn8&#34; class=&#34;footnote-ref&#34; id=&#34;fnref8&#34;&gt;&lt;sup&gt;8&lt;/sup&gt;&lt;/a&gt; These metrics represent a minimal group spanning various stages of the reading process and are leveraged to study differences between online and offline processing among extrinsic metrics. In the experimental part, gaze scores are often averaged across participants to reduce noise in measurements and obtain a single label for each metric that can later be used as a reference in a regression setting. The average fixation probability across participants for each AOI is a value comprised in the range &lt;span class=&#34;math inline&#34;&gt;\([0,1]\)&lt;/span&gt; and represents the proportion of subjects that accessed the region during their first gaze pass.&lt;/p&gt;
&lt;table class=&#34;table&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;caption&gt;
&lt;span id=&#34;tab:et-metrics&#34;&gt;Table 1.3: &lt;/span&gt;Eye-tracking metrics used in this study.
&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;font-weight: bold;&#34;&gt;
Type
&lt;/th&gt;
&lt;th style=&#34;text-align:left;font-weight: bold;&#34;&gt;
Metric Name
&lt;/th&gt;
&lt;th style=&#34;text-align:left;font-weight: bold;&#34;&gt;
Description
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;vertical-align: top !important;&#34; rowspan=&#34;3&#34;&gt;
Early
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
First Fixation Duration (FFD)
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 17em; &#34;&gt;
Duration of the first fixation over the region, including single fixations.
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
First Pass Duration (FPD)
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 17em; &#34;&gt;
Duration of the first pass over a region.
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Fixation Probability (FXP)
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 17em; &#34;&gt;
Boolean value reflecting if the region was fixated or skipped during the first pass.
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;vertical-align: top !important;&#34; rowspan=&#34;2&#34;&gt;
Late
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Fixation Count (FXC)
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 17em; &#34;&gt;
Number of total fixations over a region.
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Total Fixation Duration (TFD)
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 17em; &#34;&gt;
Sum of all fixation durations over a region.
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Contextual
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Total Regression Duration (TRD)
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 17em; &#34;&gt;
Duration of regressive saccades performed after a region’s first access and before going past it.
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;span class=&#34;custompar&#34;&gt;Eye-tracking Corpora&lt;/span&gt; The experimental part of this thesis work leverages four widely used eye-tracking resources: the Dundee corpus &lt;span class=&#34;citation&#34;&gt;(Kennedy, Hill, and Pynte &lt;a href=&#34;#ref-kennedy-etal-2003-dundee&#34;&gt;2003&lt;/a&gt;)&lt;/span&gt;, the GECO corpus &lt;span class=&#34;citation&#34;&gt;(Cop et al. &lt;a href=&#34;#ref-cop-etal-2017-presenting&#34;&gt;2017&lt;/a&gt;)&lt;/span&gt;, the ZuCo corpus &lt;span class=&#34;citation&#34;&gt;(Hollenstein et al. &lt;a href=&#34;#ref-hollenstein-2018-zuco&#34;&gt;2018&lt;/a&gt;)&lt;/span&gt;, and ZuCo 2.0 &lt;span class=&#34;citation&#34;&gt;(Hollenstein, Troendle, et al. &lt;a href=&#34;#ref-hollenstein-etal-2020-zuco&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt;. There are multiple reasons behind the choice of using multiple gaze-annotated corpora for this study. First, those corpora span different domains and provide us with a better intuition of what structures are perceived as complex in different settings and by different pools of subjects. Secondly, neural-network-based complexity models used in this work greatly benefit from a broader availability of annotated data to achieve higher performances in predicting eye-tracking metrics. Finally, while all corpora relied on different procedures and instrumentation, they are all derived from very similar experimental settings (i.e., natural reading on multiple lines), and can be easily merged after an individual normalization procedure &lt;span class=&#34;citation&#34;&gt;(Hollenstein and Zhang &lt;a href=&#34;#ref-hollenstein-zhang-2019-entity&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt;. Table &lt;a href=&#34;chap-ling-comp.html#tab:et-corpora&#34;&gt;1.4&lt;/a&gt; presents some descriptive statistics of the four corpora.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The &lt;strong&gt;Dundee Corpus&lt;/strong&gt; developed by &lt;span class=&#34;citation&#34;&gt;Kennedy, Hill, and Pynte (&lt;a href=&#34;#ref-kennedy-etal-2003-dundee&#34;&gt;2003&lt;/a&gt;)&lt;/span&gt; contains gaze data for ten native English speakers tasked with reading twenty newspaper articles from &lt;em&gt;The Independent&lt;/em&gt;. The English section of the Dundee corpus includes 51,240 tokens in 2368 sentences. Texts were presented to subjects on a screen five lines at a time and recorded using a &lt;em&gt;Dr. Bois Oculometer Eyetracker&lt;/em&gt; with 1 kHz monocular (right) sampling. Dundee corpus data are the oldest among selected corpora and have been extensively used in psycholinguistic research about naturalistic reading.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The &lt;strong&gt;Ghent Eye-tracking Corpus&lt;/strong&gt; (GECO) by &lt;span class=&#34;citation&#34;&gt;Cop et al. (&lt;a href=&#34;#ref-cop-etal-2017-presenting&#34;&gt;2017&lt;/a&gt;)&lt;/span&gt; was created more recently to study eye movements of both monolingual and bilingual subjects during naturalistic reading of the novel &lt;em&gt;The Mysterious Affair at Styles&lt;/em&gt; by Agatha &lt;span class=&#34;citation&#34;&gt;Christie (&lt;a href=&#34;#ref-christie-2003-mysterious&#34;&gt;2003&lt;/a&gt;)&lt;/span&gt;. In the context of this work, only the monolingual portion collected from 14 native English speakers is used, comprising 56,409 tokens in 5,387 sentences. Eye movements were recorded with an &lt;em&gt;EyeLink 1000&lt;/em&gt; system with 1 kHz binocular sampling (only right eye movements were considered), and the text was presented one paragraph at a time.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The &lt;strong&gt;Zurich Cognitive Language Processing Corpus&lt;/strong&gt; (ZuCo) by &lt;span class=&#34;citation&#34;&gt;Hollenstein et al. (&lt;a href=&#34;#ref-hollenstein-2018-zuco&#34;&gt;2018&lt;/a&gt;)&lt;/span&gt; is a dataset including both eye-tracking and EEG measurements collected simultaneously during both natural and task-oriented reading. The corpus contains 1100 English sentences from the Stanford Sentiment Treebank &lt;span class=&#34;citation&#34;&gt;(Socher et al. &lt;a href=&#34;#ref-socher-etal-2013-recursive&#34;&gt;2013&lt;/a&gt;)&lt;/span&gt; and the Wikipedia dump used in &lt;span class=&#34;citation&#34;&gt;Culotta, McCallum, and Betz (&lt;a href=&#34;#ref-culotta-etal-2006-integrating&#34;&gt;2006&lt;/a&gt;)&lt;/span&gt; with gaze data for 12 adult native speakers. Only the first two portions are used for the present work since they contain natural reading data, totalizing 700 sentences and 13,630 tokens. The text was presented on-screen one sentence at a time, and data were collected with an &lt;em&gt;EyeLink 1000&lt;/em&gt; as for GECO.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;ZuCo 2.0&lt;/strong&gt; is an extension of ZuCo, including 739 sentences extracted from the Wikipedia corpus by &lt;span class=&#34;citation&#34;&gt;Culotta, McCallum, and Betz (&lt;a href=&#34;#ref-culotta-etal-2006-integrating&#34;&gt;2006&lt;/a&gt;)&lt;/span&gt;. Only the 349 sentences for which natural reading data were collected are used, and the 100 duplicates shared with ZuCo to evaluate differences in setup and participants are removed. Data were collected from 18 native English speakers using an &lt;em&gt;EyeLink 1000 Plus&lt;/em&gt; with 500 kHz sampling.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;table class=&#34;table&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;caption&gt;
&lt;span id=&#34;tab:et-corpora&#34;&gt;Table 1.4: &lt;/span&gt;Descriptive statistics of eye-tracking corpora.
&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;font-weight: bold;&#34;&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:center;font-weight: bold;&#34;&gt;
Dundee
&lt;/th&gt;
&lt;th style=&#34;text-align:center;font-weight: bold;&#34;&gt;
GECO
&lt;/th&gt;
&lt;th style=&#34;text-align:center;font-weight: bold;&#34;&gt;
ZuCo
&lt;/th&gt;
&lt;th style=&#34;text-align:center;font-weight: bold;&#34;&gt;
ZuCo 2.0
&lt;/th&gt;
&lt;th style=&#34;text-align:center;font-weight: bold;&#34;&gt;
Total
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
domain(s)
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
news
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
literature
&lt;/td&gt;
&lt;td style=&#34;text-align:center;width: 8em; &#34;&gt;
movie reviews, Wiki articles
&lt;/td&gt;
&lt;td style=&#34;text-align:center;border-right:1px solid;&#34;&gt;
Wiki articles
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
# of sentences
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
2368
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
5387
&lt;/td&gt;
&lt;td style=&#34;text-align:center;width: 8em; &#34;&gt;
700
&lt;/td&gt;
&lt;td style=&#34;text-align:center;border-right:1px solid;&#34;&gt;
349
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
8804
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
mean sent. length
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
21.64
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
10.47
&lt;/td&gt;
&lt;td style=&#34;text-align:center;width: 8em; &#34;&gt;
19.47
&lt;/td&gt;
&lt;td style=&#34;text-align:center;border-right:1px solid;&#34;&gt;
19.51
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
17.77
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
# of tokens
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
51240
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
56409
&lt;/td&gt;
&lt;td style=&#34;text-align:center;width: 8em; &#34;&gt;
13630
&lt;/td&gt;
&lt;td style=&#34;text-align:center;border-right:1px solid;&#34;&gt;
6810
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
128089
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
unique token types
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
9928
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
6155
&lt;/td&gt;
&lt;td style=&#34;text-align:center;width: 8em; &#34;&gt;
4650
&lt;/td&gt;
&lt;td style=&#34;text-align:center;border-right:1px solid;&#34;&gt;
2521
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
16320
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
mean token length
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
4.88
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
4.6
&lt;/td&gt;
&lt;td style=&#34;text-align:center;width: 8em; &#34;&gt;
5.05
&lt;/td&gt;
&lt;td style=&#34;text-align:center;border-right:1px solid;&#34;&gt;
5.01
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
4.89
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
mean fix. duration
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
200
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
210
&lt;/td&gt;
&lt;td style=&#34;text-align:center;width: 8em; &#34;&gt;
117
&lt;/td&gt;
&lt;td style=&#34;text-align:center;border-right:1px solid;&#34;&gt;
117
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
161
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
mean gaze duration
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
280
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
234
&lt;/td&gt;
&lt;td style=&#34;text-align:center;width: 8em; &#34;&gt;
139
&lt;/td&gt;
&lt;td style=&#34;text-align:center;border-right:1px solid;&#34;&gt;
134
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
197
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Tokens are obtained using whitespace tokenization, which is the same approach used to perform gaze annotations across all eye-tracking corpora. Mean sentence length is expressed in number of tokens, and the number of unique types is computed as the size of the vocabulary after removing punctuation from all tokens. Approximately 128,000 tokens annotated with gaze recordings from multiple participants were used in the experiments of Chapters &lt;a href=&#34;chap-ex2.html#chap:ex2&#34;&gt;4&lt;/a&gt; and &lt;a href=&#34;chap-ex3.html#chap:ex3&#34;&gt;5&lt;/a&gt;, while only GECO was used for the analysis of Chapter &lt;a href=&#34;chap-ex1.html#chap:ex1&#34;&gt;3&lt;/a&gt;. Similarly to the PCP task, scores were averaged across subjects to reduce noise and obtain general estimates: in particular, reading times that were missing due to skipping were considered as having the lowest duration across annotators, which is a practice commonly used in literature. Again, considering individual participants’ scores is deemed attractive in a personalization perspective but far beyond this work’s scope.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;subchap:garden-path&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.4&lt;/span&gt; Garden-path Sentences&lt;/h2&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:syntax-trees&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;figures/1_syntax_trees_gp.png&#34; alt=&#34;Syntax trees for the initial and complete parse of garden-path example (1).&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1.2: Syntax trees for the initial and complete parse of garden-path example (1).
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Garden-path sentences&lt;/strong&gt;, named from the expression “leading down the garden path” implying deception, are grammatically correct sentences that create a momentarily ambiguous interpretation in readers. The initial interpretation is later falsified by words encountered during sequential reading, becoming a significant source of processing difficulties. For this reason, garden-path constructions are used to evaluate models of linguistic complexity in the experiments of Chapter &lt;a href=&#34;chap-ex3.html#chap:ex3&#34;&gt;5&lt;/a&gt;. Consider the following recent headline by the newspaper &lt;em&gt;The Guardian&lt;/em&gt;:&lt;a href=&#34;#fn9&#34; class=&#34;footnote-ref&#34; id=&#34;fnref9&#34;&gt;&lt;sup&gt;9&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Vaccine trials halted after patient fell ill &lt;span class=&#34;underline&#34;&gt;restart&lt;/span&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;p&gt;Readers exposed to (1) tend to initially prefer the interpretation in which halted acts as the main verb of the sentence in simple past, i.e., &lt;em&gt;“Vaccine trials halted after patient fell ill.”&lt;/em&gt; is interpreted as a well-formed and semantically meaningful sentence. When the verb &lt;em&gt;restart&lt;/em&gt; is reached, it suddenly becomes evident that the original parse would lead to an ungrammatical sentence, and a reanalysis requiring nontrivial cognitive processing is triggered. In conclusion, one understands that &lt;em&gt;halted&lt;/em&gt; is used as a passive participle, and &lt;em&gt;Vaccine trials&lt;/em&gt; are the subordinate clause’s direct object, as shown in Figure &lt;a href=&#34;chap-ling-comp.html#fig:syntax-trees&#34;&gt;1.2&lt;/a&gt;. We can rephrase the sentence with minimal changes to make it unambiguous:&lt;/p&gt;
&lt;blockquote&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Vaccine trials that were halted after patient fell ill restart.&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;p&gt;The choice for the initial parse can be explained in terms of frequency of occurrence: subject-verb-object sentences are encountered much more frequently than ones containing reduced relatives in everyday settings, making the first parse more likely &lt;span class=&#34;citation&#34;&gt;(Fine et al. &lt;a href=&#34;#ref-fine-2013-rapid&#34;&gt;2013&lt;/a&gt;)&lt;/span&gt;. We refer to the verb causing the reanalysis as &lt;em&gt;disambiguator&lt;/em&gt;, and to the difference in cognitive processing between (1) and (2), measured using proxies such as gaze metrics, as &lt;em&gt;garden-path effect&lt;/em&gt; &lt;span class=&#34;citation&#34;&gt;(Bever &lt;a href=&#34;#ref-bever-1970-cognitive&#34;&gt;1970&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;citation&#34;&gt;Schijndel and Linzen (&lt;a href=&#34;#ref-schjindel-linzen-2020-single&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt; present two families of cognitive processing theories trying to motivate the underlying difficulties in which humans incur with garden-path sentences:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Two-stage accounts&lt;/em&gt; assume that readers consider only one or a subset of possible parses for each sentence that it is reading &lt;span class=&#34;citation&#34;&gt;(Gibson &lt;a href=&#34;#ref-gibson-1991-computational&#34;&gt;1991&lt;/a&gt;; Jurafsky &lt;a href=&#34;#ref-jurafsky-1996-probabilistic&#34;&gt;1996&lt;/a&gt;)&lt;/span&gt;, and processing difficulties arise as a consequence of the reanalysis process need to reconstruct parses that were initially disregarded or not considered &lt;span class=&#34;citation&#34;&gt;(Frazier and Fodor &lt;a href=&#34;#ref-frazier-1978-sausage&#34;&gt;1978&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;One-stage accounts&lt;/em&gt; such as &lt;strong&gt;surprisal theory&lt;/strong&gt; &lt;span class=&#34;citation&#34;&gt;(Hale &lt;a href=&#34;#ref-hale-2001-probabilistic&#34;&gt;2001&lt;/a&gt;; Levy &lt;a href=&#34;#ref-levy-2008-expectation&#34;&gt;2008&lt;/a&gt;)&lt;/span&gt; instead consider difficulties produced by garden paths as the products of a single processing mechanism. Dispreferred parses are not discarded, but rather associated with a lower probability compared to that of likely ones: “processing difficulty on every word in the sentence, including the disambiguating words in garden-path sentences, arises from the extent to which the word shifts the reader’s subjective probability distribution over possible parses” &lt;span class=&#34;citation&#34;&gt;(Schijndel and Linzen &lt;a href=&#34;#ref-schjindel-linzen-2020-single&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There are multiple types of garden-path sentences, usually categorized based on their respective syntactic ambiguities &lt;span class=&#34;citation&#34;&gt;(Frazier &lt;a href=&#34;#ref-frazier-1978-comprehending&#34;&gt;1978&lt;/a&gt;)&lt;/span&gt;. In this work, two classic garden-path families are studied in three different settings using examples taken from &lt;span class=&#34;citation&#34;&gt;Futrell et al. (&lt;a href=&#34;#ref-futrell-etal-2019-neural&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt;. The first type is the &lt;strong&gt;MV/RR ambiguity&lt;/strong&gt; presented in example (1), and repeated in (3a):&lt;/p&gt;
&lt;blockquote&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;The woman brought the sandwich &lt;span class=&#34;underline&#34;&gt;fell&lt;/span&gt; in the dining room. &lt;span class=&#34;verysmall&#34;&gt;[RED., AMBIG.]&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;The woman who was brought the sandwich fell in the dining room. &lt;span class=&#34;verysmall&#34;&gt;[UNRED., AMBIG.]&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;The woman given the sandwich fell in the dining room. &lt;span class=&#34;verysmall&#34;&gt;[RED., UNAMBIG.]&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;The woman who was given the sandwich fell in the dining room. &lt;span class=&#34;verysmall&#34;&gt;[UNRED., UNAMBIG.]&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;p&gt;The label MV/RR indicates that &lt;em&gt;brought&lt;/em&gt; can be initially parsed either as the main verb (MV) in the past tense of the clause or as a passive participle introducing a reduced relative (RR) clause, which postmodifies the subject. It is possible to rewrite the sentence by changing the ambiguous verb to an equivalent one having different forms for simple past and past participle (such as &lt;em&gt;gave&lt;/em&gt; vs. &lt;em&gt;given&lt;/em&gt;). In this case, we expect that the difference in cognitive processing for the disambiguator &lt;em&gt;fell&lt;/em&gt; between the reduced (3c) and the unreduced (3d) version is smaller since the ambiguity is ruled out from the beginning.&lt;/p&gt;
&lt;p&gt;The second type of ambiguity is the &lt;strong&gt;NP/Z ambiguity&lt;/strong&gt; presented in (4a):&lt;/p&gt;
&lt;blockquote&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;As the criminal shot the woman &lt;span class=&#34;underline&#34;&gt;yelled&lt;/span&gt; at the top of her lungs. &lt;span class=&#34;verysmall&#34;&gt;[TRANS., NO COMMA]&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;As the criminal fled the woman yelled at the top of her lungs. &lt;span class=&#34;verysmall&#34;&gt;[INTRANS., NO COMMA]&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;As the criminal shot, the woman yelled at the top of her lungs. &lt;span class=&#34;verysmall&#34;&gt;[TRANS., COMMA]&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;As the criminal fled, the woman yelled at the top of her lungs. &lt;span class=&#34;verysmall&#34;&gt;[INTRANS., COMMA]&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;p&gt;The label NP/Z is used to indicate that the transitive verb &lt;em&gt;shot&lt;/em&gt; can initially be understood to have either have a noun phrase (NP) object like &lt;em&gt;the woman&lt;/em&gt; or a zero (Z), i.e., null object if used intransitively as it is the case for (4a). The sentence can be rewritten by substituting the transitive verb generating the ambiguity with an intransitive one, e.g., replacing &lt;em&gt;shot&lt;/em&gt; with &lt;em&gt;fled&lt;/em&gt; in (4b), by adding a disambiguating comma to force the null-object parse as in (4c), or by doing both as in (4d). We expect that the cognitive processing difference for the disambiguator &lt;em&gt;yelled&lt;/em&gt; between the ambiguous (4a) and the unambiguous (4b) is smaller since the ambiguity is ruled out from the beginning.&lt;/p&gt;
&lt;p&gt;As an additional NP/Z setting evaluation, consider the case in which an overt object is added to the verb introducing the ambiguity:&lt;/p&gt;
&lt;blockquote&gt;
&lt;ol start=&#34;5&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;As the criminal shot the woman &lt;span class=&#34;underline&#34;&gt;yelled&lt;/span&gt; at the top of her lungs. &lt;span class=&#34;verysmall&#34;&gt;[NO OBJ., NO COMMA]&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;As the criminal shot his gun the woman yelled at the top of her lungs. &lt;span class=&#34;verysmall&#34;&gt;[OBJ., NO COMMA]&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;As the criminal shot, the woman yelled at the top of her lungs. &lt;span class=&#34;verysmall&#34;&gt;[NO OBJ., COMMA]&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;As the criminal shot his gun, the woman yelled at the top of her lungs. &lt;span class=&#34;verysmall&#34;&gt;[OBJ., COMMA]&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;p&gt;Again, we expect that the difference in cognitive processing for &lt;em&gt;yelled&lt;/em&gt; is higher in the non-object pair (5a)-(5c), where the first item is a garden-path sentence, rather than in the pair (5b)-(5d) where both sentences are unambiguous.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;custompar&#34;&gt;Gaze metrics and Garden-path Sentences&lt;/span&gt; As can be intuitively assumed, garden-path effects are reflected in gaze metrics collected during natural reading. Multiple studies have focused on quantifying the difference between garden-path sentences and their unambiguous counterparts on reading times in human subjects. &lt;span class=&#34;citation&#34;&gt;Sturt, Pickering, and Crocker (&lt;a href=&#34;#ref-sturt-etal-1999-structural&#34;&gt;1999&lt;/a&gt;)&lt;/span&gt; found a massive delay of 152ms for each word in the disambiguating region of NP/Z sentences. &lt;span class=&#34;citation&#34;&gt;Grodner et al. (&lt;a href=&#34;#ref-grodner-etal-2003-against&#34;&gt;2003&lt;/a&gt;)&lt;/span&gt; estimate an average delay of 64ms over the disambiguating region for NP/Z constructs using 53 college students’ reading times over a set of 20 ambiguous sentences. More recently, &lt;span class=&#34;citation&#34;&gt;Prasad and Linzen (&lt;a href=&#34;#ref-prasad-linzen-2019-much&#34;&gt;2019&lt;/a&gt;&lt;a href=&#34;#ref-prasad-linzen-2019-much&#34;&gt;b&lt;/a&gt;)&lt;/span&gt; recorded eye measurements for 224 participants recruited through Amazon Mechanical Turk on the same set of NP/Z sentences as &lt;span class=&#34;citation&#34;&gt;Grodner et al. (&lt;a href=&#34;#ref-grodner-etal-2003-against&#34;&gt;2003&lt;/a&gt;)&lt;/span&gt;, finding a much lower average delay of 28ms, and suggesting an overestimation in previous studies due to small sample size and publication contingency to significant results. &lt;span class=&#34;citation&#34;&gt;Prasad and Linzen (&lt;a href=&#34;#ref-prasad-linzen-2019-self&#34;&gt;2019&lt;/a&gt;&lt;a href=&#34;#ref-prasad-linzen-2019-self&#34;&gt;a&lt;/a&gt;)&lt;/span&gt; collected self-paced reading times from 73 participants recruited on the Prolific Academic crowdsourcing platform and measured an average delay of 22ms over the disambiguating region for MV/RR constructs.&lt;/p&gt;
&lt;p&gt;Given the high variability in results across studies, it can be hypothesized that the way in which stimuli were presented to subjects plays a significant role in determining the magnitude of garden-path effects &lt;span class=&#34;citation&#34;&gt;(Van Schijndel and Linzen &lt;a href=&#34;#ref-schjindel-linzen-2018-modeling&#34;&gt;2018&lt;/a&gt;)&lt;/span&gt;. For example, a sentence presented word-by-word to subjects may yield more ecologically valid reading times estimates than a sentence presented region-by-region. Another problematic factor involves constraining the impact of garden-path effects to the disambiguating region: first, because &lt;em&gt;parafoveal preview effects may slightly anticipate the start of the effect&lt;/em&gt; &lt;span class=&#34;citation&#34;&gt;(Schotter, Angele, and Rayner &lt;a href=&#34;#ref-schotter-2012-parafoveal&#34;&gt;2012&lt;/a&gt;; Schotter &lt;a href=&#34;#ref-schotter-2018-reading&#34;&gt;2018&lt;/a&gt;)&lt;/span&gt;; and second, because due to &lt;em&gt;spillover&lt;/em&gt; &lt;span class=&#34;citation&#34;&gt;(Mitchell &lt;a href=&#34;#ref-mitchell-1984-evaluation&#34;&gt;1984&lt;/a&gt;)&lt;/span&gt;, a phenomenon in which the surprisal of a word influences the reading times for itself and at least three subsequent words &lt;span class=&#34;citation&#34;&gt;(Smith and Levy &lt;a href=&#34;#ref-smith-levy-2013-effect&#34;&gt;2013&lt;/a&gt;)&lt;/span&gt;, reading times of the disambiguating region are influenced by preceding words, and influence subsequent ones, spreading the garden-path effect on a much broader context. For this reason, eye-tracking metrics are studied for all sentence regions in the experiments of Chapter &lt;a href=&#34;chap-ex3.html#chap:ex3&#34;&gt;5&lt;/a&gt;.&lt;/p&gt;



&lt;!-- Needed for leaving space to the quote, * is for no indentation after title --&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;h3&gt;References&lt;/h3&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-ambati-etal-2016-assessing&#34;&gt;
&lt;p&gt;Ambati, Bharat Ram, Siva Reddy, and Mark Steedman. 2016. “Assessing Relative Sentence Complexity Using an Incremental CCG Parser.” In &lt;em&gt;Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies&lt;/em&gt;, 1051–7. San Diego, California: Association for Computational Linguistics. &lt;a href=&#34;https://doi.org/10.18653/v1/N16-1120&#34;&gt;https://doi.org/10.18653/v1/N16-1120&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-barrett-etal-2016-weakly&#34;&gt;
&lt;p&gt;Barrett, Maria, Joachim Bingel, Frank Keller, and Anders Søgaard. 2016. “Weakly Supervised Part-of-Speech Tagging Using Eye-Tracking Data.” In &lt;em&gt;Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)&lt;/em&gt;, 579–84. Berlin, Germany: Association for Computational Linguistics. &lt;a href=&#34;https://doi.org/10.18653/v1/P16-2094&#34;&gt;https://doi.org/10.18653/v1/P16-2094&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-berruto-2011-linguistica&#34;&gt;
&lt;p&gt;Berruto, Gaetano, and Massimo Simone Cerruti. 2011. &lt;em&gt;La Linguistica. Un Corso Introduttivo&lt;/em&gt;. De Agostini.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-berzak-etal-2018-assessing&#34;&gt;
&lt;p&gt;Berzak, Yevgeni, Boris Katz, and Roger Levy. 2018. “Assessing Language Proficiency from Eye Movements in Reading.” In &lt;em&gt;Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)&lt;/em&gt;, 1986–96. New Orleans, Louisiana: Association for Computational Linguistics. &lt;a href=&#34;https://doi.org/10.18653/v1/N18-1180&#34;&gt;https://doi.org/10.18653/v1/N18-1180&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-bever-1970-cognitive&#34;&gt;
&lt;p&gt;Bever, Thomas G. 1970. “The Cognitive Basis for Linguistic Structures.” &lt;em&gt;Cognition and the Development of Language&lt;/em&gt;. Wiley.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-brunato-etal-2020-profiling&#34;&gt;
&lt;p&gt;Brunato, Dominique, Andrea Cimino, Felice Dell’Orletta, Giulia Venturi, and Simonetta Montemagni. 2020. “Profiling-UD: A Tool for Linguistic Profiling of Texts.” In &lt;em&gt;Proceedings of the 12th Language Resources and Evaluation Conference&lt;/em&gt;, 7145–51. Marseille, France: European Language Resources Association. &lt;a href=&#34;https://www.aclweb.org/anthology/2020.lrec-1.883&#34;&gt;https://www.aclweb.org/anthology/2020.lrec-1.883&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-brunato-etal-2018-sentence&#34;&gt;
&lt;p&gt;Brunato, Dominique, Lorenzo De Mattei, Felice Dell’Orletta, Benedetta Iavarone, and Giulia Venturi. 2018. “Is This Sentence Difficult? Do You Agree?” In &lt;em&gt;Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing&lt;/em&gt;, 2690–9. Brussels, Belgium: Association for Computational Linguistics. &lt;a href=&#34;https://doi.org/10.18653/v1/D18-1289&#34;&gt;https://doi.org/10.18653/v1/D18-1289&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-cangelosi-turner-2002-emergere&#34;&gt;
&lt;p&gt;Cangelosi, Angelo, and Huck Turner. 2002. “L’emergere Del Linguaggio.” &lt;em&gt;Scienze Della Mente&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-carr-etal-2020-algorithms&#34;&gt;
&lt;p&gt;Carr, Jon W, Valentina N Pescuma, Michele Furlan, Maria Ktori, and Davide Crepaldi. 2020. “Algorithms for the Automated Correction of Vertical Drift in Eye Tracking Data.” &lt;em&gt;OSF Preprints&lt;/em&gt;, June. &lt;a href=&#34;osf.io/jg3nc&#34;&gt;osf.io/jg3nc&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-caruana-1997-multitask&#34;&gt;
&lt;p&gt;Caruana, Rich. 1997. “Multitask Learning.” &lt;em&gt;Machine Learning&lt;/em&gt; 28: 41–75. &lt;a href=&#34;https://www.cs.utexas.edu/~kuipers/readings/Caruana-mlj-97.pdf&#34;&gt;https://www.cs.utexas.edu/~kuipers/readings/Caruana-mlj-97.pdf&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-christie-2003-mysterious&#34;&gt;
&lt;p&gt;Christie, Agatha. 2003. &lt;em&gt;The Mysterious Affair at Styles: A Detective Story&lt;/em&gt;. Modern Library.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-cop-etal-2017-presenting&#34;&gt;
&lt;p&gt;Cop, Uschi, Nicolas Dirix, Denis Drieghe, and Wouter Duyck. 2017. “Presenting Geco: An Eyetracking Corpus of Monolingual and Bilingual Sentence Reading.” &lt;em&gt;Behavior Research Methods&lt;/em&gt; 49 (2). Springer: 602–15.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-culotta-etal-2006-integrating&#34;&gt;
&lt;p&gt;Culotta, Aron, Andrew McCallum, and Jonathan Betz. 2006. “Integrating Probabilistic Extraction Models and Data Mining to Discover Relations and Patterns in Text.” In &lt;em&gt;Proceedings of the Human Language Technology Conference of the NAACL, Main Conference&lt;/em&gt;, 296–303. New York City, USA: Association for Computational Linguistics. &lt;a href=&#34;https://www.aclweb.org/anthology/N06-1038&#34;&gt;https://www.aclweb.org/anthology/N06-1038&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-day-2004-religion&#34;&gt;
&lt;p&gt;Day, Matthew. 2004. “Religion, Off-Line Cognition and the Extended Mind.” &lt;em&gt;Journal of Cognition and Culture&lt;/em&gt; 4 (1). Brill: 101–21.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-demberg-keller-2008-data&#34;&gt;
&lt;p&gt;Demberg, Vera, and Frank Keller. 2008. “Data from Eye-Tracking Corpora as Evidence for Theories of Syntactic Processing Complexity.” &lt;em&gt;Cognition&lt;/em&gt; 109 (2). Elsevier: 193–210.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-edmonds-1999-syntactic&#34;&gt;
&lt;p&gt;Edmonds, Bruce M. 1999. “Syntactic Measures of Complexity.” PhD thesis, University of Manchester Manchester, UK.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-fine-2013-rapid&#34;&gt;
&lt;p&gt;Fine, Alex B, T Florian Jaeger, Thomas A Farmer, and Ting Qian. 2013. “Rapid Expectation Adaptation During Syntactic Comprehension.” &lt;em&gt;PloS One&lt;/em&gt; 8 (10). Public Library of Science: e77661.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-frazier-1978-comprehending&#34;&gt;
&lt;p&gt;Frazier, Lyn. 1978. “On Comprehending Sentences: Syntactic Parsing Strategies.” PhD thesis, University of Connecticut.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-frazier-1978-sausage&#34;&gt;
&lt;p&gt;Frazier, Lyn, and Janet Dean Fodor. 1978. “The Sausage Machine: A New Two-Stage Parsing Model.” &lt;em&gt;Cognition&lt;/em&gt; 6 (4). Elsevier: 291–325.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-futrell-2020-lossy&#34;&gt;
&lt;p&gt;Futrell, Richard, Edward Gibson, and Roger P Levy. 2020. “Lossy-Context Surprisal: An Information-Theoretic Model of Memory Effects in Sentence Processing.” &lt;em&gt;Cognitive Science&lt;/em&gt; 44 (3). Wiley Online Library: e12814.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-futrell-etal-2019-neural&#34;&gt;
&lt;p&gt;Futrell, Richard, Ethan Wilcox, Takashi Morita, Peng Qian, Miguel Ballesteros, and Roger Levy. 2019. “Neural Language Models as Psycholinguistic Subjects: Representations of Syntactic State.” In &lt;em&gt;Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)&lt;/em&gt;, 32–42. Minneapolis, Minnesota: Association for Computational Linguistics. &lt;a href=&#34;https://doi.org/10.18653/v1/N19-1004&#34;&gt;https://doi.org/10.18653/v1/N19-1004&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-gibson-1991-computational&#34;&gt;
&lt;p&gt;Gibson, Edward. 1991. “A Computational Theory of Human Linguistic Processing: Memory Limitations and Processing Breakdown.” PhD thesis, Pittsburgh, PA: Carnegie Mellon University.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-gibson-1998-linguistic&#34;&gt;
&lt;p&gt;Gibson, Edward. 1998. “Linguistic Complexity: Locality of Syntactic Dependencies.” &lt;em&gt;Cognition&lt;/em&gt; 68 (1). Elsevier: 1–76.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-gibson-2000-dependency&#34;&gt;
&lt;p&gt;Gibson, Edward. 2000. “The Dependency Locality Theory: A Distance-Based Theory of Linguistic Complexity.” &lt;em&gt;Image, Language, Brain&lt;/em&gt; 2000: 95–126.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-gonzalez-garduno-sogaard-2018-learning&#34;&gt;
&lt;p&gt;González-Garduño, Ana Valeria, and Anders Søgaard. 2018. “Learning to Predict Readability Using Eye-Movement Data from Natives and Learners.” AAAI Conference on Artificial Intelligence.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-goodman-2001-bit&#34;&gt;
&lt;p&gt;Goodman, Joshua. 2001. “A Bit of Progress in Language Modeling.” &lt;em&gt;arXiv Preprint Cs/0108005&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-grodner-etal-2003-against&#34;&gt;
&lt;p&gt;Grodner, Daniel, Edward Gibson, Vered Argaman, and Maria Babyonyshev. 2003. “Against Repair-Based Reanalysis in Sentence Comprehension.” &lt;em&gt;Journal of Psycholinguistic Research&lt;/em&gt; 32 (2). Springer: 141–66.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-hale-2001-probabilistic&#34;&gt;
&lt;p&gt;Hale, John. 2001. “A Probabilistic Earley Parser as a Psycholinguistic Model.” In &lt;em&gt;Second Meeting of the North American Chapter of the Association for Computational Linguistics&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-hale-2016-information&#34;&gt;
&lt;p&gt;Hale, John. 2016. “Information-Theoretical Complexity Metrics.” &lt;em&gt;Language and Linguistics Compass&lt;/em&gt; 10 (9). Wiley Online Library: 397–412.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-hauser-2002-faculty&#34;&gt;
&lt;p&gt;Hauser, Marc D, Noam Chomsky, and W Tecumseh Fitch. 2002. “The Faculty of Language: What Is It, Who Has It, and How Did It Evolve?” &lt;em&gt;Science&lt;/em&gt; 298 (5598). American Association for the Advancement of Science: 1569–79.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-hollenstein-2018-zuco&#34;&gt;
&lt;p&gt;Hollenstein, Nora, Jonathan Rotsztejn, Marius Troendle, Andreas Pedroni, Ce Zhang, and Nicolas Langer. 2018. “ZuCo, a Simultaneous Eeg and Eye-Tracking Resource for Natural Sentence Reading.” &lt;em&gt;Scientific Data&lt;/em&gt; 5 (1). Nature Publishing Group: 1–13.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-hollenstein-etal-2020-zuco&#34;&gt;
&lt;p&gt;Hollenstein, Nora, Marius Troendle, Ce Zhang, and Nicolas Langer. 2020. “ZuCo 2.0: A Dataset of Physiological Recordings During Natural Reading and Annotation.” In &lt;em&gt;Proceedings of the 12th Language Resources and Evaluation Conference&lt;/em&gt;, 138–46. Marseille, France: European Language Resources Association. &lt;a href=&#34;https://www.aclweb.org/anthology/2020.lrec-1.18&#34;&gt;https://www.aclweb.org/anthology/2020.lrec-1.18&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-hollenstein-zhang-2019-entity&#34;&gt;
&lt;p&gt;Hollenstein, Nora, and Ce Zhang. 2019. “Entity Recognition at First Sight: Improving NER with Eye Movement Information.” In &lt;em&gt;Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)&lt;/em&gt;, 1–10. Minneapolis, Minnesota: Association for Computational Linguistics. &lt;a href=&#34;https://doi.org/10.18653/v1/N19-1001&#34;&gt;https://doi.org/10.18653/v1/N19-1001&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-iverson-thelen-1999-hand&#34;&gt;
&lt;p&gt;Iverson, Jana M, and Esther Thelen. 1999. “Hand, Mouth and Brain. The Dynamic Emergence of Speech and Gesture.” &lt;em&gt;Journal of Consciousness Studies&lt;/em&gt; 6 (11-12). Imprint Academic: 19–40.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-jurafsky-1996-probabilistic&#34;&gt;
&lt;p&gt;Jurafsky, Daniel. 1996. “A Probabilistic Model of Lexical and Syntactic Access and Disambiguation.” &lt;em&gt;Cognitive Science&lt;/em&gt; 20 (2). Wiley Online Library: 137–94.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-kennedy-etal-2003-dundee&#34;&gt;
&lt;p&gt;Kennedy, Alan, Robin Hill, and Joël Pynte. 2003. “The Dundee Corpus.” In &lt;em&gt;Proceedings of the 12th European Conference on Eye Movement&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-kusters-2003-linguistic&#34;&gt;
&lt;p&gt;Kusters, Wouter. 2003. “Linguistic Complexity.” PhD thesis, Netherlands Graduate School of Linguistics.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-kusters-2008-complexity&#34;&gt;
&lt;p&gt;Kusters, Wouter. 2008. “Complexity in Linguistic Theory, Language Learning and Language Change.” In &lt;em&gt;Language Complexity: Typology, Contact, Change&lt;/em&gt;, 3–22. John Benjamins Amsterdam, The Netherlands.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-levy-2008-expectation&#34;&gt;
&lt;p&gt;Levy, Roger. 2008. “Expectation-Based Syntactic Comprehension.” &lt;em&gt;Cognition&lt;/em&gt; 106 (3). Elsevier: 1126–77.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-mcdonald-etal-2013-universal&#34;&gt;
&lt;p&gt;McDonald, Ryan, Joakim Nivre, Yvonne Quirmbach-Brundage, Yoav Goldberg, Dipanjan Das, Kuzman Ganchev, Keith Hall, et al. 2013. “Universal Dependency Annotation for Multilingual Parsing.” In &lt;em&gt;Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)&lt;/em&gt;, 92–97. Sofia, Bulgaria: Association for Computational Linguistics. &lt;a href=&#34;https://www.aclweb.org/anthology/P13-2017&#34;&gt;https://www.aclweb.org/anthology/P13-2017&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-mcwhorter-2001-world&#34;&gt;
&lt;p&gt;McWhorter, John H. 2001. “The Worlds Simplest Grammars Are Creole Grammars.” &lt;em&gt;Linguistic Typology&lt;/em&gt; 5 (2-3). De Gruyter Mouton: 125–66.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-meyer-rice-1992-prose&#34;&gt;
&lt;p&gt;Meyer, Bonnie JF, and G Elizabeth Rice. 1992. “12 Prose Processing in Adulthood: The Text, the Reader, and the Task.” &lt;em&gt;Everyday Cognition in Adulthood and Late Life&lt;/em&gt;. Cambridge Univ Pr, 157.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-miestamo-2004-feasibility&#34;&gt;
&lt;p&gt;Miestamo, Matti. 2004. “On the Feasibility of Complexity Metrics.” In &lt;em&gt;FinEst Linguistics, Proceedings of the Annual Finnish and Estonian Conference of Linguistics&lt;/em&gt;, 11–26. Tallin, Finland.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-miestamo-2008-grammatical&#34;&gt;
&lt;p&gt;Miestamo, Matti. 2008. “Grammatical Complexity in a Cross-Linguistic Perspective.” In &lt;em&gt;Language Complexity: Typology, Contact, Change&lt;/em&gt;, 41. John Benjamins Amsterdam, The Netherlands.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-mishra-etal-2017-learning&#34;&gt;
&lt;p&gt;Mishra, Abhijit, Kuntal Dey, and Pushpak Bhattacharyya. 2017. “Learning Cognitive Features from Gaze Data for Sentiment and Sarcasm Classification Using Convolutional Neural Network.” In &lt;em&gt;Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)&lt;/em&gt;, 377–87. Vancouver, Canada: Association for Computational Linguistics. &lt;a href=&#34;https://doi.org/10.18653/v1/P17-1035&#34;&gt;https://doi.org/10.18653/v1/P17-1035&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-mitchell-1984-evaluation&#34;&gt;
&lt;p&gt;Mitchell, Don C. 1984. “An Evaluation of Subject-Paced Reading Tasks and Other Methods for Investigating Immediate Processes in Reading.” &lt;em&gt;New Methods in Reading Comprehension Research&lt;/em&gt;, 69–89.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-munro-etal-2010-crowdsourcing&#34;&gt;
&lt;p&gt;Munro, Robert, Steven Bethard, Victor Kuperman, Vicky Tzuyin Lai, Robin Melnick, Christopher Potts, Tyler Schnoebelen, and Harry Tily. 2010. “Crowdsourcing and Language Studies: The New Generation of Linguistic Data.” In &lt;em&gt;Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk&lt;/em&gt;, 122–30. Los Angeles: Association for Computational Linguistics. &lt;a href=&#34;https://www.aclweb.org/anthology/W10-0719&#34;&gt;https://www.aclweb.org/anthology/W10-0719&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-nivre-etal-2016-universal&#34;&gt;
&lt;p&gt;Nivre, Joakim, Marie-Catherine de Marneffe, Filip Ginter, Yoav Goldberg, Jan Hajič, Christopher D. Manning, Ryan McDonald, et al. 2016. “Universal Dependencies V1: A Multilingual Treebank Collection.” In &lt;em&gt;Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC’16)&lt;/em&gt;, 1659–66. Portorož, Slovenia: European Language Resources Association (ELRA). &lt;a href=&#34;https://www.aclweb.org/anthology/L16-1262&#34;&gt;https://www.aclweb.org/anthology/L16-1262&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-prasad-linzen-2019-self&#34;&gt;
&lt;p&gt;Prasad, Grusha, and Tal Linzen. 2019a. “Do Self-Paced Reading Studies Provide Evidence for Rapid Syntactic Adaptation?” &lt;em&gt;PsyArXiv Pre-Print&lt;/em&gt;. &lt;a href=&#34;https://tallinzen.net/media/papers/prasad_linzen_2019_adaptation.pdf&#34;&gt;https://tallinzen.net/media/papers/prasad_linzen_2019_adaptation.pdf&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-prasad-linzen-2019-much&#34;&gt;
&lt;p&gt;Prasad, Grusha, and Tal Linzen. 2019b. “How Much Harder Are Hard Garden-Path Sentences Than Easy Ones?” &lt;em&gt;OSF Preprint&lt;/em&gt; syh3j. &lt;a href=&#34;https://osf.io/syh3j/&#34;&gt;https://osf.io/syh3j/&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-ruder-2017-overview&#34;&gt;
&lt;p&gt;Ruder, Sebastian. 2017. “An Overview of Multi-Task Learning in Deep Neural Networks.” &lt;em&gt;ArXiv Pre-Print&lt;/em&gt; 1706.05098. &lt;a href=&#34;https://arxiv.org/abs/1706.05098&#34;&gt;https://arxiv.org/abs/1706.05098&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-schjindel-linzen-2020-single&#34;&gt;
&lt;p&gt;Schijndel, Marten van, and Tal Linzen. 2020. “Single-Stage Prediction Models Do Not Explain the Magnitude of Syntactic Disambiguation Difficulty.” &lt;em&gt;PsyArXiv Pre-Print&lt;/em&gt; sgbqy. &lt;a href=&#34;https://psyarxiv.com/sgbqy/&#34;&gt;https://psyarxiv.com/sgbqy/&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-schotter-2018-reading&#34;&gt;
&lt;p&gt;Schotter, Elizabeth R. 2018. “Reading Ahead by Hedging Our Bets on Seeing the Future: Eye Tracking and Electrophysiology Evidence for Parafoveal Lexical Processing and Saccadic Control by Partial Word Recognition.” In &lt;em&gt;Psychology of Learning and Motivation&lt;/em&gt;, 68:263–98. Elsevier.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-schotter-2020-eyetracking&#34;&gt;
&lt;p&gt;Schotter, Elizabeth R. 2020. &lt;em&gt;Eye Tracking for Cognitive Science&lt;/em&gt;. SISSA Course.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-schotter-2012-parafoveal&#34;&gt;
&lt;p&gt;Schotter, Elizabeth R, Bernhard Angele, and Keith Rayner. 2012. “Parafoveal Processing in Reading.” &lt;em&gt;Attention, Perception, &amp;amp; Psychophysics&lt;/em&gt; 74 (1). Springer: 5–35.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-simi-etal-2014-less&#34;&gt;
&lt;p&gt;Simi, Maria, Cristina Bosco, and Simonetta Montemagni. 2014. “Less Is More? Towards a Reduced Inventory of Categories for Training a Parser for the Italian Stanford Dependencies.” In &lt;em&gt;Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC’14)&lt;/em&gt;, 83–90. Reykjavik, Iceland: European Language Resources Association (ELRA). &lt;a href=&#34;http://www.lrec-conf.org/proceedings/lrec2014/pdf/818_Paper.pdf&#34;&gt;http://www.lrec-conf.org/proceedings/lrec2014/pdf/818_Paper.pdf&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-singh-etal-2016-quantifying&#34;&gt;
&lt;p&gt;Singh, Abhinav Deep, Poojan Mehta, Samar Husain, and Rajkumar Rajakrishnan. 2016. “Quantifying Sentence Complexity Based on Eye-Tracking Measures.” In &lt;em&gt;Proceedings of the Workshop on Computational Linguistics for Linguistic Complexity (CL4LC)&lt;/em&gt;, 202–12. Osaka, Japan: The COLING 2016 Organizing Committee. &lt;a href=&#34;https://www.aclweb.org/anthology/W16-4123&#34;&gt;https://www.aclweb.org/anthology/W16-4123&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-sinnemaki-2011-language&#34;&gt;
&lt;p&gt;Sinnemäki, Kaius. 2011. “Language Universals and Linguistic Complexity: Three Case Studies in Core Argument Marking.” PhD thesis, University of Helsinki.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-smith-levy-2013-effect&#34;&gt;
&lt;p&gt;Smith, Nathaniel J, and Roger Levy. 2013. “The Effect of Word Predictability on Reading Time Is Logarithmic.” &lt;em&gt;Cognition&lt;/em&gt; 128 (3). Elsevier: 302–19.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-socher-etal-2013-recursive&#34;&gt;
&lt;p&gt;Socher, Richard, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. 2013. “Recursive Deep Models for Semantic Compositionality over a Sentiment Treebank.” In &lt;em&gt;Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing&lt;/em&gt;, 1631–42. Seattle, Washington, USA: Association for Computational Linguistics. &lt;a href=&#34;https://www.aclweb.org/anthology/D13-1170&#34;&gt;https://www.aclweb.org/anthology/D13-1170&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-straka-etal-2016-udpipe&#34;&gt;
&lt;p&gt;Straka, Milan, Jan Hajič, and Jana Straková. 2016. “UDPipe: Trainable Pipeline for Processing CoNLL-U Files Performing Tokenization, Morphological Analysis, POS Tagging and Parsing.” In &lt;em&gt;Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC’16)&lt;/em&gt;, 4290–7. Portorož, Slovenia: European Language Resources Association (ELRA). &lt;a href=&#34;https://www.aclweb.org/anthology/L16-1680&#34;&gt;https://www.aclweb.org/anthology/L16-1680&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-strzyz-etal-2019-towards&#34;&gt;
&lt;p&gt;Strzyz, Michalina, David Vilares, and Carlos Gómez-Rodríguez. 2019. “Towards Making a Dependency Parser See.” In &lt;em&gt;Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (Emnlp-Ijcnlp)&lt;/em&gt;, 1500–1506. Hong Kong, China: Association for Computational Linguistics. &lt;a href=&#34;https://doi.org/10.18653/v1/D19-1160&#34;&gt;https://doi.org/10.18653/v1/D19-1160&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-sturt-etal-1999-structural&#34;&gt;
&lt;p&gt;Sturt, Patrick, Martin J Pickering, and Matthew W Crocker. 1999. “Structural Change and Reanalysis Difficulty in Language Comprehension.” &lt;em&gt;Journal of Memory and Language&lt;/em&gt; 40 (1). Elsevier: 136–50.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-taylor-1953-cloze&#34;&gt;
&lt;p&gt;Taylor, Wilson L. 1953. “‘Cloze Procedure’: A New Tool for Measuring Readability.” &lt;em&gt;Journalism Quarterly&lt;/em&gt; 30 (4). SAGE Publications Sage CA: Los Angeles, CA: 415–33.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-vajjala-lucic-2019-understanding&#34;&gt;
&lt;p&gt;Vajjala, Sowmya, and Ivana Lucic. 2019. “On Understanding the Relation Between Expert Annotations of Text Readability and Target Reader Comprehension.” In &lt;em&gt;Proceedings of the Fourteenth Workshop on Innovative Use of Nlp for Building Educational Applications&lt;/em&gt;, 349–59. Florence, Italy: Association for Computational Linguistics. &lt;a href=&#34;https://doi.org/10.18653/v1/W19-4437&#34;&gt;https://doi.org/10.18653/v1/W19-4437&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-vajjala-lucic-2018-onestopenglish&#34;&gt;
&lt;p&gt;Vajjala, Sowmya, and Ivana Lučić. 2018. “OneStopEnglish Corpus: A New Corpus for Automatic Readability Assessment and Text Simplification.” In &lt;em&gt;Proceedings of the Thirteenth Workshop on Innovative Use of NLP for Building Educational Applications&lt;/em&gt;, 297–304. New Orleans, Louisiana: Association for Computational Linguistics. &lt;a href=&#34;https://doi.org/10.18653/v1/W18-0535&#34;&gt;https://doi.org/10.18653/v1/W18-0535&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-schjindel-linzen-2018-modeling&#34;&gt;
&lt;p&gt;Van Schijndel, Marten, and Tal Linzen. 2018. “Modeling Garden Path Effects Without Explicit Hierarchical Syntax.” In &lt;em&gt;Proceedings of the 40th Annual Conference of the Cognitive Science Society&lt;/em&gt;, 2600–2605.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-vasishth-etal-2013-what&#34;&gt;
&lt;p&gt;Vasishth, Shravan, Titus von der Malsburg, and Felix Engelmann. 2013. “What Eye Movements Can Tell Us About Sentence Comprehension.” &lt;em&gt;Cognitive Science&lt;/em&gt; 4 2. Wiley interdisciplinary reviews: 125–34. &lt;a href=&#34;https://onlinelibrary.wiley.com/doi/full/10.1002/wcs.1209&#34;&gt;https://onlinelibrary.wiley.com/doi/full/10.1002/wcs.1209&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-voghera-2001-riflessioni&#34;&gt;
&lt;p&gt;Voghera, Miriam. 2001. “Riflessioni Su Semplificazione, Complessità E Modalità Di Trasmissione: Sintassi E Semantica.” &lt;em&gt;Scritto E Parlato. Metodi, Testi E Contesti&lt;/em&gt;. Aracne, 65–78.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-xu-etal-2015-problems&#34;&gt;
&lt;p&gt;Xu, Wei, Chris Callison-Burch, and Courtney Napoles. 2015. “Problems in Current Text Simplification Research: New Data Can Help.” &lt;em&gt;Transactions of the Association for Computational Linguistics&lt;/em&gt; 3: 283–97. &lt;a href=&#34;https://doi.org/10.1162/tacl_a_00139&#34;&gt;https://doi.org/10.1162/tacl_a_00139&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;Available at &lt;a href=&#34;http://linguistic-profiling.italianlp.it&#34; class=&#34;uri&#34;&gt;http://linguistic-profiling.italianlp.it&lt;/a&gt;&lt;a href=&#34;chap-ling-comp.html#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;This motivates the previous claim about the interdependence of intrinsic and extrinsic approaches. See Section 2.1 of &lt;span class=&#34;citation&#34;&gt;Martinc, Pollak, and Robnik-Sikonja (&lt;a href=&#34;#ref-martinc-2019-supervised&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt; for an overview of the most popular metrics for English.&lt;a href=&#34;chap-ling-comp.html#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn4&#34;&gt;&lt;p&gt;This is an admittedly simplistic reduction, given the importance of parafoveal processing in reading &lt;span class=&#34;citation&#34;&gt;(Schotter, Angele, and Rayner &lt;a href=&#34;#ref-schotter-2012-parafoveal&#34;&gt;2012&lt;/a&gt;; Schotter &lt;a href=&#34;#ref-schotter-2018-reading&#34;&gt;2018&lt;/a&gt;)&lt;/span&gt;&lt;a href=&#34;chap-ling-comp.html#fnref4&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn5&#34;&gt;&lt;p&gt;See &lt;span class=&#34;citation&#34;&gt;Collins-Thompson (&lt;a href=&#34;#ref-collins-2014-computational&#34;&gt;2014&lt;/a&gt;)&lt;/span&gt; for a thorough review of ARA approaches.&lt;a href=&#34;chap-ling-comp.html#fnref5&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn6&#34;&gt;&lt;p&gt;See &lt;span class=&#34;citation&#34;&gt;Rayner (&lt;a href=&#34;#ref-rayner-1998-eye&#34;&gt;1998&lt;/a&gt;)&lt;/span&gt; for a comprehensive survey on findings related to eye-tracking research.&lt;a href=&#34;chap-ling-comp.html#fnref6&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn7&#34;&gt;&lt;p&gt;See &lt;span class=&#34;citation&#34;&gt;Hollenstein, Barrett, and Beinborn (&lt;a href=&#34;#ref-hollenstein-etal-2020-towards&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt; for an exhaustive overview of current approaches and best practices.&lt;a href=&#34;chap-ling-comp.html#fnref7&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn8&#34;&gt;&lt;p&gt;Appendix &lt;a href=&#34;app-et-metrics.html#app:et-metrics&#34;&gt;B&lt;/a&gt; contains information about deriving metric values for all corpora.&lt;a href=&#34;chap-ling-comp.html#fnref8&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn9&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://twitter.com/drswissmiss/status/1304856856649756673&#34; class=&#34;uri&#34;&gt;https://twitter.com/drswissmiss/status/1304856856649756673&lt;/a&gt;&lt;a href=&#34;chap-ling-comp.html#fnref9&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
            &lt;/section&gt;

          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
&lt;a href=&#34;introduction.html&#34; class=&#34;navigation navigation-prev &#34; aria-label=&#34;Previous page&#34;&gt;&lt;i class=&#34;fa fa-angle-left&#34;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&#34;chap-models.html&#34; class=&#34;navigation navigation-next &#34; aria-label=&#34;Next page&#34;&gt;&lt;i class=&#34;fa fa-angle-right&#34;&gt;&lt;/i&gt;&lt;/a&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/app.min.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/lunr.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/clipboard.min.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/plugin-search.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/plugin-sharing.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/plugin-fontsettings.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/plugin-bookdown.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/jquery.highlight.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/plugin-clipboard.js&#34;&gt;&lt;/script&gt;
&lt;script&gt;
gitbook.require([&#34;gitbook&#34;], function(gitbook) {
gitbook.start({
&#34;sharing&#34;: {
&#34;github&#34;: true,
&#34;facebook&#34;: true,
&#34;twitter&#34;: true,
&#34;linkedin&#34;: true,
&#34;weibo&#34;: false,
&#34;instapaper&#34;: false,
&#34;vk&#34;: false,
&#34;all&#34;: false
},
&#34;fontsettings&#34;: {
&#34;theme&#34;: &#34;white&#34;,
&#34;family&#34;: &#34;sans&#34;,
&#34;size&#34;: 2
},
&#34;edit&#34;: {
&#34;link&#34;: &#34;https://github.com/gsarti/master-thesis/tree/master/01-Linguistic-Complexity.Rmd&#34;,
&#34;text&#34;: &#34;Edit&#34;
},
&#34;history&#34;: {
&#34;link&#34;: null,
&#34;text&#34;: null
},
&#34;view&#34;: {
&#34;link&#34;: null,
&#34;text&#34;: null
},
&#34;download&#34;: [[&#34;Sarti_2020_Interpreting_NLMs_for_LCA.pdf&#34;, &#34;PDF&#34;]],
&#34;toc&#34;: {
&#34;collapse&#34;: &#34;subsection&#34;,
&#34;scroll_highlight&#34;: true
},
&#34;info&#34;: false
});
});
&lt;/script&gt;

&lt;!-- dynamically load mathjax for compatibility with self-contained --&gt;
&lt;script&gt;
  (function () {
    var script = document.createElement(&#34;script&#34;);
    script.type = &#34;text/javascript&#34;;
    var src = &#34;true&#34;;
    if (src === &#34;&#34; || src === &#34;true&#34;) src = &#34;https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML&#34;;
    if (location.protocol !== &#34;file:&#34;)
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, &#39;&#39;);
    script.src = src;
    document.getElementsByTagName(&#34;head&#34;)[0].appendChild(script);
  })();
&lt;/script&gt;
&lt;/body&gt;

&lt;/html&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>http://localhost:1313/msc-thesis/chap-models/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/msc-thesis/chap-models/</guid>
      <description>&lt;!DOCTYPE html&gt;
&lt;html lang=&#34;&#34; xml:lang=&#34;&#34;&gt;
&lt;head&gt;

  &lt;meta charset=&#34;utf-8&#34; /&gt;
  &lt;meta http-equiv=&#34;X-UA-Compatible&#34; content=&#34;IE=edge&#34; /&gt;
  &lt;title&gt;2 Models of Linguistic Complexity | Interpreting Neural Language Models for Linguistic Complexity Assessment&lt;/title&gt;
  &lt;meta name=&#34;description&#34; content=&#34;MSc Thesis at the University of Trieste and SISSA under the supervision of Prof. Davide Crepaldi and Dott. Felice dell&#39;Orletta&#34; /&gt;
  &lt;meta name=&#34;generator&#34; content=&#34;bookdown 0.20.6 and GitBook 2.6.7&#34; /&gt;

  &lt;meta property=&#34;og:title&#34; content=&#34;2 Models of Linguistic Complexity | Interpreting Neural Language Models for Linguistic Complexity Assessment&#34; /&gt;
  &lt;meta property=&#34;og:type&#34; content=&#34;book&#34; /&gt;
  &lt;meta property=&#34;og:url&#34; content=&#34;https://gsarti.com/master-thesis&#34; /&gt;
  &lt;meta property=&#34;og:image&#34; content=&#34;https://gsarti.com/master-thesisfigures/cover.png&#34; /&gt;
  &lt;meta property=&#34;og:description&#34; content=&#34;MSc Thesis at the University of Trieste and SISSA under the supervision of Prof. Davide Crepaldi and Dott. Felice dell&#39;Orletta&#34; /&gt;
  &lt;meta name=&#34;github-repo&#34; content=&#34;gsarti/interpreting-complexity&#34; /&gt;

  &lt;meta name=&#34;twitter:card&#34; content=&#34;summary&#34; /&gt;
  &lt;meta name=&#34;twitter:title&#34; content=&#34;2 Models of Linguistic Complexity | Interpreting Neural Language Models for Linguistic Complexity Assessment&#34; /&gt;
  
  &lt;meta name=&#34;twitter:description&#34; content=&#34;MSc Thesis at the University of Trieste and SISSA under the supervision of Prof. Davide Crepaldi and Dott. Felice dell&#39;Orletta&#34; /&gt;
  &lt;meta name=&#34;twitter:image&#34; content=&#34;https://gsarti.com/master-thesisfigures/cover.png&#34; /&gt;

&lt;meta name=&#34;author&#34; content=&#34;Gabriele Sarti&#34; /&gt;



  &lt;meta name=&#34;viewport&#34; content=&#34;width=device-width, initial-scale=1&#34; /&gt;
  &lt;meta name=&#34;apple-mobile-web-app-capable&#34; content=&#34;yes&#34; /&gt;
  &lt;meta name=&#34;apple-mobile-web-app-status-bar-style&#34; content=&#34;black&#34; /&gt;
  &lt;link rel=&#34;apple-touch-icon-precomposed&#34; sizes=&#34;152x152&#34; href=&#34;figures/icons/apple-icon.png&#34; /&gt;
  &lt;link rel=&#34;shortcut icon&#34; href=&#34;figures/icons/favicon.ico&#34; type=&#34;image/x-icon&#34; /&gt;
&lt;link rel=&#34;prev&#34; href=&#34;chap-ling-comp.html&#34;/&gt;
&lt;link rel=&#34;next&#34; href=&#34;chap-ex1.html&#34;/&gt;
&lt;style type=&#34;text/css&#34;&gt;
p.abstract{
  text-align: center;
  font-weight: bold;
}
div.abstract{
  margin: auto;
  width: 90%;
}
&lt;/style&gt;
&lt;script src=&#34;libs/jquery-2.2.3/jquery.min.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;libs/gitbook-2.6.7/css/style.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;link href=&#34;libs/gitbook-2.6.7/css/plugin-table.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;link href=&#34;libs/gitbook-2.6.7/css/plugin-bookdown.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;link href=&#34;libs/gitbook-2.6.7/css/plugin-highlight.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;link href=&#34;libs/gitbook-2.6.7/css/plugin-search.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;link href=&#34;libs/gitbook-2.6.7/css/plugin-fontsettings.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;link href=&#34;libs/gitbook-2.6.7/css/plugin-clipboard.css&#34; rel=&#34;stylesheet&#34; /&gt;









&lt;script src=&#34;libs/kePrint-0.0.1/kePrint.js&#34;&gt;&lt;/script&gt;



&lt;link rel=&#34;stylesheet&#34; href=&#34;templates/style.css&#34; type=&#34;text/css&#34; /&gt;
&lt;/head&gt;

&lt;body&gt;



  &lt;div class=&#34;book without-animation with-summary font-size-2 font-family-1&#34; data-basepath=&#34;.&#34;&gt;

    &lt;div class=&#34;book-summary&#34;&gt;
      &lt;nav role=&#34;navigation&#34;&gt;

&lt;ul class=&#34;summary&#34;&gt;
&lt;li&gt;&lt;a href=&#34;introduction.html#introduction&#34;&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1&lt;/b&gt; &lt;strong&gt;Linguistic Complexity&lt;/strong&gt;&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.1&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subchap:categorizing&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.1&lt;/b&gt; Categorizing Linguistic Complexity Measures&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.2&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subchap:intrinsic&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.2&lt;/b&gt; Intrinsic Perspective&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.2.1&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subsubchap:structural&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.2.1&lt;/b&gt; Structural Linguistic Complexity&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.2.2&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subsubchap:lm-surprisal&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.2.2&lt;/b&gt; Language Modeling Surprisal&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.3&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subchap:extrinsic&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.3&lt;/b&gt; Extrinsic Perspective&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.3.1&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subsubchap:readability&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.3.1&lt;/b&gt; Automatic Readability Assessment&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.3.2&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subsubchap:pc&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.3.2&lt;/b&gt; Perceived Complexity Prediction&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.3.3&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subsubchap:eye-tracking&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.3.3&lt;/b&gt; Gaze Metrics Prediction&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.4&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subchap:garden-path&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.4&lt;/b&gt; Garden-path Sentences&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2&lt;/b&gt; &lt;strong&gt;Models of Linguistic Complexity&lt;/strong&gt;&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2.1&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html#subchap:desiderata&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2.1&lt;/b&gt; Desiderata for Models of Linguistic Complexity&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2.2&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html#subchap:nlm&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2.2&lt;/b&gt; Neural Language Models: Unsupervised Multitask Learners&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2.2.1&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html#subsubchap:syntax-nlm&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2.2.1&lt;/b&gt; Emergent Linguistic Structures in Neural Language Models&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2.3&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html#subchap:analyzing-nlm&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2.3&lt;/b&gt; Analyzing Neural Models of Complexity&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2.3.1&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html#subsubchap:probe&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2.3.1&lt;/b&gt; Probing classifiers&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2.3.2&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html#subsubchap:rsa&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2.3.2&lt;/b&gt; Representational Similarity Analysis&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2.3.3&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html#subsubchap:pwcca&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2.3.3&lt;/b&gt; Projection-Weighted Canonical Correlation Analysis&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3&lt;/b&gt; &lt;strong&gt;Complexity Phenomena in Linguistic Annotations and Language Models&lt;/strong&gt;&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3.1&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html#subchap:ex1-data&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3.1&lt;/b&gt; Data and Preprocessing&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3.2&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html#subchap:ex1-analysis&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3.2&lt;/b&gt; Analysis of Linguistic Phenomena&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3.2.1&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html#subsubchap:ex1-analysis-bins&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3.2.1&lt;/b&gt; Linguistic Phenomena in Length-controlled Bins&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3.3&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html#subchap:ex1-modeling&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3.3&lt;/b&gt; Modeling Online and Offline Linguistic Complexity&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3.3.1&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html#subsubchap:ex1-modeling-bins&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3.3.1&lt;/b&gt; Modeling Complexity in Length-controlled Bins&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3.4&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html#subchap:ex1-probing&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3.4&lt;/b&gt; Probing Linguistic Phenomena in ALBERT Representations&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3.5&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html#subchap:ex1-summary&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3.5&lt;/b&gt; Summary&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;4&#34; data-path=&#34;chap-ex2.html&#34;&gt;&lt;a href=&#34;chap-ex2.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;4&lt;/b&gt; &lt;strong&gt;Representational Similarity in Models of Complexity&lt;/strong&gt;&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;4.1&#34; data-path=&#34;chap-ex2.html&#34;&gt;&lt;a href=&#34;chap-ex2.html#knowledge-driven-requirements-for-learning-models&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;4.1&lt;/b&gt; Knowledge-driven Requirements for Learning Models&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;4.2&#34; data-path=&#34;chap-ex2.html&#34;&gt;&lt;a href=&#34;chap-ex2.html#subchap:ex2-experiments&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;4.2&lt;/b&gt; Experimentsl Evaluation&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;4.2.1&#34; data-path=&#34;chap-ex2.html&#34;&gt;&lt;a href=&#34;chap-ex2.html#subsubchap:ex2-data&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;4.2.1&lt;/b&gt; Data&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;4.2.2&#34; data-path=&#34;chap-ex2.html&#34;&gt;&lt;a href=&#34;chap-ex2.html#subsubchap:ex2-inter&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;4.2.2&lt;/b&gt; Inter-model Representational Similarity&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;4.2.3&#34; data-path=&#34;chap-ex2.html&#34;&gt;&lt;a href=&#34;chap-ex2.html#subsubchap:ex2-intra&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;4.2.3&lt;/b&gt; Intra-model Representational Similarity&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;4.3&#34; data-path=&#34;chap-ex2.html&#34;&gt;&lt;a href=&#34;chap-ex2.html#subchap:ex2-summary&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;4.3&lt;/b&gt; Summary&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;5&#34; data-path=&#34;chap-ex3.html&#34;&gt;&lt;a href=&#34;chap-ex3.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;5&lt;/b&gt; &lt;strong&gt;Gaze-informed Models for Cognitive Processing Prediction&lt;/strong&gt;&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;5.1&#34; data-path=&#34;chap-ex3.html&#34;&gt;&lt;a href=&#34;chap-ex3.html#subchap:ex3-setup&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;5.1&lt;/b&gt; Experimental Setup&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;5.2&#34; data-path=&#34;chap-ex3.html&#34;&gt;&lt;a href=&#34;chap-ex3.html#subchap:ex3-experiments&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;5.2&lt;/b&gt; Experimental Evaluation&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;5.2.1&#34; data-path=&#34;chap-ex3.html&#34;&gt;&lt;a href=&#34;chap-ex3.html#subsubchap:ex3-magnitudes&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;5.2.1&lt;/b&gt; Estimating Magnitudes of Garden-path Delays&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;5.2.2&#34; data-path=&#34;chap-ex3.html&#34;&gt;&lt;a href=&#34;chap-ex3.html#subsubchap:ex3-predicting&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;5.2.2&lt;/b&gt; Predicting Delays with Surprisal and Gaze Metrics&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;5.3&#34; data-path=&#34;chap-ex3.html&#34;&gt;&lt;a href=&#34;chap-ex3.html#subchap:ex3-summary&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;5.3&lt;/b&gt; Summary&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;conclusion.html#conclusion&#34;&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;&#34; data-path=&#34;conclusion.html&#34;&gt;&lt;a href=&#34;conclusion.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;Broader Impact and Ethical Perspectives&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;&#34; data-path=&#34;conclusion.html&#34;&gt;&lt;a href=&#34;conclusion.html#future-directions&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;Future Directions&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;appendix&#34;&gt;&lt;span&gt;&lt;b&gt;Appendix&lt;/b&gt;&lt;/span&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;A&#34; data-path=&#34;app-ling-feats.html&#34;&gt;&lt;a href=&#34;app-ling-feats.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;A&lt;/b&gt; Linguistic Features&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;A.1&#34; data-path=&#34;app-ling-feats.html&#34;&gt;&lt;a href=&#34;app-ling-feats.html#raw-text-properties-and-lexical-variety&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;A.1&lt;/b&gt; Raw Text Properties and Lexical Variety&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;A.2&#34; data-path=&#34;app-ling-feats.html&#34;&gt;&lt;a href=&#34;app-ling-feats.html#morpho-syntacting-information&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;A.2&lt;/b&gt; Morpho-syntacting Information&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;A.3&#34; data-path=&#34;app-ling-feats.html&#34;&gt;&lt;a href=&#34;app-ling-feats.html#verbal-predicate-structure&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;A.3&lt;/b&gt; Verbal Predicate Structure&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;A.4&#34; data-path=&#34;app-ling-feats.html&#34;&gt;&lt;a href=&#34;app-ling-feats.html#global-and-local-parsed-tree-structures&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;A.4&lt;/b&gt; Global and Local Parsed Tree Structures&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;A.5&#34; data-path=&#34;app-ling-feats.html&#34;&gt;&lt;a href=&#34;app-ling-feats.html#syntactic-relations&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;A.5&lt;/b&gt; Syntactic Relations&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;A.6&#34; data-path=&#34;app-ling-feats.html&#34;&gt;&lt;a href=&#34;app-ling-feats.html#subordination-phenomena&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;A.6&lt;/b&gt; Subordination Phenomena&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;B&#34; data-path=&#34;app-et-metrics.html&#34;&gt;&lt;a href=&#34;app-et-metrics.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;B&lt;/b&gt; Precisions on Eye-tracking Metrics and Preprocessing&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;C&#34; data-path=&#34;app-et-modeling.html&#34;&gt;&lt;a href=&#34;app-et-modeling.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;C&lt;/b&gt; Multi-task Token-level Regression for Gaze Metrics Prediction&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;D&#34; data-path=&#34;app-intra-sim.html&#34;&gt;&lt;a href=&#34;app-intra-sim.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;D&lt;/b&gt; Intra-model Similarity for All Models&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;E&#34; data-path=&#34;app-garden-paths-et.html&#34;&gt;&lt;a href=&#34;app-garden-paths-et.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;E&lt;/b&gt; Gaze Metrics Predictions for Garden Path Sentences&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;F&#34; data-path=&#34;app-params.html&#34;&gt;&lt;a href=&#34;app-params.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;F&lt;/b&gt; Reproducibility and Environmental Impact&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;&#34; data-path=&#34;references.html&#34;&gt;&lt;a href=&#34;references.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;References&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;divider&#34;&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://gsarti.com&#34;&gt;Back to my website&lt;/a&gt;&lt;/li&gt;

&lt;/ul&gt;

      &lt;/nav&gt;
    &lt;/div&gt;

    &lt;div class=&#34;book-body&#34;&gt;
      &lt;div class=&#34;body-inner&#34;&gt;
        &lt;div class=&#34;book-header&#34; role=&#34;navigation&#34;&gt;
          &lt;h1&gt;
            &lt;i class=&#34;fa fa-circle-o-notch fa-spin&#34;&gt;&lt;/i&gt;&lt;a href=&#34;./&#34;&gt;Interpreting Neural Language Models&lt;br /&gt;
for Linguistic Complexity Assessment&lt;/a&gt;
          &lt;/h1&gt;
        &lt;/div&gt;

        &lt;div class=&#34;page-wrapper&#34; tabindex=&#34;-1&#34; role=&#34;main&#34;&gt;
          &lt;div class=&#34;page-inner&#34;&gt;

            &lt;section class=&#34;normal&#34; id=&#34;section-&#34;&gt;
&lt;div id=&#34;chap:models&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;2&lt;/span&gt; &lt;strong&gt;Models of Linguistic Complexity&lt;/strong&gt;&lt;/h1&gt;
&lt;p&gt;&lt;!-- this will include a mini table of contents--&gt;&lt;/p&gt;

&lt;p&gt;Standard linguistic complexity studies analyze complexity annotations produced by human subjects to evaluate how specific language structures influence our perception of complexity under various viewpoints. For example, one can derive insights about early cognitive processing by looking at early gaze metrics, like first pass duration and first fixation duration, or study language comprehension by evaluating perceived complexity annotations. These approaches rely on a single implicit assumption: that &lt;em&gt;complexity annotations contain enough information to reflect the input’s underlying complexity properties&lt;/em&gt; appropriately. Without this premise, there would be a complete disconnect between human subjective perception, as reflected by annotations and linguistic structures. Given the ever-growing compelling evidence derived from carefully-planned complexity research, I argue that this is a relatively safe assumption to be made.&lt;/p&gt;
&lt;p&gt;This work instead adopts a modeling-driven approach for the study of linguistic complexity. Annotations produced by human subjects still play a fundamental role in this context. However, instead of acting as the main subject of analysis, they are used as a source of distant supervision to create computational models of linguistic complexity. More specifically, machine learning models are trained to predict complexity annotation from raw input text by minimizing a task-specific loss function. The &lt;strong&gt;learning step&lt;/strong&gt; here is fundamental, given the connection mentioned above between linguistic complexity and knowledge acquisition. After the training process, human annotations are put aside, and the model itself is studied as a complexity-sensitive subject: in particular, this study focuses on how the information encoded in the parameters of complexity-trained models is related to structural linguistic properties (Chapter &lt;a href=&#34;chap-ex1.html#chap:ex1&#34;&gt;3&lt;/a&gt;), how this information differs when models are exposed to different complexity perspectives during training (Chapter &lt;a href=&#34;chap-ex2.html#chap:ex2&#34;&gt;4&lt;/a&gt;) and finally how the encoded knowledge affects models’ generalization capabilities over unseen constructs (Chapter &lt;a href=&#34;chap-ex3.html#chap:ex3&#34;&gt;5&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;While this approach still relies on the &lt;strong&gt;annotation pertinence assumption&lt;/strong&gt; stated above, it requires making a second, stronger hypothesis: that &lt;em&gt;models employed can grasp a significant portion of the relations subsisting between language structures and complexity perspectives&lt;/em&gt;. This assumption can be further declined in two requirements. First, from a &lt;strong&gt;conceptual&lt;/strong&gt; point-of-view, we must ensure that the model architecture is endowed with meaningful inductive biases concerning what is currently known about linguistic complexity. This includes having sufficient approximation capabilities to capture linguistic complexity phenomena, which are likely to be highly-nonlinear functions of the input. From a &lt;strong&gt;functional&lt;/strong&gt; perspective, then, we should confirm that the quality of model predictions is sufficiently close to human-produced annotations to make their production mechanisms worth investigating.&lt;/p&gt;
&lt;p&gt;This chapter justifies the selected modeling approach and introduces models later employed in complexity assessment experiments. Section &lt;a href=&#34;chap-models.html#subchap:desiderata&#34;&gt;2.1&lt;/a&gt; discusses the conceptual requirements for linguistic complexity modeling and motivates the choice of pretrained &lt;strong&gt;neural language models&lt;/strong&gt; as primary subjects of this thesis work. Section &lt;a href=&#34;chap-models.html#subchap:nlm&#34;&gt;2.2&lt;/a&gt; presents the architectures used in experimental sections and their desirable properties regarding the encoding of linguistic structures in latent representations. Finally, Section &lt;a href=&#34;chap-models.html#subchap:analyzing-nlm&#34;&gt;2.3&lt;/a&gt; presents the challenge of interpreting NLM’s representations and behaviors and introduces various interpretability approaches used throughout this study.&lt;/p&gt;
&lt;div id=&#34;subchap:desiderata&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.1&lt;/span&gt; Desiderata for Models of Linguistic Complexity&lt;/h2&gt;
&lt;p&gt;From the in-depth analysis of Chapter &lt;a href=&#34;chap-ling-comp.html#chap:ling-comp&#34;&gt;1&lt;/a&gt;, we can distill some general desiderata for an idealized LCA model &lt;span class=&#34;math inline&#34;&gt;\(M^*\)&lt;/span&gt;. From a linguistic perspective:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(M^*\)&lt;/span&gt; &lt;em&gt;should distinguish between lexical forms and be informed about their probability of occurrence.&lt;/em&gt; This is a basic (although fundamental) step given the importance of words’ variety and frequency in determining our perception of complexity.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(M^*\)&lt;/span&gt; &lt;em&gt;should be aware of syntactic structures and sensitive to their properties.&lt;/em&gt; As we saw with garden-path sentences, atypical or ambiguous syntax constructs are among the most prominent factors for determining the magnitude of processing difficulties. An ideal model should map complex syntactic constructs to higher complexity scores and discriminate potentially ambiguous or problematic structures from regular ones, even when changes in the form are minimal (e.g., when a single comma is missing).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(M^*\)&lt;/span&gt; &lt;em&gt;should capture semantic information and relations between entities.&lt;/em&gt; Ideally, this means the ability to frame agents, patients, and actions in a semantic context and evaluate how likely or typical the latter is. For example, semantically unrelated entities occurring together in a sentence should produce an increase in processing difficulties. This includes the ability to disambiguate polysemic terms (e.g., “fly” verb vs. noun) given the surrounding context.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Then, from a technical standpoint:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(M^*\)&lt;/span&gt; &lt;em&gt;should not rely on hand-crafted features to represent language&lt;/em&gt;. This is an implicit requirement since this study aims to analyze how the model autonomously learns to represent language in its parameters while simultaneously encoding information about its complexity. Chapter &lt;a href=&#34;chap-ex1.html#chap:ex1&#34;&gt;3&lt;/a&gt; presents how complexity models with hand-crafted features compare to those selected for the study.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(M^*\)&lt;/span&gt; &lt;em&gt;should not rely too heavily on labeled data.&lt;/em&gt; Complexity datasets presented in Chapter &lt;a href=&#34;chap-ling-comp.html#chap:ling-comp&#34;&gt;1&lt;/a&gt; are usually composed of a few thousand labeled examples. While this may seem a lot to our eyes, a language model may require a lot more information to achieve sufficient generalization capabilities. A viable option in this context, as we will see with NLMs, is to prime models with general linguistic knowledge through an unsupervised pretraining procedure before training them on complexity-related tasks.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(M^*\)&lt;/span&gt; &lt;em&gt;should be sufficiently interpretable.&lt;/em&gt; Ideally, we would like to draw direct causal relations from input properties to complexity prediction in a consistent way across complexity perspectives. More realistically, we need at least to find coherent patterns between the model’s inputs and its predictive behaviors.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Most standard modeling approaches fail to encompass even a small subset of those non-trivial requirements. For example, one can consider modeling complexity properties with static word representations &lt;span class=&#34;citation&#34;&gt;(Turian, Ratinov, and Bengio &lt;a href=&#34;#ref-turian-etal-2010-word&#34;&gt;2010&lt;/a&gt;)&lt;/span&gt; such as Word2Vec or GloVe embeddings &lt;span class=&#34;citation&#34;&gt;(Mikolov et al. &lt;a href=&#34;#ref-mikolov-etal-2013-efficient&#34;&gt;2013&lt;/a&gt;; Pennington, Socher, and Manning &lt;a href=&#34;#ref-pennington-etal-2014-glove&#34;&gt;2014&lt;/a&gt;)&lt;/span&gt;. In these approaches, feature vectors representing words are learned by a neural network through a pretraining procedure to model word co-occurrences. While these approaches were shown to capture a significant amount of semantic information while reducing the dependence on labeled data thanks to pretraining, static word embeddings generally yield modest results when employed for syntactic predictions &lt;span class=&#34;citation&#34;&gt;(Andreas and Klein &lt;a href=&#34;#ref-andreas-klein-2014-much&#34;&gt;2014&lt;/a&gt;)&lt;/span&gt;. Moreover, since the model learns a direct mapping &lt;span class=&#34;math inline&#34;&gt;\(f: t_i \rightarrow \textbf{v}_i\)&lt;/span&gt; from lexical forms to vectorial representations, polysemic terms are reduced to single context-independent representation, and contextual information that often plays a crucial role in determining complexity is mixed and diluted.&lt;/p&gt;
&lt;p&gt;Among more sophisticated modeling approaches for representing language, I argue that modern &lt;strong&gt;neural language models&lt;/strong&gt; (NLMs) are the approaches that yield a better match for the requirements stated above. These models consist of multi-layer neural networks &lt;span class=&#34;citation&#34;&gt;(Goodfellow et al. &lt;a href=&#34;#ref-goodfellow-etal-2016-deep&#34;&gt;2016&lt;/a&gt;)&lt;/span&gt; pretrained using standard language modeling or masked language modeling training objectives to produce &lt;strong&gt;contextualized word embeddings&lt;/strong&gt;, which were shown to be very effective in downstream syntactic and semantic tasks &lt;span class=&#34;citation&#34;&gt;(Peters et al. &lt;a href=&#34;#ref-peters-etal-2018-deep&#34;&gt;2018&lt;/a&gt;)&lt;/span&gt; even with relatively few labeled examples. Moreover, being language models, NLMs predict a probability distribution over their vocabulary at each step, enabling us to compute information-theoretic metrics such as surprisal that we saw being conceptually close to one-stage cognitive processing accounts. Finally, their high parameter counts and the presence of self-attention mechanisms &lt;span class=&#34;citation&#34;&gt;(Bahdanau, Cho, and Bengio &lt;a href=&#34;#ref-bahdanau-etal-2015-neural&#34;&gt;2015&lt;/a&gt;; Vaswani et al. &lt;a href=&#34;#ref-vaswani-etal-2017-attention&#34;&gt;2017&lt;/a&gt;)&lt;/span&gt; as learned weighting functions suggests that NLMs might be capable of learning to approximate highly nonlinear functions effectively.&lt;/p&gt;
&lt;p&gt;The most significant downside of NLMs in the context of our analysis is their opaqueness. As for most neural networks, the nonlinear multi-layer structure that characterizes NLMs makes them incredibly valid function approximators. At the same time, though, it hinders our efforts in interpreting their behaviors &lt;span class=&#34;citation&#34;&gt;(Samek et al. &lt;a href=&#34;#ref-samek-etal-2019-explainable&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt;. Because of this fact, in recent years, we witnessed a surge in approaches trying to “open the black box” of neural networks by using various techniques borrowed from information theory &lt;span class=&#34;citation&#34;&gt;(Shwartz-Ziv and Tishby &lt;a href=&#34;#ref-shwartz-tishby-2017-opening&#34;&gt;2017&lt;/a&gt;)&lt;/span&gt; and cognitive science &lt;span class=&#34;citation&#34;&gt;(Kriegeskorte, Mur, and Bandettini &lt;a href=&#34;#ref-kriegeskorte-etal-2008-representational&#34;&gt;2008&lt;/a&gt;)&lt;/span&gt;. Given the wide availability of these approaches, this work joins the choir of interpretability researchers and argues that studying how such performant models encode their knowledge about language complexity is still a matter of interest and worth exploring. In the next section, the architecture and training process of NLMs will be formalized, and their properties will be described in detail.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;subchap:nlm&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.2&lt;/span&gt; Neural Language Models: Unsupervised Multitask Learners&lt;/h2&gt;
&lt;p&gt;The objective of natural language processing applications such as &lt;em&gt;summarization&lt;/em&gt;, &lt;em&gt;machine translation&lt;/em&gt;, and &lt;em&gt;dialogue generation&lt;/em&gt; is to produce text that is both &lt;strong&gt;fluent&lt;/strong&gt; and contextually accurate. As we saw in Chapter &lt;a href=&#34;chap-ling-comp.html#chap:ling-comp&#34;&gt;1&lt;/a&gt;, a text’s fluency can also be used as a significant factor in determining its complexity from a linguistic viewpoint. A possible approach to establishing a sentence’s fluency is to rely on &lt;strong&gt;relative frequency estimates&lt;/strong&gt; for words in large corpora. Consider a sentence &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt; and a large corpus &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{C}\)&lt;/span&gt;. We can estimate its probability of occurrence in natural language as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
P(s) = \frac{\text{count}(s)}{|\mathcal{C}|}
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;While this is an unbiased estimator since it converges to the actual frequency value when the corpus size is sufficiently large, it is both very data-reliant and highly unreliable. If a sentence happens to be absent in &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{C}\)&lt;/span&gt;, it will be assigned probability equal to zero. Therefore, we need to rely on other approaches, such as language models, to obtain reliable estimates from limited training datasets.&lt;/p&gt;
&lt;p&gt;As we saw in Chapter &lt;a href=&#34;chap-ling-comp.html#subsubchap:lm-surprisal&#34;&gt;1.2.2&lt;/a&gt;, language models assign probabilities to sequences of tokens. Formally, this can be framed as learning words’ conditional probability distributions given their context, either &lt;em&gt;preceding&lt;/em&gt; or &lt;em&gt;bidirectional&lt;/em&gt; depending on the language modeling approach. I will here refer to sequential language models unless otherwise mentioned.&lt;/p&gt;
&lt;p&gt;Language models are trained on sequences &lt;span class=&#34;math inline&#34;&gt;\(\textbf{x} = \langle x_1, \dots, x_n \rangle\)&lt;/span&gt; composed by &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; tokens taken from a predefined vocabulary &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{V}\)&lt;/span&gt;. Each token &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; can be represented as a one-hot encoded vector &lt;span class=&#34;math inline&#34;&gt;\(x_i \in \{0,1\}^{|\mathcal{V}|}\)&lt;/span&gt;, and the probability of sequence &lt;span class=&#34;math inline&#34;&gt;\(\textbf{x}\)&lt;/span&gt; is factored using the chain rule:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
P(\textbf{x}) = \prod_{t=1}^{n}\,P(x_t\,|\,x_1,\dots,x_{t-1})
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;After the training process, we can use the likelihood that the model assigns to &lt;strong&gt;held-out data&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(\textbf{y}\)&lt;/span&gt; treated as a single stream of &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; tokens as an intrinsic evaluation metric for the quality of its predictions:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
\ell(\textbf{y}) = \sum_{t=1}^m \log P(x_t|x_1,\dots,x_{t-1})
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\ell(\textbf{y})\)&lt;/span&gt; can be rephrased in terms of &lt;strong&gt;perplexity&lt;/strong&gt;, an information-theoretic metric independent from the size of the held-out set:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
\text{PPL}(\textbf{y}) = 2^{-\ell(\textbf{y})/m}
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\text{PPL}\)&lt;/span&gt; is equal to 1 if the language model is perfect (i.e., predicts all tokens in the held-out corpus with probability 1) and matches the vocabulary size &lt;span class=&#34;math inline&#34;&gt;\(|\mathcal{V}|\)&lt;/span&gt; when the model assign a uniform probability to all tokens in the vocabulary (a “random” language model):&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align} 
\log_2(\textbf{y}) = \sum_{t=1}^m \log_2 \frac{1}{|\mathcal{V}|} = - \sum_{t=1}^m \log_2 |\mathcal{V}| = -m \log_2 |\mathcal{V}| \\
\text{PPL}(\textbf{y}) = 2^{\frac{1}{m}m\log_2 |\mathcal{V}|} = 2^{\log_2 |\mathcal{V}|} = |\mathcal{V}|
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Perplexity represents the number of bits required to encode the average word in the corpora. For example, reporting a perplexity score of 10 over a held-out corpus means that the language model will predict on average words with the same accuracy as if it had to choose uniformly and independently across ten possibilities for each word.&lt;/p&gt;
&lt;p&gt;While tokens used by language models generally correspond to words in most NLP pipelines, recent language modeling work highlighted the effectiveness of using subword tokens &lt;span class=&#34;citation&#34;&gt;(Sennrich, Haddow, and Birch &lt;a href=&#34;#ref-sennrich-etal-2016-neural&#34;&gt;2016&lt;/a&gt;; Wu et al. &lt;a href=&#34;#ref-wu-etal-2016-google&#34;&gt;2016&lt;/a&gt;; Kudo and Richardson &lt;a href=&#34;#ref-kudo-richardson-2018-sentencepiece&#34;&gt;2018&lt;/a&gt;)&lt;/span&gt; or even single characters to further improve LM’s generalization performances. In particular, models used in this work rely on SentencePiece and Byte-Pair Encoding (BPE) subword tokenization &lt;span class=&#34;citation&#34;&gt;(Sennrich, Haddow, and Birch &lt;a href=&#34;#ref-sennrich-etal-2016-neural&#34;&gt;2016&lt;/a&gt;; Kudo and Richardson &lt;a href=&#34;#ref-kudo-richardson-2018-sentencepiece&#34;&gt;2018&lt;/a&gt;)&lt;/span&gt;. The SentencePiece algorithm derives a fixed-size vocabulary from word co-occurrences in a large corpus and treats whitespace as a normal symbol by converting it to “&lt;strong&gt;_&lt;/strong&gt;”, while BPE does the same using the “Ġ” character. For example:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Input sentence:&lt;/strong&gt; Heteroscedasticity is hard to model!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;SentencePiece tokenization:&lt;/strong&gt; &lt;strong&gt;_&lt;/strong&gt;Hetero s ced astic ity &lt;strong&gt;_&lt;/strong&gt;is &lt;strong&gt;_&lt;/strong&gt;hard &lt;strong&gt;_&lt;/strong&gt;to &lt;strong&gt;_&lt;/strong&gt;model !&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;BPE tokenization:&lt;/strong&gt; H eter os ced astic ity Ġis Ġhard Ġto Ġmodel !&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;where whitespaces correspond to separators after tokenization. From the example, we can observe that frequent words like &lt;em&gt;hard&lt;/em&gt;, &lt;em&gt;to&lt;/em&gt; and &lt;em&gt;model&lt;/em&gt; are treated similarly by both tokenizers, while rare words like &lt;em&gt;heteroscedasticity&lt;/em&gt; are split into subwords depending on their observed frequency inside the tokenizer’s training corpus.&lt;/p&gt;
&lt;p&gt;In recent years n-gram language models, which were the most common approach to estimate probabilities from relative frequencies, have been largely supplanted by neural networks. A significant advantage of neural approaches is the overcoming of context restrictions: relevant information can be incorporated from arbitrarily distant contexts while preserving the tractability of the problem from both a statistical and a computational viewpoint.&lt;/p&gt;
&lt;p&gt;Neural language models treat language modeling as a &lt;em&gt;discriminative&lt;/em&gt; learning task aimed at maximizing the log conditional probability of a corpus. Formally, the probability distribution &lt;span class=&#34;math inline&#34;&gt;\(p(x|c)\)&lt;/span&gt; is reparametrized as the dot product of two dense numeric vectors &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol\theta_x, \boldsymbol h_c \in \mathbb{R}^H\)&lt;/span&gt; under a softmax transformation:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34; id=&#34;eq:softmax-lm&#34;&gt;\[\begin{equation}
P(x|c) = \frac{\exp(\boldsymbol\theta_x \cdot \boldsymbol h_c)}{\sum_{x&amp;#39;\in\mathcal{V}} \exp(\boldsymbol\theta_{x&amp;#39;} \cdot \boldsymbol h_c)}
\tag{2.1}
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In &lt;a href=&#34;chap-models.html#eq:softmax-lm&#34;&gt;(2.1)&lt;/a&gt;, the denominator is present to ensure that the probability distribution is properly normalized over vocabulary &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{V}\)&lt;/span&gt;. &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol\theta_x\)&lt;/span&gt; represent model parameters that can be learned through an iterative procedure, while &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol h_c\)&lt;/span&gt; is the contextual information that can be computed in different ways depending on the model. For example, a neural language model based on the &lt;strong&gt;recurrent neural network&lt;/strong&gt; architecture (RNN; &lt;span class=&#34;citation&#34;&gt;Mikolov et al. (&lt;a href=&#34;#ref-mikolov-etal-2010-recurrent&#34;&gt;2010&lt;/a&gt;)&lt;/span&gt;) recurrently updates context vectors initialized at random with relevant information that needs to be preserved while moving through the sequence.&lt;a href=&#34;#fn10&#34; class=&#34;footnote-ref&#34; id=&#34;fnref10&#34;&gt;&lt;sup&gt;10&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This work leverages models belonging to the most recent and influential family of neural language models at the time of writing, that is, the one based on the &lt;strong&gt;Transformer&lt;/strong&gt; architecture &lt;span class=&#34;citation&#34;&gt;(Vaswani et al. &lt;a href=&#34;#ref-vaswani-etal-2017-attention&#34;&gt;2017&lt;/a&gt;)&lt;/span&gt;. Transformers are deep learning models designed to handle sequential data and were conceived to compensate for a significant downside of recurrent models: the need to process data in an orderly manner to perform backpropagation through time. By replacing recurrent computations with attention mechanisms to maintain contextual information throughout the model, Transformers’ operations are entirely parallelizable on dedicated hardware and &lt;em&gt;therefore lead to reduced training times&lt;/em&gt;. This fact is especially relevant considering the massive corpora size used to pretrain neural language models to obtain contextual representations. &lt;strong&gt;Self-attention&lt;/strong&gt; was also shown to behave better than other approaches at learning long-range dependencies, avoiding the &lt;em&gt;vanishing gradient&lt;/em&gt; problem that plagued non-gated recurrent NLMs altogether &lt;span class=&#34;citation&#34;&gt;(Pascanu, Mikolov, and Bengio &lt;a href=&#34;#ref-pascanu-etal-2013-difficulty&#34;&gt;2013&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;

&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:transformer&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;figures/2_transformer.png&#34; alt=&#34;The original Transformer model architecture by Vaswani et al. (2017).&#34; width=&#34;50%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 2.1: The original Transformer model architecture by &lt;span class=&#34;citation&#34;&gt;Vaswani et al. (&lt;a href=&#34;#ref-vaswani-etal-2017-attention&#34;&gt;2017&lt;/a&gt;)&lt;/span&gt;.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The original Transformer architecture comprises an encoder and a decoder, each composed of a stacked sequence of identical layers that transform input embeddings in outputs with the same dimension (hence the name). First, the encoder maps the sequence &lt;span class=&#34;math inline&#34;&gt;\((x_1, \dots, x_n)\)&lt;/span&gt; to a sequence of embeddings &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol z = (z_1, \dots, z_n)\)&lt;/span&gt;. Given &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol z\)&lt;/span&gt;, the decoder then autoregressively produces an output token sequence &lt;span class=&#34;math inline&#34;&gt;\((y_1, \dots, y_m)\)&lt;/span&gt;. Each layer of the Transformer encoder comprises two sublayers, a &lt;strong&gt;multi-head self-attention mechanism&lt;/strong&gt; and a &lt;strong&gt;feed-forward network&lt;/strong&gt;, surrounded by residual connections and followed by layer normalization. The decoder includes a third layer that performs multi-head self-attention over the encoder output and modifies the original self-attention sublayer to prevent attending to future context, as required by the language modeling objective. Figure &lt;a href=&#34;chap-models.html#fig:transformer&#34;&gt;2.1&lt;/a&gt; presents the original architecture for a &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;-layer Transformer. I will now proceed to describe the main components of the Transformer model.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;custompar&#34;&gt;Positional Encodings&lt;/span&gt; The original Transformer relies on two sets of embeddings to represent the input sequence: learned &lt;strong&gt;word embeddings&lt;/strong&gt;, used as vector representations for each token in the vocabulary, and fixed &lt;strong&gt;positional encodings&lt;/strong&gt; (PEs) used to inject information about the position of tokens in the sequence. Those are needed since no information about the sequential nature of the input would otherwise be preserved. For position &lt;span class=&#34;math inline&#34;&gt;\(pos\)&lt;/span&gt; and dimension &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;, PEs correspond to sinusoidal periodic functions that were empirically shown to perform on par with learned embeddings, and were chosen to enable extrapolation for longer sequences:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align} 
PE_{pos, 2i} = \sin(\text{pos}/10000^{2i/|h|}) \\
PE_{pos, 2i + 1} = \cos(\text{pos}/10000^{2i/|h|})
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(|h|\)&lt;/span&gt; is the model’s hidden layer size. Embeddings and PEs are summed and passed to the attention layer.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;custompar&#34;&gt;Self-Attention&lt;/span&gt; The &lt;em&gt;scaled dot-product self-attention&lt;/em&gt; mechanisms is the driving force of the Transformer architecture. Given an input embedding matrix &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;, we multiply it by three weight matrices &lt;span class=&#34;math inline&#34;&gt;\(W^Q, W^K, W^V\)&lt;/span&gt; obtaining the projections &lt;span class=&#34;math inline&#34;&gt;\(Q\)&lt;/span&gt; (&lt;strong&gt;queries&lt;/strong&gt;), &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; (&lt;strong&gt;keys&lt;/strong&gt;) and &lt;span class=&#34;math inline&#34;&gt;\(V\)&lt;/span&gt; (&lt;strong&gt;values&lt;/strong&gt;). Those are then combined by the self-attention function as follows:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
\text{Attention(Q,K,V)} = \text{softmax}\Big ( \frac{QK^T}{\sqrt{d_k}}\Big)V
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(d_k\)&lt;/span&gt; is the size of individual query and key vectors. The output of this operation is a matrix &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; which will be passed to the feed-forward layer. The self attention mechanism is further extended to &lt;strong&gt;multi-head self-attention&lt;/strong&gt; in Transformer architectures. In the multi-head variant, the attention function is applied in parallel to &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; version of queries, keys and values projected with learned parameter matrices, and outputs are finally concatenated and projected again to obtain final values:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
\text{MultiHead}(Q,K,V) = \text{Concat}(\text{head}_1,\dots, \text{head}_n)W^O \\
\text{where } \text{head}_i = \text{Attention}(QW_i^Q,KW_i^K,VW_i^V)
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Where &lt;span class=&#34;math inline&#34;&gt;\(W_i^Q \in \mathbb{R}^{|h| \times d_k}\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(W_i^K \in \mathbb{R}^{|h| \times d_k}\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(W_i^V \in \mathbb{R}^{|h| \times d_v}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(W^O \in \mathbb{R}^{nd_v \times |h|}\)&lt;/span&gt;. In multi-head attention layers of Figure &lt;a href=&#34;chap-models.html#fig:transformer&#34;&gt;2.1&lt;/a&gt;, each position can attend to all position from the previous layer, while in the &lt;strong&gt;masked multi-head attention&lt;/strong&gt; layer only previous positions in the sequence can be attended by applying a triangular mask to attention matrices. This additional step is needed to preserve the autoregressive property during decoding.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;custompar&#34;&gt;Feed-forward Layer&lt;/span&gt; Each block in the encoder and the decoder contains an independent fully connected 2-layer feed-forward network with a ReLU nonlinearity applied separately to each position of the sequence:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
\text{FFN}(Z) = \max(0,Z\,\Theta_1 + b_1)\Theta_2 + b_2
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt; are the representations passed forward from the attention sublayer, &lt;span class=&#34;math inline&#34;&gt;\(\Theta_1, \Theta_2\)&lt;/span&gt; are two learned independent parameter matrices for each layer and &lt;span class=&#34;math inline&#34;&gt;\(b_1, b_2\)&lt;/span&gt; are their respective bias vectors.&lt;/p&gt;
&lt;p&gt;Now that the main concepts regarding the Transformer architecture have been introduced, the two Transformer-based models used in this study will be presented.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;custompar&#34;&gt;GPT-2&lt;/span&gt; GPT-2 &lt;span class=&#34;citation&#34;&gt;(Radford et al. &lt;a href=&#34;#ref-radford-etal-2019-language&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt; is a transformer model built using only the decoder blocks with masked self-attention, alongside BPE tokenization. The latter’s autoregressive capabilities, i.e. being able to iteratively add a newly predicted token to the existing sequence in the next steps, make it especially suitable for text generation and related tasks. The learning of model parameters is performed in two stages. First, an &lt;strong&gt;unsupervised pretraining&lt;/strong&gt; is carried out to learn a high capacity language model on a large corpus: in particular, here the model is trained to maximize the likelihood of sequential language modeling over &lt;strong&gt;WebText&lt;/strong&gt;, a corpus containing roughly 8 million documents (40GB of text), by adapting its parameters using stochastic gradient descent. The purpose of this step is to learn contextual word embeddings encoding both low and high-level information that can be recycled in downstream tasks, following the &lt;strong&gt;transfer learning&lt;/strong&gt; approach inspired by the field of computer vision and initially proposed by &lt;span class=&#34;citation&#34;&gt;Howard and Ruder (&lt;a href=&#34;#ref-howard-ruder-2018-universal&#34;&gt;2018&lt;/a&gt;)&lt;/span&gt; for NLP. The second step is a &lt;strong&gt;supervised fine-tuning&lt;/strong&gt;, where the language modeling softmax layer is replaced by a task-specific layer (called &lt;strong&gt;head&lt;/strong&gt;) with parameters &lt;span class=&#34;math inline&#34;&gt;\(W_y\)&lt;/span&gt; receiving final transformer activations &lt;span class=&#34;math inline&#34;&gt;\(h_l\)&lt;/span&gt; and predicting a label &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; (e.g. in a classification task) as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
P(y|x_1,\dots, x_m) = \text{softmax}(h^{sent}_lW_y)
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(h_l^{sent}\)&lt;/span&gt; is the sentence-level representation for &lt;span class=&#34;math inline&#34;&gt;\((x_1, \dots, x_m)\)&lt;/span&gt;. The parameters of the whole model, including transformer blocks and task-specific heads, can then be tuned by minimizing the loss &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{L}\)&lt;/span&gt; over the whole supervised corpus &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{C}\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
\mathcal{L}(\mathcal{C}) = - \sum_{(x,y)} \log P(y|x_1, \dots, x_m) 
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Figure &lt;a href=&#34;chap-models.html#fig:gpt2&#34;&gt;2.2&lt;/a&gt; visualizes the forward pass through the GPT-2 architecture. We see from the figure that attention patterns learned during pre-trained are often interpretable. Here, the token &lt;em&gt;it&lt;/em&gt; is correctly identified as the pronoun referring to the subject &lt;em&gt;a robot&lt;/em&gt;. Authors show how large NLMs such as GPT-2 become strong unsupervised multitask learners when trained on sufficiently large corpora, providing the initial motivation for choosing pretrained Transformer models for experiments throughout this study. GPT-2 will be specifically be employed in the experiments of Chapter &lt;a href=&#34;chap-ex3.html#chap:ex3&#34;&gt;5&lt;/a&gt;, where its autoregressive nature is ideal for replicating human surprisal estimates during sequential reading on garden-path sentences.&lt;/p&gt;

&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:gpt2&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;figures/2_gpt2.png&#34; alt=&#34;An overview of the forward pass in GPT-2. Adapted from Alammar (2018b).&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 2.2: An overview of the forward pass in GPT-2. Adapted from &lt;span class=&#34;citation&#34;&gt;Alammar (&lt;a href=&#34;#ref-alammar-2018-illustratedgpt2&#34;&gt;2018&lt;/a&gt;&lt;a href=&#34;#ref-alammar-2018-illustratedgpt2&#34;&gt;b&lt;/a&gt;)&lt;/span&gt;.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;span class=&#34;custompar&#34;&gt;ALBERT&lt;/span&gt; ALBERT &lt;span class=&#34;citation&#34;&gt;(Lan et al. &lt;a href=&#34;#ref-lan-etal-2020-albert&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt; is an efficient variant of the Bidirectional Encoder Representations from Transformers (&lt;strong&gt;BERT&lt;/strong&gt;) approach by &lt;span class=&#34;citation&#34;&gt;Devlin et al. (&lt;a href=&#34;#ref-devlin-etal-2019-bert&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt;. BERT was built following the intuition that many sentence-level tasks would greatly benefit from an approach capable of incorporating bidirectional context inside language representations. This is not the case for decoder-based approaches like GPT-2 that, being aimed at generation-oriented tasks, could only leverage the previous context using masked self-attention. BERT tackles the unidirectional constraint by introducing &lt;strong&gt;masked language modeling&lt;/strong&gt; (MLM, see Equation &lt;a href=&#34;chap-ling-comp.html#eq:sent-surprisal-cases&#34;&gt;(1.2)&lt;/a&gt;) and using a stack of transformer encoder layers with GELU nonlinearities &lt;span class=&#34;citation&#34;&gt;(Hendrycks and Gimpel &lt;a href=&#34;#ref-hendrycks-gimpel-2016-gaussian&#34;&gt;2016&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;As for GPT-2, the pretraining and fine-tuning steps are taken to provide the model with general language knowledge and subsequently adapt it to specific downstream tasks. At each pretraining step, a fixed portion of input tokens get masked, and the model predicts the original vocabulary id of masked tokens. Moreover, a sentence-level task is used to improve discourse coherence. For BERT, the &lt;strong&gt;next sentence prediction&lt;/strong&gt; (NSP) task is adopted, i.e. determining whether, given two sentences, they are consecutive or not in the original text using both positive and negative pairs. NSP was found unreliable by subsequent studies and was replaced in ALBERT by a &lt;strong&gt;sentence ordering prediction&lt;/strong&gt; loss that is more challenging for the model. A third set of &lt;strong&gt;segment embeddings&lt;/strong&gt; is added to initial representations to distinguish input sentences in multi-sentence tasks. Special tokens &lt;code&gt;[CLS]&lt;/code&gt; and &lt;code&gt;[SEP]&lt;/code&gt; are added as sentence-level representations.&lt;/p&gt;
&lt;p&gt;ALBERT introduces two main contributions aimed at reducing the final number of model parameters inside BERT:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Factorized embedding parametrization&lt;/strong&gt;: a projection layer is introduced between the embedding matrix &lt;span class=&#34;math inline&#34;&gt;\(E\)&lt;/span&gt; and the hidden layer &lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt; of the model so that the dimensions of the two are untied. This approach modifies embedding parameter count from &lt;span class=&#34;math inline&#34;&gt;\(O(|V| \times |E|)\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\(O(|\mathcal{V}| \times |E| + |E| \times |h|)\)&lt;/span&gt;, with &lt;span class=&#34;math inline&#34;&gt;\(|\mathcal{V}|, |E|, |h|\)&lt;/span&gt; being respectively the sizes of vocabulary, embedding vectors and hidden layers. A significant reduction in model parameters is therefore produce when &lt;span class=&#34;math inline&#34;&gt;\(|h| \gg |E|\)&lt;/span&gt;, which is desirable since &lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt; contains &lt;em&gt;context-dependent representations&lt;/em&gt; that encode more information than the &lt;em&gt;context-independent&lt;/em&gt; ones of &lt;span class=&#34;math inline&#34;&gt;\(E\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Cross-layer parameter sharing&lt;/strong&gt;: All layers of ALBERT share the same set of feed-forward and self-attention parameters. Therefore, we can see ALBERT as an iterated function &lt;span class=&#34;math inline&#34;&gt;\(f_A^n: h \rightarrow h&amp;#39;\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; is the number of encoder layers present in the model (in this study &lt;span class=&#34;math inline&#34;&gt;\(n=12\)&lt;/span&gt;), with parameters trained using end-to-end stochastic gradient descent.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Both factors significantly contribute to reducing the computational complexity of the model without affecting too much its performances: the ALBERT base used in all experimental chapters of this study have 9x fewer parameters than a regular BERT base model (12M vs. 108M) while performing comparably well on many natural language understanding benchmarks such as GLUE &lt;span class=&#34;citation&#34;&gt;(Wang et al. &lt;a href=&#34;#ref-wang-etal-2018-glue&#34;&gt;2018&lt;/a&gt;)&lt;/span&gt; and SQuAD &lt;span class=&#34;citation&#34;&gt;(Rajpurkar et al. &lt;a href=&#34;#ref-rajpurkar-etal-2016-squad&#34;&gt;2016&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Figure &lt;a href=&#34;chap-models.html#fig:albert&#34;&gt;2.3&lt;/a&gt; presents how a pretrained ALBERT model can be leveraged for sentence classification, using the ARA task as an example. We note that the procedure is the same as for GPT-2: a task-specific classification head is initialized with random weights, and the whole model-head architecture is fine-tuned on the target task end-to-end. The figure also shows how the common choice for BERT-based models is to use their &lt;code&gt;[CLS]&lt;/code&gt; token &lt;span class=&#34;math inline&#34;&gt;\(h_{12}^{1}\)&lt;/span&gt; as the full-sentence representation equivalent &lt;span class=&#34;math inline&#34;&gt;\(h_{12}^{sent}\)&lt;/span&gt;.&lt;/p&gt;

&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:albert&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;figures/2_albert.png&#34; alt=&#34;Using a pretrained ALBERT model for the ARA task. Adapted from Alammar (2018a).&#34; width=&#34;85%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 2.3: Using a pretrained ALBERT model for the ARA task. Adapted from &lt;span class=&#34;citation&#34;&gt;Alammar (&lt;a href=&#34;#ref-alammar-2018-illustratedbert&#34;&gt;2018&lt;/a&gt;&lt;a href=&#34;#ref-alammar-2018-illustratedbert&#34;&gt;a&lt;/a&gt;)&lt;/span&gt;.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;To conclude, the fine-tuning approach relying on a pretrained model “body” and a task-specific head adopted in both GPT-2 and ALBERT can be extended out-of-the-box to a &lt;strong&gt;multitask learning&lt;/strong&gt; scenario. A multitask approach can prove useful when considering parallel annotations on the same corpus that provide similar but complementary information about a studied phenomenon’s nature. We can interpret this as an inductive bias that encourages finding knowledge representations to explain multiple sets of annotations at once.&lt;a href=&#34;#fn11&#34; class=&#34;footnote-ref&#34; id=&#34;fnref11&#34;&gt;&lt;sup&gt;11&lt;/sup&gt;&lt;/a&gt; More specifically, multitask learning with &lt;strong&gt;hard parameter sharing&lt;/strong&gt; &lt;span class=&#34;citation&#34;&gt;(Caruana &lt;a href=&#34;#ref-caruana-1997-multitask&#34;&gt;1997&lt;/a&gt;)&lt;/span&gt; is performed in all experimental sections over eye-tracking scores to produce representations encompassing the whole set of phenomena related to natural reading. For doing so, each metric was associated with a task-specific head, and the whole set of heads was trained while sharing the same underlying model.&lt;/p&gt;
&lt;div id=&#34;subsubchap:syntax-nlm&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.2.1&lt;/span&gt; Emergent Linguistic Structures in Neural Language Models&lt;/h3&gt;
&lt;p&gt;This section presents evidence in support of the ability of pretrained language models to effectively encode language-related properties in their learned representations.&lt;a href=&#34;#fn12&#34; class=&#34;footnote-ref&#34; id=&#34;fnref12&#34;&gt;&lt;sup&gt;12&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;citation&#34;&gt;Lin, Tan, and Frank (&lt;a href=&#34;#ref-lin-etal-2019-open&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt; were among the first to highlight how BERT representations encode hierarchical structures akin to syntax trees, despite the absence of syntactic information or recurrent biases during pretraining. &lt;span class=&#34;citation&#34;&gt;Liu et al. (&lt;a href=&#34;#ref-liu-etal-2019-linguistic&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt; and &lt;span class=&#34;citation&#34;&gt;Tenney, Das, and Pavlick (&lt;a href=&#34;#ref-tenney-etal-2019-bert&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt; further showed that contextualized embeddings produced by BERT encode information about part-of-speech, entity roles, and partial syntactic structures.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;citation&#34;&gt;Hewitt and Manning (&lt;a href=&#34;#ref-hewitt-manning-2019-structural&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt; formulate the &lt;strong&gt;syntax distance hypothesis&lt;/strong&gt;, assuming that there exists a linear transformation &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; of the word representation space under which vector distance encodes parse trees. They proceed to test this assumption equating L2 distance in the 2-dimensional space of representations projected by &lt;span class=&#34;math inline&#34;&gt;\(B \in \mathbb{R}^{2 \times |h|}\)&lt;/span&gt; and tree distances in parse trees, finding a close match between BERT representational space and Penn Treebank formalisms. The approach is visualized in Figure &lt;a href=&#34;chap-models.html#fig:struct-probe&#34;&gt;2.4&lt;/a&gt;. &lt;span class=&#34;citation&#34;&gt;Jawahar, Sagot, and Seddah (&lt;a href=&#34;#ref-jawahar-etal-2019-bert&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt; work support these findings, highlighting a close match between BERT representation and dependency trees after testing multiple decomposition schemes. The syntax distance hypothesis’s validity is especially relevant to this work, given the aforementioned importance of syntactic properties in driving human subjects’ perception of complexity.&lt;/p&gt;

&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:struct-probe&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;figures/2_struct_probe.png&#34; alt=&#34;The mapping from 2D representation space to syntax tree distances adopted in Hewitt and Manning (2019).&#34; width=&#34;85%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 2.4: The mapping from 2D representation space to syntax tree distances adopted in &lt;span class=&#34;citation&#34;&gt;Hewitt and Manning (&lt;a href=&#34;#ref-hewitt-manning-2019-structural&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt;.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Despite the evidence of syntactic knowledge in contextual word representations, recent results suggest that the model may not leverage this for its predictions. &lt;span class=&#34;citation&#34;&gt;Ettinger (&lt;a href=&#34;#ref-ettinger-2020-bert&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt; highlights the insensitivity of BERT to negation and malformed inputs using psycholinguistic diagnostics commonly used with human subjects, while &lt;span class=&#34;citation&#34;&gt;Wallace et al. (&lt;a href=&#34;#ref-wallace-etal-2019-nlp&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt; show that nonsensical inputs do not affect the prediction quality of BERT, despite having a clear input on underlying syntactic structures. These results are coherent with the experimental findings of this study and will be further discussed in later sections.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;subchap:analyzing-nlm&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.3&lt;/span&gt; Analyzing Neural Models of Complexity&lt;/h2&gt;
&lt;p&gt;Having introduced the model architectures that will be used throughout this study, we will now focus on the interpretability approaches allowing us to analyze and compare neural network representations.&lt;/p&gt;
&lt;p&gt;When training deep neural networks, we would like to go beyond predictive performance and understand how different design choices and training objectives affect learned representations from a qualitative viewpoint. This fact is especially crucial in the model-driven approach adopted in this work, as stated at the end of Section &lt;a href=&#34;chap-models.html#subchap:desiderata&#34;&gt;2.1&lt;/a&gt;. While for linear models, the direct correspondence between the magnitude of feature coefficients and feature importance provides us with some out-of-the-box insights about decision boundaries and feature importance, the hierarchical and nonlinear structure that characterizes neural networks produce model weights that are relatively uninformative when taken in isolation.&lt;/p&gt;
&lt;p&gt;This work focuses on two interpretability perspectives: highlighting linguistic knowledge encoded in model representations (Chapter &lt;a href=&#34;chap-ex1.html#chap:ex1&#34;&gt;3&lt;/a&gt;) and comparing representations across models trained on different complexity-related tasks (Chapter &lt;a href=&#34;chap-ex2.html#chap:ex2&#34;&gt;4&lt;/a&gt;). For the first objective, &lt;em&gt;probing classifiers&lt;/em&gt;, which have become the de-facto standard in the interpretability literature, are used to evaluate the amount of information encoded in each layer of the model.&lt;a href=&#34;#fn13&#34; class=&#34;footnote-ref&#34; id=&#34;fnref13&#34;&gt;&lt;sup&gt;13&lt;/sup&gt;&lt;/a&gt; In the second case, two multivariate statistical analysis methods, namely &lt;em&gt;representational similarity analysis&lt;/em&gt; and &lt;em&gt;canonical correlation analysis&lt;/em&gt;, are leveraged to quantify the relation between model embeddings by evaluating their second-order similarity and learning a mapping to a shared low-dimensional space, respectively. The following sections conclude the chapter by presenting the three approaches in detail.&lt;/p&gt;
&lt;div id=&#34;subsubchap:probe&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.3.1&lt;/span&gt; Probing classifiers&lt;/h3&gt;
&lt;p&gt;The &lt;strong&gt;probing task approach&lt;/strong&gt; is a natural way to estimate the mutual information shared by a neural network’s parameters and some latent property that the model could have implicitly learned during training. During probing experiments, a supervised model (&lt;em&gt;probe&lt;/em&gt;) is trained to predict the latent information from the network’s learned representations. If the probe does well, we may conclude that the network effectively encodes some knowledge related to the selected property.&lt;/p&gt;
&lt;p&gt;Formally speaking, let &lt;span class=&#34;math inline&#34;&gt;\(f: x_i \rightarrow y_i\)&lt;/span&gt; be a neural network model mapping a corpus of input sentences &lt;span class=&#34;math inline&#34;&gt;\(X = (x_1, \dots, x_n)\)&lt;/span&gt; to a set of outputs &lt;span class=&#34;math inline&#34;&gt;\(Y = (y_1, \dots, y_n)\)&lt;/span&gt;. Assume that each sentence &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; is also labeled with some linguistic annotations &lt;span class=&#34;math inline&#34;&gt;\(z_i\)&lt;/span&gt;, reflecting the underlying properties we aim to detect. Let also &lt;span class=&#34;math inline&#34;&gt;\(h_l(x_i)\)&lt;/span&gt; be the network’s output at the &lt;span class=&#34;math inline&#34;&gt;\(l\)&lt;/span&gt;-th layer given the sentence &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; as input. To estimate the quality of representations &lt;span class=&#34;math inline&#34;&gt;\(h_l\)&lt;/span&gt; with respect to property &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt;, a supervised model &lt;span class=&#34;math inline&#34;&gt;\(g: h_l(x_i) \rightarrow z_i\)&lt;/span&gt; mapping representations to property values is trained. We take such model’s performances as a proxy of &lt;span class=&#34;math inline&#34;&gt;\(H(h_l(x),z)\)&lt;/span&gt;. In information theoretic terms, the probe is trained to minimize entropy &lt;span class=&#34;math inline&#34;&gt;\(H(z|h_l(x))\)&lt;/span&gt;, and by doing that it maximizes mutual information between the two quantities.&lt;/p&gt;
&lt;p&gt;The probe &lt;span class=&#34;math inline&#34;&gt;\(g\)&lt;/span&gt; does not need to be a linear model. While historically simple linear probes were used to minimize the risk of memorization, recent results show that more complex probes produce tighter estimates for the actual underlying information &lt;span class=&#34;citation&#34;&gt;(Pimentel et al. &lt;a href=&#34;#ref-pimentel-etal-2020-information&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt;. To account for the probe’s ability to learn the task through sheer memorization, &lt;span class=&#34;citation&#34;&gt;Hewitt and Liang (&lt;a href=&#34;#ref-hewitt-liang-2019-designing&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt; introduce &lt;em&gt;control tasks&lt;/em&gt; using the performances of a probe exposed to random labels as baselines.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;citation&#34;&gt;Alain and Bengio (&lt;a href=&#34;#ref-alain-bengio-2016-understanding&#34;&gt;2016&lt;/a&gt;)&lt;/span&gt; were among the first to use linear probing classifiers as tools to evaluate the presence of task-specific information inside neural networks’ layers. The approach was later extended to the field of NLP by &lt;span class=&#34;citation&#34;&gt;Conneau et al. (&lt;a href=&#34;#ref-conneau-etal-2018-cram&#34;&gt;2018&lt;/a&gt;)&lt;/span&gt; and &lt;span class=&#34;citation&#34;&gt;Zhang and Bowman (&lt;a href=&#34;#ref-zhang-bowman-2018-language&#34;&gt;2018&lt;/a&gt;)&lt;/span&gt; &lt;em&gt;inter alia&lt;/em&gt;, which evaluated the presence of semantic and syntactic information inside sentence embeddings generated by LSTM encoders &lt;span class=&#34;citation&#34;&gt;(Hochreiter and Schmidhuber &lt;a href=&#34;#ref-hochreiter-1997-long&#34;&gt;1997&lt;/a&gt;)&lt;/span&gt; pretrained on different objectives using probing task suites. Recently, &lt;span class=&#34;citation&#34;&gt;Miaschi and Dell’Orletta (&lt;a href=&#34;#ref-miaschi-dellorletta-2020-contextual&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt; showed how contextual representations produced by pretrained Transformer models could encode sentence-level properties within single-word embeddings. Moreover, &lt;span class=&#34;citation&#34;&gt;Miaschi et al. (&lt;a href=&#34;#ref-miaschi-etal-2020-linguistic&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt; highlighted the tendency of pretrained NLMs to lose general linguistic information during the fine-tuning process and found a positive relation between encoded linguistic information and the downstream performances of the model.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;subsubchap:rsa&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.3.2&lt;/span&gt; Representational Similarity Analysis&lt;/h3&gt;

&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:rsa&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;figures/2_rsa.png&#34; alt=&#34;The Representational Similarity Analysis (RSA) algorithm applied to the representations of three models. Image taken from Abnar (2020).&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 2.5: The Representational Similarity Analysis (RSA) algorithm applied to the representations of three models. Image taken from &lt;span class=&#34;citation&#34;&gt;Abnar (&lt;a href=&#34;#ref-abnar-2020-visualization&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt;.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Representational similarity analysis&lt;/strong&gt; (RSA, &lt;span class=&#34;citation&#34;&gt;Laakso and Cottrell (&lt;a href=&#34;#ref-laakso-2000-content&#34;&gt;2000&lt;/a&gt;)&lt;/span&gt;) is a technique developed in the field of cognitive science to evaluate the similarity of fMRI responses in selected regions of the brain after a stimulus &lt;span class=&#34;citation&#34;&gt;(Kriegeskorte, Mur, and Bandettini &lt;a href=&#34;#ref-kriegeskorte-etal-2008-representational&#34;&gt;2008&lt;/a&gt;)&lt;/span&gt;. The technique can be extended to compare the heterogeneous representational spaces formed by a set of computational models &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; exposed to a shared set of observations. Figure &lt;a href=&#34;chap-models.html#fig:rsa&#34;&gt;2.5&lt;/a&gt; visualizes the approach. First, each model is fed with a shared corpus of &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; sentences to produce a set of matrix embeddings &lt;span class=&#34;math inline&#34;&gt;\((E^1, \dots, E^m)\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(E^i_j\)&lt;/span&gt; represents the embedding produced by the last layer of the &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;-th model on the &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;-th sentence of the corpus.&lt;a href=&#34;#fn14&#34; class=&#34;footnote-ref&#34; id=&#34;fnref14&#34;&gt;&lt;sup&gt;14&lt;/sup&gt;&lt;/a&gt; Next, for each matrix &lt;span class=&#34;math inline&#34;&gt;\(E^i\)&lt;/span&gt; a representational distance matrix &lt;span class=&#34;math inline&#34;&gt;\(S^i\)&lt;/span&gt; is produced such that &lt;span class=&#34;math inline&#34;&gt;\(S^i_{j,k} = \text{sim}(E^i_j, E^i_k),\;S^i \in \mathbb{R}^{n \times n}\)&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(\text{sim}_1\)&lt;/span&gt; is a similarity function (here, &lt;em&gt;dot product&lt;/em&gt;). &lt;span class=&#34;math inline&#34;&gt;\(S_i\)&lt;/span&gt; encodes information on the similarity subsisting between model activations across different observations. Finally, a second-level &lt;em&gt;representational similarity matrix&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(S&amp;#39;\)&lt;/span&gt; is computed, where for each pair of matrices &lt;span class=&#34;math inline&#34;&gt;\((S^i, S^j)\)&lt;/span&gt; the corresponding &lt;span class=&#34;math inline&#34;&gt;\(S&amp;#39;_{i,j}\)&lt;/span&gt; entry has value:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
S&amp;#39;_{i,j} = S&amp;#39;_{j,i} = \frac{1}{n}\sum_{k=1}^n \text{sim}_2\big(\,\eta\,(S^i_k),\eta\,(S^j_k)\big)
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\eta\)&lt;/span&gt; is the L1 normalization function and &lt;span class=&#34;math inline&#34;&gt;\(\text{sim}_2\)&lt;/span&gt; is a similarity function (here, &lt;em&gt;Pearson’s correlation coefficient&lt;/em&gt;). Each entry &lt;span class=&#34;math inline&#34;&gt;\(S&amp;#39;_{i,j}\)&lt;/span&gt; corresponds to a similarity score between activity patterns of model &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; and model &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; across the entire set of &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; observations.&lt;/p&gt;
&lt;p&gt;In the context of NLP, &lt;span class=&#34;citation&#34;&gt;Abnar et al. (&lt;a href=&#34;#ref-abnar-etal-2019-blackbox&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt; recently used RSA to compare the activations of multiple neural language models and evaluated the impact of parameter values on the representations formed by a single model. Interestingly, they also use RSA to compare fMRI imaging data collected from human subjects and NLMs activations. &lt;span class=&#34;citation&#34;&gt;Abdou et al. (&lt;a href=&#34;#ref-abdou-etal-2019-higher&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt; use RSA to highlight the connection between processing difficulties (measured by high gaze metrics values) and the representational divergence, both inter and intra-encoder. &lt;span class=&#34;citation&#34;&gt;Abnar, Dehghani, and Zuidema (&lt;a href=&#34;#ref-abnar-etal-2020-transferring&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt; visualize training paths of various neural network architectures as 2D projections of RSA and show how different inductive biases can be transferred across network categories using knowledge distillation &lt;span class=&#34;citation&#34;&gt;(Hinton, Vinyals, and Dean &lt;a href=&#34;#ref-hinton-etal-2015-distilling&#34;&gt;2015&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;subsubchap:pwcca&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.3.3&lt;/span&gt; Projection-Weighted Canonical Correlation Analysis&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Canonical correlation analysis&lt;/strong&gt; (CCA, &lt;span class=&#34;citation&#34;&gt;Thompson (&lt;a href=&#34;#ref-thompson-1984-canonical&#34;&gt;1984&lt;/a&gt;)&lt;/span&gt;) is a statistical technique for relating two sets of observations arising from an underlying unknown process. In the context of this work, the underlying process is represented by NLMs being trained on complexity-related tasks. Given a corpus of sentences &lt;span class=&#34;math inline&#34;&gt;\(X = (x_1, \dots, x_m)\)&lt;/span&gt; annotated with complexity labels, we have that &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol z^l_ = (z_i^l(x_1), \dots z_i^l(x_m))\)&lt;/span&gt; corresponds to all activations of neuron &lt;span class=&#34;math inline&#34;&gt;\(z_i\)&lt;/span&gt; at layer &lt;span class=&#34;math inline&#34;&gt;\(l\)&lt;/span&gt; stacked to form a vector.&lt;a href=&#34;#fn15&#34; class=&#34;footnote-ref&#34; id=&#34;fnref15&#34;&gt;&lt;sup&gt;15&lt;/sup&gt;&lt;/a&gt; If we consider all activations of all neurons in a layer &lt;span class=&#34;math inline&#34;&gt;\(L_i = (z^i_1, \dots, z^i_n)\)&lt;/span&gt; for all inputs, we can represent them as a matrix &lt;span class=&#34;math inline&#34;&gt;\(A_i \in \mathbb{R}^{m \times n}\)&lt;/span&gt;, i.e. a set of multidimensional variates where &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; is the number of neurons in the layer. The CCA algorithm aims to &lt;em&gt;identify the best&lt;/em&gt; (i.e. most correlated) &lt;em&gt;linear relationship under mutual orthogonality and norm constraints between two sets of multidimensional variates&lt;/em&gt;, which in this case are activation matrices like &lt;span class=&#34;math inline&#34;&gt;\(L_1\)&lt;/span&gt;. This approach was used, among other things, to study the coherence between modeled and real brain activations &lt;span class=&#34;citation&#34;&gt;(Sussillo et al. &lt;a href=&#34;#ref-sussillo-etal-2015-neural&#34;&gt;2015&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Formally, if we have two activation matrices &lt;span class=&#34;math inline&#34;&gt;\(A_1, A_2 \in \mathbb{R}^{m \times n}\)&lt;/span&gt; we aim to find vectors &lt;span class=&#34;math inline&#34;&gt;\(w, v \in \mathbb{R}^m\)&lt;/span&gt; such that the correlation:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
\rho = \frac{\langle w^TA_1, v^TA_2 \rangle}{\|w^TA_1\| \cdot \| v^T A_2\|}
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;is maximized. The formula can be solved by changing the basis and recurring to singular value decomposition. The output of CCA is a set of singular pairwise orthogonal vectors &lt;span class=&#34;math inline&#34;&gt;\(u, v\)&lt;/span&gt; and their canonical correlation coefficients &lt;span class=&#34;math inline&#34;&gt;\(\rho \in [0,1]\)&lt;/span&gt; representing the correlation of vectors &lt;span class=&#34;math inline&#34;&gt;\(w^TA_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(v^TA_2\)&lt;/span&gt;.&lt;/p&gt;

&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:pwcca&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;figures/2_pwcca.png&#34; alt=&#34;Projection-Weighted Canonical Correlation Analysis (PWCCA) applied to last-layer representations of two language models.&#34; width=&#34;85%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 2.6: Projection-Weighted Canonical Correlation Analysis (PWCCA) applied to last-layer representations of two language models.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The SVCCA method &lt;span class=&#34;citation&#34;&gt;(Raghu et al. &lt;a href=&#34;#ref-guyon-etal-2017-svcca&#34;&gt;2017&lt;/a&gt;)&lt;/span&gt; extends the CCA approach for deep learning research by pruning neurons through a singular value decomposition step before computing canonical correlation coefficients. As the authors mention, “This is especially important in neural network representations, where as we will show many low variance directions (neurons) are primarily noise”. Then, the similarity between two layers &lt;span class=&#34;math inline&#34;&gt;\(L_1, L_2\)&lt;/span&gt; is computed as the mean correlation coefficient produce by SVCCA, and adapted to a distance measure for evaluation:
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
d_{\text{SVCCA}}(A_1, A_2) = 1 - \frac{1}{|\rho|} \sum_{i=1}^{|\rho|} \rho^{(i)}
\end{equation}\]&lt;/span&gt;
&lt;span class=&#34;citation&#34;&gt;Morcos, Raghu, and Bengio (&lt;a href=&#34;#ref-morcos-etal-2018-insights&#34;&gt;2018&lt;/a&gt;)&lt;/span&gt; suggest that the equal importance given to all the &lt;span class=&#34;math inline&#34;&gt;\(|\rho|\)&lt;/span&gt; SVCCA vectors during the final averaging step may be problematic since it has been extensively shown that overparametrized neural networks often do not recur to their full dimensionality for representing solutions &lt;span class=&#34;citation&#34;&gt;(Frankle and Carbin &lt;a href=&#34;#ref-frankle-carbin-2018-lottery&#34;&gt;2018&lt;/a&gt;)&lt;/span&gt;. They suggest replacing the mean with a weighted mean:
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
d_{\text{PWCCA}}(A_1, A_2) = 1 - \sum_{i=1}^{|\rho|} \alpha \rho^{(i)} \;\;\text{with} \;\; \tilde \alpha_i = \sum_j |\langle h_i, x_j \rangle|
\end{equation}\]&lt;/span&gt;
where weights &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; corresponds to the portion of inputs &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; accounted for by CCA vectors &lt;span class=&#34;math inline&#34;&gt;\(h\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\tilde \alpha_i\)&lt;/span&gt; values are normalized such that &lt;span class=&#34;math inline&#34;&gt;\(\sum_i \alpha_i = 1\)&lt;/span&gt;. The resulting approach, &lt;em&gt;projection-weighted canonical correlation analysis&lt;/em&gt; (PWCCA), is used in this study and was shown to be much more robust than SVCCA to filter noise in activations. Figure &lt;a href=&#34;chap-models.html#fig:pwcca&#34;&gt;2.6&lt;/a&gt; visualizes the selected approach.&lt;/p&gt;
&lt;p&gt;Notable applications of CCA-related methods in NLP are &lt;span class=&#34;citation&#34;&gt;Saphra and Lopez (&lt;a href=&#34;#ref-saphra-lopez-2019-understanding&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt;, where SVCCA is used to study the evolution of LSTM language models’ representations during training, and &lt;span class=&#34;citation&#34;&gt;Voita, Sennrich, and Titov (&lt;a href=&#34;#ref-voita-etal-2019-bottom&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt;, where PWCCA is used to compare Transformer language models across layers and pretraining objectives.&lt;/p&gt;


&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;h3&gt;References&lt;/h3&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-abdou-etal-2019-higher&#34;&gt;
&lt;p&gt;Abdou, Mostafa, Artur Kulmizev, Felix Hill, Daniel M. Low, and Anders Søgaard. 2019. “Higher-Order Comparisons of Sentence Encoder Representations.” In &lt;em&gt;Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (Emnlp-Ijcnlp)&lt;/em&gt;, 5838–45. Hong Kong, China: Association for Computational Linguistics. &lt;a href=&#34;https://doi.org/10.18653/v1/D19-1593&#34;&gt;https://doi.org/10.18653/v1/D19-1593&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-abnar-2020-visualization&#34;&gt;
&lt;p&gt;Abnar, Samira. 2020. “Visualizing Model Comparison.” &lt;em&gt;Blog Post&lt;/em&gt;. &lt;a href=&#34;https://samiraabnar.github.io/articles/2020-05/vizualization&#34;&gt;https://samiraabnar.github.io/articles/2020-05/vizualization&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-abnar-etal-2019-blackbox&#34;&gt;
&lt;p&gt;Abnar, Samira, Lisa Beinborn, Rochelle Choenni, and Willem Zuidema. 2019. “Blackbox Meets Blackbox: Representational Similarity &amp;amp; Stability Analysis of Neural Language Models and Brains.” In &lt;em&gt;Proceedings of the 2019 Acl Workshop Blackboxnlp: Analyzing and Interpreting Neural Networks for Nlp&lt;/em&gt;, 191–203. Florence, Italy: Association for Computational Linguistics. &lt;a href=&#34;https://doi.org/10.18653/v1/W19-4820&#34;&gt;https://doi.org/10.18653/v1/W19-4820&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-abnar-etal-2020-transferring&#34;&gt;
&lt;p&gt;Abnar, Samira, Mostafa Dehghani, and Willem Zuidema. 2020. “Transferring Inductive Biases Through Knowledge Distillation.” &lt;em&gt;ArXiv Pre-Print&lt;/em&gt; 2006.00555. &lt;a href=&#34;https://arxiv.org/abs/2006.00555&#34;&gt;https://arxiv.org/abs/2006.00555&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-alain-bengio-2016-understanding&#34;&gt;
&lt;p&gt;Alain, Guillaume, and Yoshua Bengio. 2016. “Understanding Intermediate Layers Using Linear Classifier Probes.” &lt;em&gt;ArXiv Pre-Print&lt;/em&gt; 1610.01644. &lt;a href=&#34;https://arxiv.org/abs/1610.01644&#34;&gt;https://arxiv.org/abs/1610.01644&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-alammar-2018-illustratedbert&#34;&gt;
&lt;p&gt;Alammar, Jay. 2018a. “The Illustrated Bert, Elmo, and Co. (How NLP Cracked Transfer Learning).” &lt;em&gt;Blog Post&lt;/em&gt;. &lt;a href=&#34;https://jalammar.github.io/illustrated-bert/&#34;&gt;https://jalammar.github.io/illustrated-bert/&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-alammar-2018-illustratedgpt2&#34;&gt;
&lt;p&gt;Alammar, Jay. 2018b. “The Illustrated Gpt-2.” &lt;em&gt;Blog Post&lt;/em&gt;. &lt;a href=&#34;https://http://jalammar.github.io/illustrated-gpt2/&#34;&gt;https://http://jalammar.github.io/illustrated-gpt2/&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-andreas-klein-2014-much&#34;&gt;
&lt;p&gt;Andreas, Jacob, and Dan Klein. 2014. “How Much Do Word Embeddings Encode About Syntax?” In &lt;em&gt;Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)&lt;/em&gt;, 822–27. Baltimore, Maryland: Association for Computational Linguistics. &lt;a href=&#34;https://doi.org/10.3115/v1/P14-2133&#34;&gt;https://doi.org/10.3115/v1/P14-2133&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-bahdanau-etal-2015-neural&#34;&gt;
&lt;p&gt;Bahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio. 2015. “Neural Machine Translation by Jointly Learning to Align and Translate.” In &lt;em&gt;Proceeding of the 3rd International Conference on Learning Representations (ICLR’15)&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-caruana-1997-multitask&#34;&gt;
&lt;p&gt;Caruana, Rich. 1997. “Multitask Learning.” &lt;em&gt;Machine Learning&lt;/em&gt; 28: 41–75. &lt;a href=&#34;https://www.cs.utexas.edu/~kuipers/readings/Caruana-mlj-97.pdf&#34;&gt;https://www.cs.utexas.edu/~kuipers/readings/Caruana-mlj-97.pdf&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-conneau-etal-2018-cram&#34;&gt;
&lt;p&gt;Conneau, Alexis, German Kruszewski, Guillaume Lample, Loı̈c Barrault, and Marco Baroni. 2018. “What You Can Cram into a Single $&amp;amp;!#* Vector: Probing Sentence Embeddings for Linguistic Properties.” In &lt;em&gt;Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)&lt;/em&gt;, 2126–36. Melbourne, Australia: Association for Computational Linguistics. &lt;a href=&#34;https://doi.org/10.18653/v1/P18-1198&#34;&gt;https://doi.org/10.18653/v1/P18-1198&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-devlin-etal-2019-bert&#34;&gt;
&lt;p&gt;Devlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. “BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding.” In &lt;em&gt;Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)&lt;/em&gt;, 4171–86. Minneapolis, Minnesota: Association for Computational Linguistics. &lt;a href=&#34;https://doi.org/10.18653/v1/N19-1423&#34;&gt;https://doi.org/10.18653/v1/N19-1423&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-ettinger-2020-bert&#34;&gt;
&lt;p&gt;Ettinger, Allyson. 2020. “What BERT Is Not: Lessons from a New Suite of Psycholinguistic Diagnostics for Language Models.” &lt;em&gt;Transactions of the Association for Computational Linguistics&lt;/em&gt; 8: 34–48. &lt;a href=&#34;https://doi.org/10.1162/tacl_a_00298&#34;&gt;https://doi.org/10.1162/tacl_a_00298&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-frankle-carbin-2018-lottery&#34;&gt;
&lt;p&gt;Frankle, Jonathan, and Michael Carbin. 2018. “The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks.” In &lt;em&gt;Proceedings of the 8th International Conference on Learning Representations (Iclr’18)&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-goodfellow-etal-2016-deep&#34;&gt;
&lt;p&gt;Goodfellow, Ian, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. 2016. &lt;em&gt;Deep Learning&lt;/em&gt;. MIT Press Cambridge.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-hendrycks-gimpel-2016-gaussian&#34;&gt;
&lt;p&gt;Hendrycks, Dan, and Kevin Gimpel. 2016. “Gaussian Error Linear Units (Gelus).” &lt;em&gt;ArXiv Pre-Print&lt;/em&gt; 1606.08415. &lt;a href=&#34;https://arxiv.org/abs/1606.08415&#34;&gt;https://arxiv.org/abs/1606.08415&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-hewitt-liang-2019-designing&#34;&gt;
&lt;p&gt;Hewitt, John, and Percy Liang. 2019. “Designing and Interpreting Probes with Control Tasks.” In &lt;em&gt;Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (Emnlp-Ijcnlp)&lt;/em&gt;, 2733–43. Hong Kong, China: Association for Computational Linguistics. &lt;a href=&#34;https://doi.org/10.18653/v1/D19-1275&#34;&gt;https://doi.org/10.18653/v1/D19-1275&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-hewitt-manning-2019-structural&#34;&gt;
&lt;p&gt;Hewitt, John, and Christopher D. Manning. 2019. “A Structural Probe for Finding Syntax in Word Representations.” In &lt;em&gt;Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)&lt;/em&gt;, 4129–38. Minneapolis, Minnesota: Association for Computational Linguistics. &lt;a href=&#34;https://doi.org/10.18653/v1/N19-1419&#34;&gt;https://doi.org/10.18653/v1/N19-1419&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-hinton-etal-2015-distilling&#34;&gt;
&lt;p&gt;Hinton, Geoffrey, Oriol Vinyals, and Jeff Dean. 2015. “Distilling the Knowledge in a Neural Network.” &lt;em&gt;ArXiv Pre-Print&lt;/em&gt; 1503.02531. &lt;a href=&#34;https://arxiv.org/abs/1503.02531&#34;&gt;https://arxiv.org/abs/1503.02531&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-hochreiter-1997-long&#34;&gt;
&lt;p&gt;Hochreiter, Sepp, and Jürgen Schmidhuber. 1997. “Long Short-Term Memory.” &lt;em&gt;Neural Computation&lt;/em&gt; 9 (8). MIT Press: 1735–80.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-howard-ruder-2018-universal&#34;&gt;
&lt;p&gt;Howard, Jeremy, and Sebastian Ruder. 2018. “Universal Language Model Fine-Tuning for Text Classification.” In &lt;em&gt;Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)&lt;/em&gt;, 328–39. Melbourne, Australia: Association for Computational Linguistics. &lt;a href=&#34;https://doi.org/10.18653/v1/P18-1031&#34;&gt;https://doi.org/10.18653/v1/P18-1031&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-jawahar-etal-2019-bert&#34;&gt;
&lt;p&gt;Jawahar, Ganesh, Benoit Sagot, and Djamé Seddah. 2019. “What Does BERT Learn About the Structure of Language?” In &lt;em&gt;Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics&lt;/em&gt;, 3651–7. Florence, Italy: Association for Computational Linguistics. &lt;a href=&#34;https://doi.org/10.18653/v1/P19-1356&#34;&gt;https://doi.org/10.18653/v1/P19-1356&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-kriegeskorte-etal-2008-representational&#34;&gt;
&lt;p&gt;Kriegeskorte, N., M. Mur, and P. Bandettini. 2008. “Representational Similarity Analysis – Connecting the Branches of Systems Neuroscience.” &lt;em&gt;Frontiers in Systems Neuroscience&lt;/em&gt; 2. &lt;a href=&#34;https://doi.org/10.3389/neuro.06.004.2008&#34;&gt;https://doi.org/10.3389/neuro.06.004.2008&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-kudo-richardson-2018-sentencepiece&#34;&gt;
&lt;p&gt;Kudo, Taku, and John Richardson. 2018. “SentencePiece: A Simple and Language Independent Subword Tokenizer and Detokenizer for Neural Text Processing.” In &lt;em&gt;Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations&lt;/em&gt;, 66–71. Brussels, Belgium: Association for Computational Linguistics. &lt;a href=&#34;https://doi.org/10.18653/v1/D18-2012&#34;&gt;https://doi.org/10.18653/v1/D18-2012&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-laakso-2000-content&#34;&gt;
&lt;p&gt;Laakso, Aarre, and Garrison Cottrell. 2000. “Content and Cluster Analysis: Assessing Representational Similarity in Neural Systems.” &lt;em&gt;Philosophical Psychology&lt;/em&gt; 13 (1). Taylor &amp;amp; Francis: 47–76.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-lan-etal-2020-albert&#34;&gt;
&lt;p&gt;Lan, Zhenzhong, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. 2020. “ALBERT: A Lite BERT for Self-Supervised Learning of Language Representations.” In &lt;em&gt;International Conference on Learning Representations&lt;/em&gt;. &lt;a href=&#34;https://openreview.net/forum?id=H1eA7AEtvS&#34;&gt;https://openreview.net/forum?id=H1eA7AEtvS&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-lin-etal-2019-open&#34;&gt;
&lt;p&gt;Lin, Yongjie, Yi Chern Tan, and Robert Frank. 2019. “Open Sesame: Getting Inside BERT’s Linguistic Knowledge.” In &lt;em&gt;Proceedings of the 2019 Acl Workshop Blackboxnlp: Analyzing and Interpreting Neural Networks for Nlp&lt;/em&gt;, 241–53. Florence, Italy: Association for Computational Linguistics. &lt;a href=&#34;https://doi.org/10.18653/v1/W19-4825&#34;&gt;https://doi.org/10.18653/v1/W19-4825&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-liu-etal-2019-linguistic&#34;&gt;
&lt;p&gt;Liu, Nelson F., Matt Gardner, Yonatan Belinkov, Matthew E. Peters, and Noah A. Smith. 2019. “Linguistic Knowledge and Transferability of Contextual Representations.” In &lt;em&gt;Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)&lt;/em&gt;, 1073–94. Minneapolis, Minnesota: Association for Computational Linguistics. &lt;a href=&#34;https://doi.org/10.18653/v1/N19-1112&#34;&gt;https://doi.org/10.18653/v1/N19-1112&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-miaschi-etal-2020-linguistic&#34;&gt;
&lt;p&gt;Miaschi, Alessio, Dominique Brunato, Felice Dell’Orletta, and Giulia Venturi. 2020. “Linguistic Profiling of a Neural Language Model.” In &lt;em&gt;Proceedings of the 28th Conference on Computational Linguistics (Coling)&lt;/em&gt;. Online: Association for Computational Linguistics. &lt;a href=&#34;https://arxiv.org/abs/2010.01869&#34;&gt;https://arxiv.org/abs/2010.01869&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-miaschi-dellorletta-2020-contextual&#34;&gt;
&lt;p&gt;Miaschi, Alessio, and Felice Dell’Orletta. 2020. “Contextual and Non-Contextual Word Embeddings: An in-Depth Linguistic Investigation.” In &lt;em&gt;Proceedings of the 5th Workshop on Representation Learning for Nlp&lt;/em&gt;, 110–19. Online: Association for Computational Linguistics. &lt;a href=&#34;https://www.aclweb.org/anthology/2020.repl4nlp-1.15&#34;&gt;https://www.aclweb.org/anthology/2020.repl4nlp-1.15&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-mikolov-etal-2013-efficient&#34;&gt;
&lt;p&gt;Mikolov, Tomas, Kai Chen, G. S. Corrado, and J. Dean. 2013. “Efficient Estimation of Word Representations in Vector Space.” &lt;em&gt;CoRR&lt;/em&gt; abs/1301.3781.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-mikolov-etal-2010-recurrent&#34;&gt;
&lt;p&gt;Mikolov, Tomas, M. Karafiát, L. Burget, J. Cernocký, and S. Khudanpur. 2010. “Recurrent Neural Network Based Language Model.” In &lt;em&gt;INTERSPEECH&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-morcos-etal-2018-insights&#34;&gt;
&lt;p&gt;Morcos, Ari, Maithra Raghu, and Samy Bengio. 2018. “Insights on Representational Similarity in Neural Networks with Canonical Correlation.” In &lt;em&gt;Advances in Neural Information Processing Systems 31&lt;/em&gt;, edited by S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, 5727–36. Curran Associates, Inc. &lt;a href=&#34;http://papers.nips.cc/paper/7815-insights-on-representational-similarity-in-neural-networks-with-canonical-correlation.pdf&#34;&gt;http://papers.nips.cc/paper/7815-insights-on-representational-similarity-in-neural-networks-with-canonical-correlation.pdf&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-pascanu-etal-2013-difficulty&#34;&gt;
&lt;p&gt;Pascanu, R., Tomas Mikolov, and Yoshua Bengio. 2013. “On the Difficulty of Training Recurrent Neural Networks.” In &lt;em&gt;Proceedings of the 30th International Conference on Machine Learning (Icml’13)&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-pennington-etal-2014-glove&#34;&gt;
&lt;p&gt;Pennington, Jeffrey, Richard Socher, and Christopher Manning. 2014. “GloVe: Global Vectors for Word Representation.” In &lt;em&gt;Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)&lt;/em&gt;, 1532–43. Doha, Qatar: Association for Computational Linguistics. &lt;a href=&#34;https://doi.org/10.3115/v1/D14-1162&#34;&gt;https://doi.org/10.3115/v1/D14-1162&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-peters-etal-2018-deep&#34;&gt;
&lt;p&gt;Peters, Matthew, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. “Deep Contextualized Word Representations.” In &lt;em&gt;Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)&lt;/em&gt;, 2227–37. New Orleans, Louisiana: Association for Computational Linguistics. &lt;a href=&#34;https://doi.org/10.18653/v1/N18-1202&#34;&gt;https://doi.org/10.18653/v1/N18-1202&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-pimentel-etal-2020-information&#34;&gt;
&lt;p&gt;Pimentel, Tiago, Josef Valvoda, Rowan Hall Maudslay, Ran Zmigrod, Adina Williams, and Ryan Cotterell. 2020. “Information-Theoretic Probing for Linguistic Structure.” In &lt;em&gt;Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics&lt;/em&gt;, 4609–22. Online: Association for Computational Linguistics. &lt;a href=&#34;https://doi.org/10.18653/v1/2020.acl-main.420&#34;&gt;https://doi.org/10.18653/v1/2020.acl-main.420&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-radford-etal-2019-language&#34;&gt;
&lt;p&gt;Radford, A., Jeffrey Wu, R. Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. “Language Models Are Unsupervised Multitask Learners.” &lt;em&gt;OpenAI Blog&lt;/em&gt;. OpenAI.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-guyon-etal-2017-svcca&#34;&gt;
&lt;p&gt;Raghu, Maithra, Justin Gilmer, Jason Yosinski, and Jascha Sohl-Dickstein. 2017. “SVCCA: Singular Vector Canonical Correlation Analysis for Deep Learning Dynamics and Interpretability.” In &lt;em&gt;Advances in Neural Information Processing Systems 30&lt;/em&gt;, edited by I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, 6076–85. Curran Associates, Inc. &lt;a href=&#34;http://papers.nips.cc/paper/7188-svcca-singular-vector-canonical-correlation-analysis-for-deep-learning-dynamics-and-interpretability.pdf&#34;&gt;http://papers.nips.cc/paper/7188-svcca-singular-vector-canonical-correlation-analysis-for-deep-learning-dynamics-and-interpretability.pdf&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-rajpurkar-etal-2016-squad&#34;&gt;
&lt;p&gt;Rajpurkar, Pranav, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. “SQuAD: 100,000+ Questions for Machine Comprehension of Text.” In &lt;em&gt;Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing&lt;/em&gt;, 2383–92. Austin, Texas: Association for Computational Linguistics. &lt;a href=&#34;https://doi.org/10.18653/v1/D16-1264&#34;&gt;https://doi.org/10.18653/v1/D16-1264&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-samek-etal-2019-explainable&#34;&gt;
&lt;p&gt;Samek, W., Grégoire Montavon, A. Vedaldi, L. Hansen, and K. Müller. 2019. “Explainable Ai: Interpreting, Explaining and Visualizing Deep Learning.” &lt;em&gt;Explainable AI: Interpreting, Explaining and Visualizing Deep Learning&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-saphra-lopez-2019-understanding&#34;&gt;
&lt;p&gt;Saphra, Naomi, and Adam Lopez. 2019. “Understanding Learning Dynamics of Language Models with SVCCA.” In &lt;em&gt;Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)&lt;/em&gt;, 3257–67. Minneapolis, Minnesota: Association for Computational Linguistics. &lt;a href=&#34;https://doi.org/10.18653/v1/N19-1329&#34;&gt;https://doi.org/10.18653/v1/N19-1329&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-sennrich-etal-2016-neural&#34;&gt;
&lt;p&gt;Sennrich, Rico, Barry Haddow, and Alexandra Birch. 2016. “Neural Machine Translation of Rare Words with Subword Units.” In &lt;em&gt;Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)&lt;/em&gt;, 1715–25. Berlin, Germany: Association for Computational Linguistics. &lt;a href=&#34;https://doi.org/10.18653/v1/P16-1162&#34;&gt;https://doi.org/10.18653/v1/P16-1162&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-shwartz-tishby-2017-opening&#34;&gt;
&lt;p&gt;Shwartz-Ziv, Ravid, and Naftali Tishby. 2017. “Opening the Black Box of Deep Neural Networks via Information.” &lt;em&gt;ArXiv Pre-Print&lt;/em&gt; 1703.00810. &lt;a href=&#34;https://arxiv.org/abs/1703.00810&#34;&gt;https://arxiv.org/abs/1703.00810&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-sussillo-etal-2015-neural&#34;&gt;
&lt;p&gt;Sussillo, David, Mark M Churchland, Matthew T Kaufman, and Krishna V Shenoy. 2015. “A Neural Network That Finds a Naturalistic Solution for the Production of Muscle Activity.” &lt;em&gt;Nature Neuroscience&lt;/em&gt; 18 (7). Nature Publishing Group: 1025–33.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-tenney-etal-2019-bert&#34;&gt;
&lt;p&gt;Tenney, Ian, Dipanjan Das, and Ellie Pavlick. 2019. “BERT Rediscovers the Classical NLP Pipeline.” In &lt;em&gt;Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics&lt;/em&gt;, 4593–4601. Florence, Italy: Association for Computational Linguistics. &lt;a href=&#34;https://doi.org/10.18653/v1/P19-1452&#34;&gt;https://doi.org/10.18653/v1/P19-1452&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-thompson-1984-canonical&#34;&gt;
&lt;p&gt;Thompson, Bruce. 1984. &lt;em&gt;Canonical Correlation Analysis: Uses and Interpretation&lt;/em&gt;. 47. Sage.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-turian-etal-2010-word&#34;&gt;
&lt;p&gt;Turian, Joseph, Lev-Arie Ratinov, and Yoshua Bengio. 2010. “Word Representations: A Simple and General Method for Semi-Supervised Learning.” In &lt;em&gt;Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics&lt;/em&gt;, 384–94. Uppsala, Sweden: Association for Computational Linguistics. &lt;a href=&#34;https://www.aclweb.org/anthology/P10-1040&#34;&gt;https://www.aclweb.org/anthology/P10-1040&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-vaswani-etal-2017-attention&#34;&gt;
&lt;p&gt;Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. “Attention Is All You Need.” In &lt;em&gt;Advances in Neural Information Processing Systems 30&lt;/em&gt;, edited by I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, 5998–6008. Curran Associates, Inc. &lt;a href=&#34;http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf&#34;&gt;http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-voita-etal-2019-bottom&#34;&gt;
&lt;p&gt;Voita, Elena, Rico Sennrich, and Ivan Titov. 2019. “The Bottom-up Evolution of Representations in the Transformer: A Study with Machine Translation and Language Modeling Objectives.” In &lt;em&gt;Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (Emnlp-Ijcnlp)&lt;/em&gt;, 4396–4406. Hong Kong, China: Association for Computational Linguistics. &lt;a href=&#34;https://doi.org/10.18653/v1/D19-1448&#34;&gt;https://doi.org/10.18653/v1/D19-1448&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-wallace-etal-2019-nlp&#34;&gt;
&lt;p&gt;Wallace, Eric, Yizhong Wang, Sujian Li, Sameer Singh, and Matt Gardner. 2019. “Do NLP Models Know Numbers? Probing Numeracy in Embeddings.” In &lt;em&gt;Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (Emnlp-Ijcnlp)&lt;/em&gt;, 5307–15. Hong Kong, China: Association for Computational Linguistics. &lt;a href=&#34;https://doi.org/10.18653/v1/D19-1534&#34;&gt;https://doi.org/10.18653/v1/D19-1534&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-wang-etal-2018-glue&#34;&gt;
&lt;p&gt;Wang, Alex, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2018. “GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding.” In &lt;em&gt;Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP&lt;/em&gt;, 353–55. Brussels, Belgium: Association for Computational Linguistics. &lt;a href=&#34;https://doi.org/10.18653/v1/W18-5446&#34;&gt;https://doi.org/10.18653/v1/W18-5446&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-wu-etal-2016-google&#34;&gt;
&lt;p&gt;Wu, Y., Mike Schuster, Z. Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey, M. Krikun, et al. 2016. “Google’s Neural Machine Translation System: Bridging the Gap Between Human and Machine Translation.” &lt;em&gt;ArXiv Pre-Print&lt;/em&gt; 1609.08144. &lt;a href=&#34;https://arxiv.org/abs/1609.08144&#34;&gt;https://arxiv.org/abs/1609.08144&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-zhang-bowman-2018-language&#34;&gt;
&lt;p&gt;Zhang, Kelly, and Samuel Bowman. 2018. “Language Modeling Teaches You More Than Translation Does: Lessons Learned Through Auxiliary Syntactic Task Analysis.” In &lt;em&gt;Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP&lt;/em&gt;, 359–61. Brussels, Belgium: Association for Computational Linguistics. &lt;a href=&#34;https://doi.org/10.18653/v1/W18-5448&#34;&gt;https://doi.org/10.18653/v1/W18-5448&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol start=&#34;10&#34;&gt;
&lt;li id=&#34;fn10&#34;&gt;&lt;p&gt;Refer to Chapter 6.3 of &lt;span class=&#34;citation&#34;&gt;Eisenstein (&lt;a href=&#34;#ref-eisenstein-2019-introduction&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt; for additional details about recurrent language models.&lt;a href=&#34;chap-models.html#fnref10&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn11&#34;&gt;&lt;p&gt;See &lt;span class=&#34;citation&#34;&gt;Ruder (&lt;a href=&#34;#ref-ruder-2017-overview&#34;&gt;2017&lt;/a&gt;)&lt;/span&gt; for a comprehensive overview&lt;a href=&#34;chap-models.html#fnref11&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn12&#34;&gt;&lt;p&gt;&lt;span class=&#34;citation&#34;&gt;Rogers, Kovaleva, and Rumshisky (&lt;a href=&#34;#ref-rogers-etal-2020-primer&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt; and &lt;span class=&#34;citation&#34;&gt;Linzen and Baroni (&lt;a href=&#34;#ref-linzen-baroni-2021-syntactic&#34;&gt;2021&lt;/a&gt;)&lt;/span&gt; are surveys covering this topic.&lt;a href=&#34;chap-models.html#fnref12&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn13&#34;&gt;&lt;p&gt;See &lt;span class=&#34;citation&#34;&gt;Belinkov and Glass (&lt;a href=&#34;#ref-belinkov-glass-2019-analysis&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt; survey and &lt;span class=&#34;citation&#34;&gt;Belinkov, Gehrmann, and Pavlick (&lt;a href=&#34;#ref-belinkov-etal-2020-interpretability&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt; tutorial.&lt;a href=&#34;chap-models.html#fnref13&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn14&#34;&gt;&lt;p&gt;This can be any layer; embeddings can be produced by different layers of the same model.&lt;a href=&#34;chap-models.html#fnref14&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn15&#34;&gt;&lt;p&gt;Different from the activation vector, i.e. all neurons’ activations for a single input &lt;span class=&#34;math inline&#34;&gt;\((z^l_1(x_1),\dots,z^l_n(x_1))\)&lt;/span&gt;&lt;a href=&#34;chap-models.html#fnref15&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
            &lt;/section&gt;

          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
&lt;a href=&#34;chap-ling-comp.html&#34; class=&#34;navigation navigation-prev &#34; aria-label=&#34;Previous page&#34;&gt;&lt;i class=&#34;fa fa-angle-left&#34;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&#34;chap-ex1.html&#34; class=&#34;navigation navigation-next &#34; aria-label=&#34;Next page&#34;&gt;&lt;i class=&#34;fa fa-angle-right&#34;&gt;&lt;/i&gt;&lt;/a&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/app.min.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/lunr.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/clipboard.min.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/plugin-search.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/plugin-sharing.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/plugin-fontsettings.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/plugin-bookdown.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/jquery.highlight.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/plugin-clipboard.js&#34;&gt;&lt;/script&gt;
&lt;script&gt;
gitbook.require([&#34;gitbook&#34;], function(gitbook) {
gitbook.start({
&#34;sharing&#34;: {
&#34;github&#34;: true,
&#34;facebook&#34;: true,
&#34;twitter&#34;: true,
&#34;linkedin&#34;: true,
&#34;weibo&#34;: false,
&#34;instapaper&#34;: false,
&#34;vk&#34;: false,
&#34;all&#34;: false
},
&#34;fontsettings&#34;: {
&#34;theme&#34;: &#34;white&#34;,
&#34;family&#34;: &#34;sans&#34;,
&#34;size&#34;: 2
},
&#34;edit&#34;: {
&#34;link&#34;: &#34;https://github.com/gsarti/master-thesis/tree/master/02-Models.Rmd&#34;,
&#34;text&#34;: &#34;Edit&#34;
},
&#34;history&#34;: {
&#34;link&#34;: null,
&#34;text&#34;: null
},
&#34;view&#34;: {
&#34;link&#34;: null,
&#34;text&#34;: null
},
&#34;download&#34;: [[&#34;Sarti_2020_Interpreting_NLMs_for_LCA.pdf&#34;, &#34;PDF&#34;]],
&#34;toc&#34;: {
&#34;collapse&#34;: &#34;subsection&#34;,
&#34;scroll_highlight&#34;: true
},
&#34;info&#34;: false
});
});
&lt;/script&gt;

&lt;!-- dynamically load mathjax for compatibility with self-contained --&gt;
&lt;script&gt;
  (function () {
    var script = document.createElement(&#34;script&#34;);
    script.type = &#34;text/javascript&#34;;
    var src = &#34;true&#34;;
    if (src === &#34;&#34; || src === &#34;true&#34;) src = &#34;https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML&#34;;
    if (location.protocol !== &#34;file:&#34;)
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, &#39;&#39;);
    script.src = src;
    document.getElementsByTagName(&#34;head&#34;)[0].appendChild(script);
  })();
&lt;/script&gt;
&lt;/body&gt;

&lt;/html&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>http://localhost:1313/msc-thesis/conclusion/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/msc-thesis/conclusion/</guid>
      <description>&lt;!DOCTYPE html&gt;
&lt;html lang=&#34;&#34; xml:lang=&#34;&#34;&gt;
&lt;head&gt;

  &lt;meta charset=&#34;utf-8&#34; /&gt;
  &lt;meta http-equiv=&#34;X-UA-Compatible&#34; content=&#34;IE=edge&#34; /&gt;
  &lt;title&gt;Conclusion | Interpreting Neural Language Models for Linguistic Complexity Assessment&lt;/title&gt;
  &lt;meta name=&#34;description&#34; content=&#34;MSc Thesis at the University of Trieste and SISSA under the supervision of Prof. Davide Crepaldi and Dott. Felice dell&#39;Orletta&#34; /&gt;
  &lt;meta name=&#34;generator&#34; content=&#34;bookdown 0.20.6 and GitBook 2.6.7&#34; /&gt;

  &lt;meta property=&#34;og:title&#34; content=&#34;Conclusion | Interpreting Neural Language Models for Linguistic Complexity Assessment&#34; /&gt;
  &lt;meta property=&#34;og:type&#34; content=&#34;book&#34; /&gt;
  &lt;meta property=&#34;og:url&#34; content=&#34;https://gsarti.com/master-thesis&#34; /&gt;
  &lt;meta property=&#34;og:image&#34; content=&#34;https://gsarti.com/master-thesisfigures/cover.png&#34; /&gt;
  &lt;meta property=&#34;og:description&#34; content=&#34;MSc Thesis at the University of Trieste and SISSA under the supervision of Prof. Davide Crepaldi and Dott. Felice dell&#39;Orletta&#34; /&gt;
  &lt;meta name=&#34;github-repo&#34; content=&#34;gsarti/interpreting-complexity&#34; /&gt;

  &lt;meta name=&#34;twitter:card&#34; content=&#34;summary&#34; /&gt;
  &lt;meta name=&#34;twitter:title&#34; content=&#34;Conclusion | Interpreting Neural Language Models for Linguistic Complexity Assessment&#34; /&gt;
  
  &lt;meta name=&#34;twitter:description&#34; content=&#34;MSc Thesis at the University of Trieste and SISSA under the supervision of Prof. Davide Crepaldi and Dott. Felice dell&#39;Orletta&#34; /&gt;
  &lt;meta name=&#34;twitter:image&#34; content=&#34;https://gsarti.com/master-thesisfigures/cover.png&#34; /&gt;

&lt;meta name=&#34;author&#34; content=&#34;Gabriele Sarti&#34; /&gt;



  &lt;meta name=&#34;viewport&#34; content=&#34;width=device-width, initial-scale=1&#34; /&gt;
  &lt;meta name=&#34;apple-mobile-web-app-capable&#34; content=&#34;yes&#34; /&gt;
  &lt;meta name=&#34;apple-mobile-web-app-status-bar-style&#34; content=&#34;black&#34; /&gt;
  &lt;link rel=&#34;apple-touch-icon-precomposed&#34; sizes=&#34;152x152&#34; href=&#34;figures/icons/apple-icon.png&#34; /&gt;
  &lt;link rel=&#34;shortcut icon&#34; href=&#34;figures/icons/favicon.ico&#34; type=&#34;image/x-icon&#34; /&gt;
&lt;link rel=&#34;prev&#34; href=&#34;chap-ex3.html&#34;/&gt;
&lt;link rel=&#34;next&#34; href=&#34;app-ling-feats.html&#34;/&gt;
&lt;style type=&#34;text/css&#34;&gt;
p.abstract{
  text-align: center;
  font-weight: bold;
}
div.abstract{
  margin: auto;
  width: 90%;
}
&lt;/style&gt;
&lt;script src=&#34;libs/jquery-2.2.3/jquery.min.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;libs/gitbook-2.6.7/css/style.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;link href=&#34;libs/gitbook-2.6.7/css/plugin-table.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;link href=&#34;libs/gitbook-2.6.7/css/plugin-bookdown.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;link href=&#34;libs/gitbook-2.6.7/css/plugin-highlight.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;link href=&#34;libs/gitbook-2.6.7/css/plugin-search.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;link href=&#34;libs/gitbook-2.6.7/css/plugin-fontsettings.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;link href=&#34;libs/gitbook-2.6.7/css/plugin-clipboard.css&#34; rel=&#34;stylesheet&#34; /&gt;









&lt;script src=&#34;libs/kePrint-0.0.1/kePrint.js&#34;&gt;&lt;/script&gt;



&lt;link rel=&#34;stylesheet&#34; href=&#34;templates/style.css&#34; type=&#34;text/css&#34; /&gt;
&lt;/head&gt;

&lt;body&gt;



  &lt;div class=&#34;book without-animation with-summary font-size-2 font-family-1&#34; data-basepath=&#34;.&#34;&gt;

    &lt;div class=&#34;book-summary&#34;&gt;
      &lt;nav role=&#34;navigation&#34;&gt;

&lt;ul class=&#34;summary&#34;&gt;
&lt;li&gt;&lt;a href=&#34;introduction.html#introduction&#34;&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1&lt;/b&gt; &lt;strong&gt;Linguistic Complexity&lt;/strong&gt;&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.1&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subchap:categorizing&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.1&lt;/b&gt; Categorizing Linguistic Complexity Measures&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.2&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subchap:intrinsic&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.2&lt;/b&gt; Intrinsic Perspective&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.2.1&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subsubchap:structural&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.2.1&lt;/b&gt; Structural Linguistic Complexity&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.2.2&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subsubchap:lm-surprisal&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.2.2&lt;/b&gt; Language Modeling Surprisal&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.3&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subchap:extrinsic&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.3&lt;/b&gt; Extrinsic Perspective&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.3.1&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subsubchap:readability&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.3.1&lt;/b&gt; Automatic Readability Assessment&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.3.2&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subsubchap:pc&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.3.2&lt;/b&gt; Perceived Complexity Prediction&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.3.3&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subsubchap:eye-tracking&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.3.3&lt;/b&gt; Gaze Metrics Prediction&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.4&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subchap:garden-path&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.4&lt;/b&gt; Garden-path Sentences&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2&lt;/b&gt; &lt;strong&gt;Models of Linguistic Complexity&lt;/strong&gt;&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2.1&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html#subchap:desiderata&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2.1&lt;/b&gt; Desiderata for Models of Linguistic Complexity&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2.2&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html#subchap:nlm&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2.2&lt;/b&gt; Neural Language Models: Unsupervised Multitask Learners&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2.2.1&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html#subsubchap:syntax-nlm&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2.2.1&lt;/b&gt; Emergent Linguistic Structures in Neural Language Models&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2.3&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html#subchap:analyzing-nlm&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2.3&lt;/b&gt; Analyzing Neural Models of Complexity&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2.3.1&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html#subsubchap:probe&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2.3.1&lt;/b&gt; Probing classifiers&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2.3.2&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html#subsubchap:rsa&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2.3.2&lt;/b&gt; Representational Similarity Analysis&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2.3.3&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html#subsubchap:pwcca&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2.3.3&lt;/b&gt; Projection-Weighted Canonical Correlation Analysis&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3&lt;/b&gt; &lt;strong&gt;Complexity Phenomena in Linguistic Annotations and Language Models&lt;/strong&gt;&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3.1&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html#subchap:ex1-data&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3.1&lt;/b&gt; Data and Preprocessing&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3.2&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html#subchap:ex1-analysis&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3.2&lt;/b&gt; Analysis of Linguistic Phenomena&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3.2.1&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html#subsubchap:ex1-analysis-bins&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3.2.1&lt;/b&gt; Linguistic Phenomena in Length-controlled Bins&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3.3&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html#subchap:ex1-modeling&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3.3&lt;/b&gt; Modeling Online and Offline Linguistic Complexity&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3.3.1&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html#subsubchap:ex1-modeling-bins&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3.3.1&lt;/b&gt; Modeling Complexity in Length-controlled Bins&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3.4&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html#subchap:ex1-probing&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3.4&lt;/b&gt; Probing Linguistic Phenomena in ALBERT Representations&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3.5&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html#subchap:ex1-summary&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3.5&lt;/b&gt; Summary&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;4&#34; data-path=&#34;chap-ex2.html&#34;&gt;&lt;a href=&#34;chap-ex2.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;4&lt;/b&gt; &lt;strong&gt;Representational Similarity in Models of Complexity&lt;/strong&gt;&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;4.1&#34; data-path=&#34;chap-ex2.html&#34;&gt;&lt;a href=&#34;chap-ex2.html#knowledge-driven-requirements-for-learning-models&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;4.1&lt;/b&gt; Knowledge-driven Requirements for Learning Models&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;4.2&#34; data-path=&#34;chap-ex2.html&#34;&gt;&lt;a href=&#34;chap-ex2.html#subchap:ex2-experiments&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;4.2&lt;/b&gt; Experimentsl Evaluation&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;4.2.1&#34; data-path=&#34;chap-ex2.html&#34;&gt;&lt;a href=&#34;chap-ex2.html#subsubchap:ex2-data&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;4.2.1&lt;/b&gt; Data&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;4.2.2&#34; data-path=&#34;chap-ex2.html&#34;&gt;&lt;a href=&#34;chap-ex2.html#subsubchap:ex2-inter&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;4.2.2&lt;/b&gt; Inter-model Representational Similarity&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;4.2.3&#34; data-path=&#34;chap-ex2.html&#34;&gt;&lt;a href=&#34;chap-ex2.html#subsubchap:ex2-intra&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;4.2.3&lt;/b&gt; Intra-model Representational Similarity&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;4.3&#34; data-path=&#34;chap-ex2.html&#34;&gt;&lt;a href=&#34;chap-ex2.html#subchap:ex2-summary&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;4.3&lt;/b&gt; Summary&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;5&#34; data-path=&#34;chap-ex3.html&#34;&gt;&lt;a href=&#34;chap-ex3.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;5&lt;/b&gt; &lt;strong&gt;Gaze-informed Models for Cognitive Processing Prediction&lt;/strong&gt;&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;5.1&#34; data-path=&#34;chap-ex3.html&#34;&gt;&lt;a href=&#34;chap-ex3.html#subchap:ex3-setup&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;5.1&lt;/b&gt; Experimental Setup&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;5.2&#34; data-path=&#34;chap-ex3.html&#34;&gt;&lt;a href=&#34;chap-ex3.html#subchap:ex3-experiments&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;5.2&lt;/b&gt; Experimental Evaluation&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;5.2.1&#34; data-path=&#34;chap-ex3.html&#34;&gt;&lt;a href=&#34;chap-ex3.html#subsubchap:ex3-magnitudes&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;5.2.1&lt;/b&gt; Estimating Magnitudes of Garden-path Delays&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;5.2.2&#34; data-path=&#34;chap-ex3.html&#34;&gt;&lt;a href=&#34;chap-ex3.html#subsubchap:ex3-predicting&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;5.2.2&lt;/b&gt; Predicting Delays with Surprisal and Gaze Metrics&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;5.3&#34; data-path=&#34;chap-ex3.html&#34;&gt;&lt;a href=&#34;chap-ex3.html#subchap:ex3-summary&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;5.3&lt;/b&gt; Summary&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;conclusion.html#conclusion&#34;&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;&#34; data-path=&#34;conclusion.html&#34;&gt;&lt;a href=&#34;conclusion.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;Broader Impact and Ethical Perspectives&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;&#34; data-path=&#34;conclusion.html&#34;&gt;&lt;a href=&#34;conclusion.html#future-directions&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;Future Directions&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;appendix&#34;&gt;&lt;span&gt;&lt;b&gt;Appendix&lt;/b&gt;&lt;/span&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;A&#34; data-path=&#34;app-ling-feats.html&#34;&gt;&lt;a href=&#34;app-ling-feats.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;A&lt;/b&gt; Linguistic Features&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;A.1&#34; data-path=&#34;app-ling-feats.html&#34;&gt;&lt;a href=&#34;app-ling-feats.html#raw-text-properties-and-lexical-variety&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;A.1&lt;/b&gt; Raw Text Properties and Lexical Variety&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;A.2&#34; data-path=&#34;app-ling-feats.html&#34;&gt;&lt;a href=&#34;app-ling-feats.html#morpho-syntacting-information&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;A.2&lt;/b&gt; Morpho-syntacting Information&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;A.3&#34; data-path=&#34;app-ling-feats.html&#34;&gt;&lt;a href=&#34;app-ling-feats.html#verbal-predicate-structure&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;A.3&lt;/b&gt; Verbal Predicate Structure&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;A.4&#34; data-path=&#34;app-ling-feats.html&#34;&gt;&lt;a href=&#34;app-ling-feats.html#global-and-local-parsed-tree-structures&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;A.4&lt;/b&gt; Global and Local Parsed Tree Structures&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;A.5&#34; data-path=&#34;app-ling-feats.html&#34;&gt;&lt;a href=&#34;app-ling-feats.html#syntactic-relations&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;A.5&lt;/b&gt; Syntactic Relations&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;A.6&#34; data-path=&#34;app-ling-feats.html&#34;&gt;&lt;a href=&#34;app-ling-feats.html#subordination-phenomena&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;A.6&lt;/b&gt; Subordination Phenomena&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;B&#34; data-path=&#34;app-et-metrics.html&#34;&gt;&lt;a href=&#34;app-et-metrics.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;B&lt;/b&gt; Precisions on Eye-tracking Metrics and Preprocessing&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;C&#34; data-path=&#34;app-et-modeling.html&#34;&gt;&lt;a href=&#34;app-et-modeling.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;C&lt;/b&gt; Multi-task Token-level Regression for Gaze Metrics Prediction&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;D&#34; data-path=&#34;app-intra-sim.html&#34;&gt;&lt;a href=&#34;app-intra-sim.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;D&lt;/b&gt; Intra-model Similarity for All Models&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;E&#34; data-path=&#34;app-garden-paths-et.html&#34;&gt;&lt;a href=&#34;app-garden-paths-et.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;E&lt;/b&gt; Gaze Metrics Predictions for Garden Path Sentences&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;F&#34; data-path=&#34;app-params.html&#34;&gt;&lt;a href=&#34;app-params.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;F&lt;/b&gt; Reproducibility and Environmental Impact&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;&#34; data-path=&#34;references.html&#34;&gt;&lt;a href=&#34;references.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;References&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;divider&#34;&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://gsarti.com&#34;&gt;Back to my website&lt;/a&gt;&lt;/li&gt;

&lt;/ul&gt;

      &lt;/nav&gt;
    &lt;/div&gt;

    &lt;div class=&#34;book-body&#34;&gt;
      &lt;div class=&#34;body-inner&#34;&gt;
        &lt;div class=&#34;book-header&#34; role=&#34;navigation&#34;&gt;
          &lt;h1&gt;
            &lt;i class=&#34;fa fa-circle-o-notch fa-spin&#34;&gt;&lt;/i&gt;&lt;a href=&#34;./&#34;&gt;Interpreting Neural Language Models&lt;br /&gt;
for Linguistic Complexity Assessment&lt;/a&gt;
          &lt;/h1&gt;
        &lt;/div&gt;

        &lt;div class=&#34;page-wrapper&#34; tabindex=&#34;-1&#34; role=&#34;main&#34;&gt;
          &lt;div class=&#34;page-inner&#34;&gt;

            &lt;section class=&#34;normal&#34; id=&#34;section-&#34;&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;&lt;/h1&gt;

&lt;p&gt;This thesis work adopted a model-driven approach to investigate the relationship between different linguistic complexity perspectives for the English language and study how those are learned and encoded by deep learning models at various abstraction levels.&lt;/p&gt;
&lt;p&gt;From the theoretical viewpoint of connecting different complexity perspectives using empirical annotations, Chapter &lt;a href=&#34;chap-ex1.html#chap:ex1&#34;&gt;3&lt;/a&gt; analysis highlighted the strong connection between online/offline complexity metrics and length-related linguistic properties of sentences. The relation was further investigated in length-controlled settings, obtaining similar results across online gaze measurements but different for offline perceived complexity annotations. The overall results identify syntagmatic complexity as the primary source of variation in both offline and online complexity perception for readers. However, they also show how the variety in parts and hierarchical structures contributes differently across different complexity perspectives when sentence length is controlled. Another theoretical aspect supported by Chapter &lt;a href=&#34;chap-ex3.html#chap:ex3&#34;&gt;5&lt;/a&gt; experimental results is the role played by cognitive mechanisms other than predictability in shaping human processing patterns on ambiguous constructions like garden-path sentences. In this context, a computational model that accurately predicts the presence or garden-path effects was used as a psycholinguistic subject to provide predictability annotations on standard and atypical constructions. A surprisal-to-reading-times conversion coefficient was then estimated from gaze annotations and surprisal scores on standard constructions. The resulting reading times were used to highlight how the model widely overestimated the magnitude of garden-path effects, following the methodology of &lt;span class=&#34;citation&#34;&gt;Schijndel and Linzen (&lt;a href=&#34;#ref-schjindel-linzen-2020-single&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt;. While results differ significantly from the latter study due to a much larger conversion coefficient, the presence of different accounts for cognitive processing is supported when considering how proportions in predicted magnitudes on different types of constructions do not match the ones reported in recent psycholinguistics literature.&lt;/p&gt;
&lt;p&gt;Despite interesting theoretical findings, this work is mostly devoted to interpreting complexity phenomena from a modeling standpoint. Chapter &lt;a href=&#34;chap-ex1.html#chap:ex1&#34;&gt;3&lt;/a&gt; evaluates the encoding of linguistic properties inside neural language models’ representations using probing tasks performed before and after model fine-tuning on complexity-related tasks. Results highlighted the emergence of task-related linguistic properties within the model’s representations after the fine-tuning process, providing evidence for the relation between models’ linguistic skills during training and their performances on morphosyntactically-related tasks. In light of these findings, it can be conjectured that linguistic probes may provide a reasonable estimate of the task-oriented quality of representations for those highly-syntactic tasks. In Chapter &lt;a href=&#34;chap-ex2.html#chap:ex2&#34;&gt;4&lt;/a&gt;, the representations learned by neural language models were compared across layers and fine-tuning tasks using representational similarity approaches. The absence of higher similarity scores between complexity-trained models compared to the pre-trained one suggests that training objectives are learned by overfitting annotations and that learned parameters hardly capture information that could be relevant for multiple complexity-related tasks.&lt;/p&gt;
&lt;p&gt;Moreover, task framing and the annotation modalities were observed to play a much larger role in defining representational similarity scores rather than the conceptual similarity between tasks. This fact supports the claim that standard optimization procedures used in deep learning are not suitable for this type of concept-driven learning. Finally, Chapter &lt;a href=&#34;chap-ex3.html#chap:ex3&#34;&gt;5&lt;/a&gt; highlighted the inability of standard neural language models in leveraging syntactic cues to improve prediction in the context of garden-path effects. Models fine-tuned on gaze annotations were tested on garden-path test suites to evaluate whether reading time predictions can perform as well as surprisal in identifying garden-path triggers. Results highlight how models heavily overfit gaze annotation and cannot predict the increase in reading times observed in human subjects despite being exposed to the temporary syntactic ambiguity that characterizes garden-path constructions.&lt;/p&gt;
&lt;p&gt;Recent trends in transfer learning have profoundly shaped the last few years of research in NLP, leading to astonishing improvements in almost all language-related tasks, including linguistic complexity prediction. Despite all the hype, the fundamental problem behind all computational linguistics research remains: even the most powerful deep learning models do not “understand” language, and their learned representations are “potentially useful, but incomplete, reflections of the actual meaning” they derive from structural training procedures &lt;span class=&#34;citation&#34;&gt;(Bender and Koller &lt;a href=&#34;#ref-bender-koller-2020-climbing&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt;. In support of this affirmation, all models leveraged in this study by following closely standard procedures were found lacking in generalization capabilities and hierarchical abstraction, despite their excellent performances on predicting in-domain observations. To conclude with a somewhat cliché affirmation, much work still needs to be done to drive generalizable, hierarchical, and compositional representation learning in language models, enabling proper human-level natural language understanding.&lt;/p&gt;
&lt;div id=&#34;broader-impact-and-ethical-perspectives&#34; class=&#34;section level2 unnumbered&#34;&gt;
&lt;h2&gt;Broader Impact and Ethical Perspectives&lt;/h2&gt;
&lt;p&gt;The findings described in this thesis work are mostly meta-analytical, and as such, mostly intended to distill theoretical insights and evaluate recent efforts in the natural language processing community. This said, some of the models and procedures described in this work can be clearly beneficial to society. For example, using models trained to predict reading patterns may be used in educational settings to identify difficult passages that can be simplified, improving reading comprehension for students in a fully-personalizable way. This type of technology can also be applied to domain-specific documents such as juridical or medical reports to identify critical areas that can be adapted to improve layman’s understanding. However, it is essential to recognize the potentially malicious usage of such systems. The integration of eye-tracking systems in mobile devices, paired with predictive models presented in this work, could be used to build harmful surveillance systems and advertisement platforms using gaze predictions for extreme behavioral manipulation. Moreover, multiple individuals’ gaze data could be leveraged by autonomous systems to enforce discriminatory practices towards neurodiverse subjects in hardly-detectable ways. In terms of research impact, the experiments presented in this work may provide useful insights into the behavior of neural language models for researchers working in the fields of interpretability in NLP and computational psycholinguistics.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;future-directions&#34; class=&#34;section level2 unnumbered&#34;&gt;
&lt;h2&gt;Future Directions&lt;/h2&gt;
&lt;p&gt;In conclusion, multiple paths to improve and extend the scope of this work were identified during the experimental process, and will be left here as a final note for my future self and for anyone interested in pushing forward research in fields related to this thesis’ topics.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Self-training has recently proven to be very effective for compensating the lack of large labeled datasets in the context of acceptability and complexity prediction &lt;span class=&#34;citation&#34;&gt;(Sarti &lt;a href=&#34;#ref-sarti-2020-umbertomtsa&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt;. In light of these results, it would be interesting to evaluate whether self-training could also improve the performances and generalization of models used for gaze metrics prediction.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Evaluate whether gaze-trained neural language models having undergone a &lt;em&gt;cloze distillation process&lt;/em&gt; &lt;span class=&#34;citation&#34;&gt;(Eisape, Zaslavsky, and Levy &lt;a href=&#34;#ref-eisape-etal-2020-cloze&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt;, combining intuitions from masked language modeling and knowledge distillation &lt;span class=&#34;citation&#34;&gt;(Hinton, Vinyals, and Dean &lt;a href=&#34;#ref-hinton-etal-2015-distilling&#34;&gt;2015&lt;/a&gt;)&lt;/span&gt;, would produce better results for modeling out-of-distribution garden-path phenomena compared to the somewhat naive approach adopted in this study.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Incorporating gaze metrics prediction in the training objectives of learning models can be interesting to account for human cognitive biases during reading. The crucial aspect is how to get a sufficient amount of annotated data to make this idea scalable for modern language models’ pre-training needs. In this regard, it could be interesting to test the approach by &lt;span class=&#34;citation&#34;&gt;Hollenstein and Zhang (&lt;a href=&#34;#ref-hollenstein-zhang-2019-entity&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt; where mean gaze scores are averaged for each type across annotators, effectively providing a way to label input sentences with robust gaze information in an unsupervised manner.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Since eye-tracking metrics are complexity signals with free human supervision, it could be possible to leverage those for simplification and other related tasks in an iterative learning-from-human-feedback paradigm similar to the one described in &lt;span class=&#34;citation&#34;&gt;Stiennon et al. (&lt;a href=&#34;#ref-stiennon-etal-2020-learning&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;It should in principle be possible to use human processing data as a replacement for the self-attention computation. The dot product critically bounds the computational efficiency of attention-based models, and fixed attention has been shown to have a limited negative impact on final results while making inference much faster &lt;span class=&#34;citation&#34;&gt;(Tay et al. &lt;a href=&#34;#ref-tay-etal-2020-synthesizer&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt;. Fixing attention weights using human attention, as measured by eye-tracking metrics, can be an exciting perspective to explore in this context. This idea can be thought of as an application of human attention regularization of LSTM attentional networks for various tasks proposed in &lt;span class=&#34;citation&#34;&gt;Barrett et al. (&lt;a href=&#34;#ref-barrett-etal-2018-sequence&#34;&gt;2018&lt;/a&gt;)&lt;/span&gt; to Transformers networks.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Would explicitly embedding complexity in the learning process of language models favor hierarchical abstraction? In this perspective, it would be exciting to evaluate whether a model trained on easy-to-hard sentences following language acquisition insights would encode different knowledge in terms of linguistic structures, concept abstraction, and allowances.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Finding better ways to instill useful inductive biases into learning models, especially for syntax-heavy downstream tasks. Concrete examples following this direction may use parsing as a complementary task to keep top-level representations sensible to syntactic changes, as tested in &lt;span class=&#34;citation&#34;&gt;Glavas and Vulic (&lt;a href=&#34;#ref-glavas-vulic-2020-supervised&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt; for natural language understanding, or use hybrid symbolic-neural models to represent syntax as in &lt;span class=&#34;citation&#34;&gt;Zanzotto et al. (&lt;a href=&#34;#ref-zanzotto-etal-2020-kermit&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;



&lt;h3&gt;References&lt;/h3&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-barrett-etal-2018-sequence&#34;&gt;
&lt;p&gt;Barrett, Maria, Joachim Bingel, Nora Hollenstein, Marek Rei, and Anders Søgaard. 2018. “Sequence Classification with Human Attention.” In &lt;em&gt;Proceedings of the 22nd Conference on Computational Natural Language Learning&lt;/em&gt;, 302–12. Brussels, Belgium: Association for Computational Linguistics. &lt;a href=&#34;https://doi.org/10.18653/v1/K18-1030&#34;&gt;https://doi.org/10.18653/v1/K18-1030&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-bender-koller-2020-climbing&#34;&gt;
&lt;p&gt;Bender, Emily M., and Alexander Koller. 2020. “Climbing Towards NLU: On Meaning, Form, and Understanding in the Age of Data.” In &lt;em&gt;Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics&lt;/em&gt;, 5185–98. Online: Association for Computational Linguistics. &lt;a href=&#34;https://doi.org/10.18653/v1/2020.acl-main.463&#34;&gt;https://doi.org/10.18653/v1/2020.acl-main.463&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-eisape-etal-2020-cloze&#34;&gt;
&lt;p&gt;Eisape, Tiwalayo, Noga Zaslavsky, and Roger Levy. 2020. “Cloze Distillation Improves Psychometric Predictive Power.” In &lt;em&gt;Proceedings of the 24th Conference on Computational Natural Language Learning&lt;/em&gt;, 609–19. Online: Association for Computational Linguistics. &lt;a href=&#34;https://www.aclweb.org/anthology/2020.conll-1.49&#34;&gt;https://www.aclweb.org/anthology/2020.conll-1.49&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-glavas-vulic-2020-supervised&#34;&gt;
&lt;p&gt;Glavas, Goran, and Ivan Vulic. 2020. “Is Supervised Syntactic Parsing Beneficial for Language Understanding? An Empirical Investigation.” &lt;em&gt;ArXiv Pre-Print&lt;/em&gt; 2008.06788. &lt;a href=&#34;https://arxiv.org/abs/2008.06788&#34;&gt;https://arxiv.org/abs/2008.06788&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-hinton-etal-2015-distilling&#34;&gt;
&lt;p&gt;Hinton, Geoffrey, Oriol Vinyals, and Jeff Dean. 2015. “Distilling the Knowledge in a Neural Network.” &lt;em&gt;ArXiv Pre-Print&lt;/em&gt; 1503.02531. &lt;a href=&#34;https://arxiv.org/abs/1503.02531&#34;&gt;https://arxiv.org/abs/1503.02531&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-hollenstein-zhang-2019-entity&#34;&gt;
&lt;p&gt;Hollenstein, Nora, and Ce Zhang. 2019. “Entity Recognition at First Sight: Improving NER with Eye Movement Information.” In &lt;em&gt;Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)&lt;/em&gt;, 1–10. Minneapolis, Minnesota: Association for Computational Linguistics. &lt;a href=&#34;https://doi.org/10.18653/v1/N19-1001&#34;&gt;https://doi.org/10.18653/v1/N19-1001&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-sarti-2020-umbertomtsa&#34;&gt;
&lt;p&gt;Sarti, Gabriele. 2020. “UmBERTo-MTSA @ AcCompl-It: Improving Complexity and Acceptability Prediction with Multi-Task Learning on Self-Supervised Annotations.” &lt;em&gt;ArXiv Pre-Print&lt;/em&gt; 2011.05197. &lt;a href=&#34;https://arxiv.org/abs/2011.05197&#34;&gt;https://arxiv.org/abs/2011.05197&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-schjindel-linzen-2020-single&#34;&gt;
&lt;p&gt;Schijndel, Marten van, and Tal Linzen. 2020. “Single-Stage Prediction Models Do Not Explain the Magnitude of Syntactic Disambiguation Difficulty.” &lt;em&gt;PsyArXiv Pre-Print&lt;/em&gt; sgbqy. &lt;a href=&#34;https://psyarxiv.com/sgbqy/&#34;&gt;https://psyarxiv.com/sgbqy/&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-stiennon-etal-2020-learning&#34;&gt;
&lt;p&gt;Stiennon, Nisan, Long Ouyang, Jeff Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul Christiano. 2020. “Learning to Summarize from Human Feedback.” &lt;em&gt;ArXiv Pre-Print&lt;/em&gt; 2009.01325. &lt;a href=&#34;https://arxiv.org/abs/2009.01325&#34;&gt;https://arxiv.org/abs/2009.01325&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-tay-etal-2020-synthesizer&#34;&gt;
&lt;p&gt;Tay, Yi, Dara Bahri, Donald Metzler, D. Juan, Zhe Zhao, and Che Zheng. 2020. “Synthesizer: Rethinking Self-Attention in Transformer Models.” &lt;em&gt;ArXiv Pre-Print&lt;/em&gt; 2005.00743. &lt;a href=&#34;https://arxiv.org/abs/2005.00743&#34;&gt;https://arxiv.org/abs/2005.00743&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-zanzotto-etal-2020-kermit&#34;&gt;
&lt;p&gt;Zanzotto, Fabio Massimo, Andrea Santilli, Leonardo Ranaldi, Dario Onorati, Pierfrancesco Tommasino, and Francesca Fallucchi. 2020. “KERMIT: Complementing Transformer Architectures with Encoders of Explicit Syntactic Interpretations.” In &lt;em&gt;Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (Emnlp)&lt;/em&gt;, 256–67. Online: Association for Computational Linguistics. &lt;a href=&#34;https://www.aclweb.org/anthology/2020.emnlp-main.18&#34;&gt;https://www.aclweb.org/anthology/2020.emnlp-main.18&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
            &lt;/section&gt;

          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
&lt;a href=&#34;chap-ex3.html&#34; class=&#34;navigation navigation-prev &#34; aria-label=&#34;Previous page&#34;&gt;&lt;i class=&#34;fa fa-angle-left&#34;&gt;&lt;/i&gt;&lt;/a&gt;
&lt;a href=&#34;app-ling-feats.html&#34; class=&#34;navigation navigation-next &#34; aria-label=&#34;Next page&#34;&gt;&lt;i class=&#34;fa fa-angle-right&#34;&gt;&lt;/i&gt;&lt;/a&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/app.min.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/lunr.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/clipboard.min.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/plugin-search.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/plugin-sharing.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/plugin-fontsettings.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/plugin-bookdown.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/jquery.highlight.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/plugin-clipboard.js&#34;&gt;&lt;/script&gt;
&lt;script&gt;
gitbook.require([&#34;gitbook&#34;], function(gitbook) {
gitbook.start({
&#34;sharing&#34;: {
&#34;github&#34;: true,
&#34;facebook&#34;: true,
&#34;twitter&#34;: true,
&#34;linkedin&#34;: true,
&#34;weibo&#34;: false,
&#34;instapaper&#34;: false,
&#34;vk&#34;: false,
&#34;all&#34;: false
},
&#34;fontsettings&#34;: {
&#34;theme&#34;: &#34;white&#34;,
&#34;family&#34;: &#34;sans&#34;,
&#34;size&#34;: 2
},
&#34;edit&#34;: {
&#34;link&#34;: &#34;https://github.com/gsarti/master-thesis/tree/master/06-Conclusion.Rmd&#34;,
&#34;text&#34;: &#34;Edit&#34;
},
&#34;history&#34;: {
&#34;link&#34;: null,
&#34;text&#34;: null
},
&#34;view&#34;: {
&#34;link&#34;: null,
&#34;text&#34;: null
},
&#34;download&#34;: [[&#34;Sarti_2020_Interpreting_NLMs_for_LCA.pdf&#34;, &#34;PDF&#34;]],
&#34;toc&#34;: {
&#34;collapse&#34;: &#34;subsection&#34;,
&#34;scroll_highlight&#34;: true
},
&#34;info&#34;: false
});
});
&lt;/script&gt;

&lt;!-- dynamically load mathjax for compatibility with self-contained --&gt;
&lt;script&gt;
  (function () {
    var script = document.createElement(&#34;script&#34;);
    script.type = &#34;text/javascript&#34;;
    var src = &#34;true&#34;;
    if (src === &#34;&#34; || src === &#34;true&#34;) src = &#34;https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML&#34;;
    if (location.protocol !== &#34;file:&#34;)
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, &#39;&#39;);
    script.src = src;
    document.getElementsByTagName(&#34;head&#34;)[0].appendChild(script);
  })();
&lt;/script&gt;
&lt;/body&gt;

&lt;/html&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>http://localhost:1313/msc-thesis/introduction/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/msc-thesis/introduction/</guid>
      <description>&lt;!DOCTYPE html&gt;
&lt;html lang=&#34;&#34; xml:lang=&#34;&#34;&gt;
&lt;head&gt;

  &lt;meta charset=&#34;utf-8&#34; /&gt;
  &lt;meta http-equiv=&#34;X-UA-Compatible&#34; content=&#34;IE=edge&#34; /&gt;
  &lt;title&gt;Interpreting Neural Language Models for Linguistic Complexity Assessment&lt;/title&gt;
  &lt;meta name=&#34;description&#34; content=&#34;MSc Thesis at the University of Trieste and SISSA under the supervision of Prof. Davide Crepaldi and Dott. Felice dell&#39;Orletta&#34; /&gt;
  &lt;meta name=&#34;generator&#34; content=&#34;bookdown 0.20.6 and GitBook 2.6.7&#34; /&gt;

  &lt;meta property=&#34;og:title&#34; content=&#34;Interpreting Neural Language Models for Linguistic Complexity Assessment&#34; /&gt;
  &lt;meta property=&#34;og:type&#34; content=&#34;book&#34; /&gt;
  &lt;meta property=&#34;og:url&#34; content=&#34;https://gsarti.com/master-thesis&#34; /&gt;
  &lt;meta property=&#34;og:image&#34; content=&#34;https://gsarti.com/master-thesisfigures/cover.png&#34; /&gt;
  &lt;meta property=&#34;og:description&#34; content=&#34;MSc Thesis at the University of Trieste and SISSA under the supervision of Prof. Davide Crepaldi and Dott. Felice dell&#39;Orletta&#34; /&gt;
  &lt;meta name=&#34;github-repo&#34; content=&#34;gsarti/interpreting-complexity&#34; /&gt;

  &lt;meta name=&#34;twitter:card&#34; content=&#34;summary&#34; /&gt;
  &lt;meta name=&#34;twitter:title&#34; content=&#34;Interpreting Neural Language Models for Linguistic Complexity Assessment&#34; /&gt;
  
  &lt;meta name=&#34;twitter:description&#34; content=&#34;MSc Thesis at the University of Trieste and SISSA under the supervision of Prof. Davide Crepaldi and Dott. Felice dell&#39;Orletta&#34; /&gt;
  &lt;meta name=&#34;twitter:image&#34; content=&#34;https://gsarti.com/master-thesisfigures/cover.png&#34; /&gt;

&lt;meta name=&#34;author&#34; content=&#34;Gabriele Sarti&#34; /&gt;



  &lt;meta name=&#34;viewport&#34; content=&#34;width=device-width, initial-scale=1&#34; /&gt;
  &lt;meta name=&#34;apple-mobile-web-app-capable&#34; content=&#34;yes&#34; /&gt;
  &lt;meta name=&#34;apple-mobile-web-app-status-bar-style&#34; content=&#34;black&#34; /&gt;
  &lt;link rel=&#34;apple-touch-icon-precomposed&#34; sizes=&#34;152x152&#34; href=&#34;figures/icons/apple-icon.png&#34; /&gt;
  &lt;link rel=&#34;shortcut icon&#34; href=&#34;figures/icons/favicon.ico&#34; type=&#34;image/x-icon&#34; /&gt;

&lt;link rel=&#34;next&#34; href=&#34;chap-ling-comp.html&#34;/&gt;
&lt;style type=&#34;text/css&#34;&gt;
p.abstract{
  text-align: center;
  font-weight: bold;
}
div.abstract{
  margin: auto;
  width: 90%;
}
&lt;/style&gt;
&lt;script src=&#34;libs/jquery-2.2.3/jquery.min.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;libs/gitbook-2.6.7/css/style.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;link href=&#34;libs/gitbook-2.6.7/css/plugin-table.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;link href=&#34;libs/gitbook-2.6.7/css/plugin-bookdown.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;link href=&#34;libs/gitbook-2.6.7/css/plugin-highlight.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;link href=&#34;libs/gitbook-2.6.7/css/plugin-search.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;link href=&#34;libs/gitbook-2.6.7/css/plugin-fontsettings.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;link href=&#34;libs/gitbook-2.6.7/css/plugin-clipboard.css&#34; rel=&#34;stylesheet&#34; /&gt;









&lt;script src=&#34;libs/kePrint-0.0.1/kePrint.js&#34;&gt;&lt;/script&gt;



&lt;link rel=&#34;stylesheet&#34; href=&#34;templates/style.css&#34; type=&#34;text/css&#34; /&gt;
&lt;/head&gt;

&lt;body&gt;



  &lt;div class=&#34;book without-animation with-summary font-size-2 font-family-1&#34; data-basepath=&#34;.&#34;&gt;

    &lt;div class=&#34;book-summary&#34;&gt;
      &lt;nav role=&#34;navigation&#34;&gt;

&lt;ul class=&#34;summary&#34;&gt;
&lt;li&gt;&lt;a href=&#34;introduction.html#introduction&#34;&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1&lt;/b&gt; &lt;strong&gt;Linguistic Complexity&lt;/strong&gt;&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.1&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subchap:categorizing&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.1&lt;/b&gt; Categorizing Linguistic Complexity Measures&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.2&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subchap:intrinsic&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.2&lt;/b&gt; Intrinsic Perspective&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.2.1&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subsubchap:structural&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.2.1&lt;/b&gt; Structural Linguistic Complexity&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.2.2&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subsubchap:lm-surprisal&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.2.2&lt;/b&gt; Language Modeling Surprisal&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.3&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subchap:extrinsic&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.3&lt;/b&gt; Extrinsic Perspective&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.3.1&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subsubchap:readability&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.3.1&lt;/b&gt; Automatic Readability Assessment&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.3.2&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subsubchap:pc&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.3.2&lt;/b&gt; Perceived Complexity Prediction&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.3.3&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subsubchap:eye-tracking&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.3.3&lt;/b&gt; Gaze Metrics Prediction&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.4&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subchap:garden-path&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.4&lt;/b&gt; Garden-path Sentences&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2&lt;/b&gt; &lt;strong&gt;Models of Linguistic Complexity&lt;/strong&gt;&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2.1&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html#subchap:desiderata&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2.1&lt;/b&gt; Desiderata for Models of Linguistic Complexity&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2.2&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html#subchap:nlm&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2.2&lt;/b&gt; Neural Language Models: Unsupervised Multitask Learners&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2.2.1&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html#subsubchap:syntax-nlm&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2.2.1&lt;/b&gt; Emergent Linguistic Structures in Neural Language Models&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2.3&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html#subchap:analyzing-nlm&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2.3&lt;/b&gt; Analyzing Neural Models of Complexity&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2.3.1&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html#subsubchap:probe&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2.3.1&lt;/b&gt; Probing classifiers&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2.3.2&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html#subsubchap:rsa&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2.3.2&lt;/b&gt; Representational Similarity Analysis&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2.3.3&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html#subsubchap:pwcca&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2.3.3&lt;/b&gt; Projection-Weighted Canonical Correlation Analysis&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3&lt;/b&gt; &lt;strong&gt;Complexity Phenomena in Linguistic Annotations and Language Models&lt;/strong&gt;&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3.1&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html#subchap:ex1-data&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3.1&lt;/b&gt; Data and Preprocessing&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3.2&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html#subchap:ex1-analysis&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3.2&lt;/b&gt; Analysis of Linguistic Phenomena&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3.2.1&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html#subsubchap:ex1-analysis-bins&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3.2.1&lt;/b&gt; Linguistic Phenomena in Length-controlled Bins&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3.3&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html#subchap:ex1-modeling&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3.3&lt;/b&gt; Modeling Online and Offline Linguistic Complexity&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3.3.1&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html#subsubchap:ex1-modeling-bins&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3.3.1&lt;/b&gt; Modeling Complexity in Length-controlled Bins&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3.4&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html#subchap:ex1-probing&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3.4&lt;/b&gt; Probing Linguistic Phenomena in ALBERT Representations&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3.5&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html#subchap:ex1-summary&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3.5&lt;/b&gt; Summary&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;4&#34; data-path=&#34;chap-ex2.html&#34;&gt;&lt;a href=&#34;chap-ex2.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;4&lt;/b&gt; &lt;strong&gt;Representational Similarity in Models of Complexity&lt;/strong&gt;&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;4.1&#34; data-path=&#34;chap-ex2.html&#34;&gt;&lt;a href=&#34;chap-ex2.html#knowledge-driven-requirements-for-learning-models&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;4.1&lt;/b&gt; Knowledge-driven Requirements for Learning Models&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;4.2&#34; data-path=&#34;chap-ex2.html&#34;&gt;&lt;a href=&#34;chap-ex2.html#subchap:ex2-experiments&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;4.2&lt;/b&gt; Experimentsl Evaluation&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;4.2.1&#34; data-path=&#34;chap-ex2.html&#34;&gt;&lt;a href=&#34;chap-ex2.html#subsubchap:ex2-data&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;4.2.1&lt;/b&gt; Data&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;4.2.2&#34; data-path=&#34;chap-ex2.html&#34;&gt;&lt;a href=&#34;chap-ex2.html#subsubchap:ex2-inter&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;4.2.2&lt;/b&gt; Inter-model Representational Similarity&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;4.2.3&#34; data-path=&#34;chap-ex2.html&#34;&gt;&lt;a href=&#34;chap-ex2.html#subsubchap:ex2-intra&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;4.2.3&lt;/b&gt; Intra-model Representational Similarity&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;4.3&#34; data-path=&#34;chap-ex2.html&#34;&gt;&lt;a href=&#34;chap-ex2.html#subchap:ex2-summary&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;4.3&lt;/b&gt; Summary&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;5&#34; data-path=&#34;chap-ex3.html&#34;&gt;&lt;a href=&#34;chap-ex3.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;5&lt;/b&gt; &lt;strong&gt;Gaze-informed Models for Cognitive Processing Prediction&lt;/strong&gt;&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;5.1&#34; data-path=&#34;chap-ex3.html&#34;&gt;&lt;a href=&#34;chap-ex3.html#subchap:ex3-setup&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;5.1&lt;/b&gt; Experimental Setup&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;5.2&#34; data-path=&#34;chap-ex3.html&#34;&gt;&lt;a href=&#34;chap-ex3.html#subchap:ex3-experiments&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;5.2&lt;/b&gt; Experimental Evaluation&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;5.2.1&#34; data-path=&#34;chap-ex3.html&#34;&gt;&lt;a href=&#34;chap-ex3.html#subsubchap:ex3-magnitudes&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;5.2.1&lt;/b&gt; Estimating Magnitudes of Garden-path Delays&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;5.2.2&#34; data-path=&#34;chap-ex3.html&#34;&gt;&lt;a href=&#34;chap-ex3.html#subsubchap:ex3-predicting&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;5.2.2&lt;/b&gt; Predicting Delays with Surprisal and Gaze Metrics&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;5.3&#34; data-path=&#34;chap-ex3.html&#34;&gt;&lt;a href=&#34;chap-ex3.html#subchap:ex3-summary&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;5.3&lt;/b&gt; Summary&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;conclusion.html#conclusion&#34;&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;&#34; data-path=&#34;conclusion.html&#34;&gt;&lt;a href=&#34;conclusion.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;Broader Impact and Ethical Perspectives&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;&#34; data-path=&#34;conclusion.html&#34;&gt;&lt;a href=&#34;conclusion.html#future-directions&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;Future Directions&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;appendix&#34;&gt;&lt;span&gt;&lt;b&gt;Appendix&lt;/b&gt;&lt;/span&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;A&#34; data-path=&#34;app-ling-feats.html&#34;&gt;&lt;a href=&#34;app-ling-feats.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;A&lt;/b&gt; Linguistic Features&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;A.1&#34; data-path=&#34;app-ling-feats.html&#34;&gt;&lt;a href=&#34;app-ling-feats.html#raw-text-properties-and-lexical-variety&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;A.1&lt;/b&gt; Raw Text Properties and Lexical Variety&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;A.2&#34; data-path=&#34;app-ling-feats.html&#34;&gt;&lt;a href=&#34;app-ling-feats.html#morpho-syntacting-information&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;A.2&lt;/b&gt; Morpho-syntacting Information&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;A.3&#34; data-path=&#34;app-ling-feats.html&#34;&gt;&lt;a href=&#34;app-ling-feats.html#verbal-predicate-structure&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;A.3&lt;/b&gt; Verbal Predicate Structure&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;A.4&#34; data-path=&#34;app-ling-feats.html&#34;&gt;&lt;a href=&#34;app-ling-feats.html#global-and-local-parsed-tree-structures&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;A.4&lt;/b&gt; Global and Local Parsed Tree Structures&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;A.5&#34; data-path=&#34;app-ling-feats.html&#34;&gt;&lt;a href=&#34;app-ling-feats.html#syntactic-relations&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;A.5&lt;/b&gt; Syntactic Relations&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;A.6&#34; data-path=&#34;app-ling-feats.html&#34;&gt;&lt;a href=&#34;app-ling-feats.html#subordination-phenomena&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;A.6&lt;/b&gt; Subordination Phenomena&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;B&#34; data-path=&#34;app-et-metrics.html&#34;&gt;&lt;a href=&#34;app-et-metrics.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;B&lt;/b&gt; Precisions on Eye-tracking Metrics and Preprocessing&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;C&#34; data-path=&#34;app-et-modeling.html&#34;&gt;&lt;a href=&#34;app-et-modeling.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;C&lt;/b&gt; Multi-task Token-level Regression for Gaze Metrics Prediction&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;D&#34; data-path=&#34;app-intra-sim.html&#34;&gt;&lt;a href=&#34;app-intra-sim.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;D&lt;/b&gt; Intra-model Similarity for All Models&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;E&#34; data-path=&#34;app-garden-paths-et.html&#34;&gt;&lt;a href=&#34;app-garden-paths-et.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;E&lt;/b&gt; Gaze Metrics Predictions for Garden Path Sentences&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;F&#34; data-path=&#34;app-params.html&#34;&gt;&lt;a href=&#34;app-params.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;F&lt;/b&gt; Reproducibility and Environmental Impact&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;&#34; data-path=&#34;references.html&#34;&gt;&lt;a href=&#34;references.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;References&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;divider&#34;&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://gsarti.com&#34;&gt;Back to my website&lt;/a&gt;&lt;/li&gt;

&lt;/ul&gt;

      &lt;/nav&gt;
    &lt;/div&gt;

    &lt;div class=&#34;book-body&#34;&gt;
      &lt;div class=&#34;body-inner&#34;&gt;
        &lt;div class=&#34;book-header&#34; role=&#34;navigation&#34;&gt;
          &lt;h1&gt;
            &lt;i class=&#34;fa fa-circle-o-notch fa-spin&#34;&gt;&lt;/i&gt;&lt;a href=&#34;./&#34;&gt;Interpreting Neural Language Models&lt;br /&gt;
for Linguistic Complexity Assessment&lt;/a&gt;
          &lt;/h1&gt;
        &lt;/div&gt;

        &lt;div class=&#34;page-wrapper&#34; tabindex=&#34;-1&#34; role=&#34;main&#34;&gt;
          &lt;div class=&#34;page-inner&#34;&gt;

            &lt;section class=&#34;normal&#34; id=&#34;section-&#34;&gt;
&lt;div id=&#34;header&#34;&gt;
&lt;h1 class=&#34;title&#34;&gt;Interpreting Neural Language Models&lt;br /&gt;
for Linguistic Complexity Assessment&lt;/h1&gt;
&lt;p class=&#34;author&#34;&gt;&lt;em&gt;Gabriele Sarti&lt;/em&gt;&lt;/p&gt;
&lt;div class=&#34;abstract&#34;&gt;
&lt;p class=&#34;abstract&#34;&gt;Abstract&lt;/p&gt;
Lo studio della complessità linguistica è un ambito profondamente multidisciplinare, che spazia dallo studio dell’elaborazione cognitiva in lettori umani alla classificazione della complessità strutturale caratterizzante espressioni in linguaggio naturale. In tempi recenti, l’utilizzo di metodi computazionali per il trattamento e l’analisi del linguaggio ha prodotto importanti sviluppi nella comprensione di molteplici fenomeni associati alla complessità linguistica. In linea con lo stato dell’arte del settore, questa tesi presenta uno studio model-driven di molteplici fenomeni associati alla complessità linguistica. In primo luogo, vengono esplorate empiricamente le relazioni che sussistono tra varie metriche estrinseche di complessità – percezione di complessità linguistica, leggibilità, elaborazione cognitiva e prevedibilità – evidenziando similitudini e differenze da una prospettiva linguisticamente e cognitivamente motivata. In seguito, viene studiato come l’informazione alla base delle diverse metriche di complessità possa essere acquisita da modelli del linguaggio basati su reti neurali, a vari livelli di astrazione e granularità, applicando tecniche di interpretabilità derivate dalla letteratura sull’elaborazione del linguaggio naturale. In conclusione, viene valutata la capacità di vari modelli computazionali di complessità nel prevedere difficoltà di elaborazione cognitiva associate a costrutti sintattici atipici, quali le &lt;em&gt;garden-path sentences&lt;/em&gt;. I risultati sperimentali di questo studio forniscono prove convergenti riguardo alle limitate capacità di astrazione e generalizzazione dei modelli di linguaggio neurali allo stato dell’arte per la previsione della complessità linguistica, e incoraggiano all’adozione di linee di ricerca che integrino informazione simbolica e interpretabile in questo settore. In un’ottica di riproducibilità, il codice utilizzato per gli esperimenti viene reso disponibile al seguente indirizzo: &lt;a href=&#34;https://github.com/gsarti/interpreting-complexity&#34;&gt;https://github.com/gsarti/interpreting-complexity&lt;/a&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;introduction&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/h1&gt;
&lt;!-- Required since it is not a counted chapter otherwise --&gt;


&lt;!-- For PDF output, we must include this LaTeX command after unnumbered headings, otherwise the numbers in the mini table of contents will be incorrect --&gt;
&lt;p&gt;The study of complexity in language production and comprehension is a multidisciplinary field encompassing approaches that range from the analysis of cognitive processing phenomena in human subjects to the classification of structural complexity in natural language utterances. Because of its inherently faceted nature, linguistic complexity still defies a univocal definition and depends heavily on the point of view adopted during experimental inquiries. In recent years, as a consequence of the astounding expansion in human technological capabilities, the scientific community witnessed a proliferation of studies leveraging computational methods to investigate different complexity perspectives and develop automatic systems for linguistic complexity assessment. The introduction of neural network models able to automatically learn hierarchical representations of language spurred new lines of research in the field of Natural Language Processing, with researchers aiming to reverse-engineer theoretical intuitions by interpreting results and learning mechanics of those models. Nowadays, deep computational models are routinely adopted to study and evaluate linguistic complexity in applicative settings such as readability assessment, simplification, and first/second language learning.&lt;/p&gt;
&lt;!--In this context, members of the Natural Language Processing community contributed to a broader overview of the topic by connecting the concepts of surprisal and perplexity to their information-theoretic equivalents.--&gt;
&lt;p&gt;This thesis fits into this current line of research by pursuing a two-fold aim. On the one hand, it investigates the connection between multiple human-centric perspectives of linguistic complexity – perception of complexity, readability, cognitive processing, and predictability – highlighting similarities and differences between them from a linguistically and cognitively-motivated viewpoint. On the other hand, it studies how those perspectives are learned by deep learning models at various levels of granularity. This work’s primary focus concerns the analysis of learned representations using multiple interpretability techniques derived from the natural language processing (NLP) literature and the study of abstraction and generalization capabilities of modern computational models of language. A model-driven approach is adopted throughout this study, following the intuition that learned representations can be leveraged as proxies of the informational content required to perform linguistic complexity assessment. The modeling of linguistic complexity is studied on multiple extensively-used corpora spanning three complexity-related tasks – &lt;em&gt;perceived complexity prediction, automatic readability assessment, and gaze metrics prediction&lt;/em&gt; – and further validated on ad-hoc psycholinguistic test suites. To further validate the impact of structural factors for complexity assessment, neural network-based annotation pipelines are notably employed alongside neural language models as black-box feature extraction systems.&lt;/p&gt;
&lt;p&gt;Chapter &lt;a href=&#34;chap-ling-comp.html#chap:ling-comp&#34;&gt;1&lt;/a&gt; marks the beginning of this work by introducing the reader to the multiple facets of linguistic complexity. It starts with a broad categorization of complexity measurements into a spectrum taking into account both the perspective of analysis (intrinsic or extrinsic) and the processing modalities (online or offline). Relevant intrinsic perspectives related to linguistic complexity are then briefly presented, focusing on the extraction and use of morphosyntactic structures in complexity studies and the use of information-theoretic surprisal from language models as a structural measure of complexity. The three extrinsic complexity tasks representing this study’s focus and their respective corpora are introduced in detail, focusing on their differences both from a conceptual and a data collection perspective. The chapter ends with an introduction to &lt;em&gt;garden-path sentences&lt;/em&gt;, peculiar syntactic constructs associated with cognitive processing difficulties, later employed in the experiments of Chapter &lt;a href=&#34;chap-ex3.html#chap:ex3&#34;&gt;5&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Chapter &lt;a href=&#34;chap-models.html#chap:models&#34;&gt;2&lt;/a&gt; motivates the choice of NLMs as the critical component in our experimental analysis: their ability to encode both semantic and structural properties of language makes them especially suitable in the context of linguistic complexity modeling. After a summary of the ascent of NLMs in the field of NLP, the two neural language models used in experimental sections are presented in detail. To conclude, three interpretability approaches are used to leverage learned representations to study complexity learning across tasks, and abstraction layers are presented.&lt;/p&gt;
&lt;p&gt;Chapter &lt;a href=&#34;chap-ex1.html#chap:ex1&#34;&gt;3&lt;/a&gt; is the first experimental section, in which perceived complexity annotations and eye-tracking metrics collected at sentence level are linked to various linguistic phenomena extracted by a linguistic parser. The same analysis is also performed by controlling sentence length to limit the disproportionate influence of length-related features on complexity measures. The predictive performances of NLMs are then evaluated on perceived complexity and various eye-tracking metrics for both length-controlled and unconditional settings. The chapter ends with probing task experiments highlighting how complexity-related linguistic properties become implicitly encoded in model representations after complexity learning, suggesting interesting perspectives in priming models with syntactic information to improve their performances on complexity-related tasks.&lt;/p&gt;
&lt;p&gt;Chapter &lt;a href=&#34;chap-ex2.html#chap:ex2&#34;&gt;4&lt;/a&gt; builds upon previous chapters’ intuitions to compare the contextual embeddings generated from a single corpus by multiple models trained on the different complexity-related tasks. First, a set of assumptions is formulated to guide the empirical evaluation of how models encode complexity properties after fine-tuning. Similarity scores are then computed layer-wise across language models using two interpretability approaches to evaluate whether the information shared across different complexity perspectives is encoded by models with different fine-tuning objectives. Finally, learned representations are compared across model layers and fine-tuning tasks to highlight whether and how fine-tuning objectives influence the abstraction hierarchy learned by language models.&lt;/p&gt;
&lt;p&gt;Chapter &lt;a href=&#34;chap-ex3.html#chap:ex3&#34;&gt;5&lt;/a&gt; concludes the experimental portion of this work by studying the connection between eye-tracking metrics and language modeling surprisal and investigating whether gaze metrics fine-tuning can enable language models to individuate cognitive processing triggers like garden-path sentences. A data-driven strategy is first adopted to establish a conversion coefficient between surprisal units and reading times. This coefficient is then used to evaluate whether a model that correctly highlights increased cognitive processing in specific constructions can also predict the magnitude of such phenomena. Autoregressive and masked language models are fine-tuned on eye-tracking measurements and then leveraged in a zero-shot setting to evaluate their ability in replicating garden-path effects in a controlled setting. Finally, models’ performances are evaluated on a set of psycholinguistic benchmarks using surprisal and gaze recordings predictions to estimate the presence and magnitude of garden-path effects.&lt;/p&gt;
&lt;p&gt;While studies on natural language complexity usually adopt a cross-lingual perspective, either by performing typological comparisons across language families or studying the impact of interlingual contacts on complexity changes, this work focuses solely on analyzing complexity annotations produced by native speakers of English. The English language was selected due to the broad availability of open-source corpora and resources, and no other languages were included in the study to keep it as self-contained as possible. Readers should be aware that English is widely considered morphologically and inflectionally poor despite its ubiquity in language studies, even compared to its Indo-European siblings. It should thus be avoided to generalize the results of this thesis work to other language families and typologies.&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; Moreover, this study focuses on the written language paradigm, but the importance of phonological phenomena in spoken language in evaluating language complexity is acknowledged &lt;span class=&#34;citation&#34;&gt;(McWhorter &lt;a href=&#34;#ref-mcwhorter-2001-world&#34;&gt;2001&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;This thesis work should be regarded as a broad, high-level exploration of multiple linguistic complexity perspectives employing modern computational approaches. In this sense, both introductory and experimental chapters are not intended to be exhaustive in providing a complete overview of the discussed topics. Instead, they aim to provide the minimal context needed to interpret experimental results correctly. Introductory chapters include pointers to additional resources discussing linguistic complexity for curious readers, and future studies on these topics will likely encompass any other perspective that was not covered by the present work.&lt;/p&gt;


&lt;!-- Needed for leaving space to the quote, * is for no indentation after title --&gt;

&lt;/div&gt;
&lt;h3&gt;References&lt;/h3&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-mcwhorter-2001-world&#34;&gt;
&lt;p&gt;McWhorter, John H. 2001. “The Worlds Simplest Grammars Are Creole Grammars.” &lt;em&gt;Linguistic Typology&lt;/em&gt; 5 (2-3). De Gruyter Mouton: 125–66.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol start=&#34;1&#34;&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;See &lt;span class=&#34;citation&#34;&gt;Ruder (&lt;a href=&#34;#ref-ruder-2020-beyond&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt; for the importance of multilingual studies in NLP.&lt;a href=&#34;introduction.html#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
            &lt;/section&gt;

          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;

&lt;a href=&#34;chap-ling-comp.html&#34; class=&#34;navigation navigation-next navigation-unique&#34; aria-label=&#34;Next page&#34;&gt;&lt;i class=&#34;fa fa-angle-right&#34;&gt;&lt;/i&gt;&lt;/a&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/app.min.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/lunr.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/clipboard.min.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/plugin-search.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/plugin-sharing.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/plugin-fontsettings.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/plugin-bookdown.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/jquery.highlight.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/plugin-clipboard.js&#34;&gt;&lt;/script&gt;
&lt;script&gt;
gitbook.require([&#34;gitbook&#34;], function(gitbook) {
gitbook.start({
&#34;sharing&#34;: {
&#34;github&#34;: true,
&#34;facebook&#34;: true,
&#34;twitter&#34;: true,
&#34;linkedin&#34;: true,
&#34;weibo&#34;: false,
&#34;instapaper&#34;: false,
&#34;vk&#34;: false,
&#34;all&#34;: false
},
&#34;fontsettings&#34;: {
&#34;theme&#34;: &#34;white&#34;,
&#34;family&#34;: &#34;sans&#34;,
&#34;size&#34;: 2
},
&#34;edit&#34;: {
&#34;link&#34;: &#34;https://github.com/gsarti/master-thesis/tree/master/Index.Rmd&#34;,
&#34;text&#34;: &#34;Edit&#34;
},
&#34;history&#34;: {
&#34;link&#34;: null,
&#34;text&#34;: null
},
&#34;view&#34;: {
&#34;link&#34;: null,
&#34;text&#34;: null
},
&#34;download&#34;: [[&#34;Sarti_2020_Interpreting_NLMs_for_LCA.pdf&#34;, &#34;PDF&#34;]],
&#34;toc&#34;: {
&#34;collapse&#34;: &#34;subsection&#34;,
&#34;scroll_highlight&#34;: true
},
&#34;info&#34;: false
});
});
&lt;/script&gt;

&lt;!-- dynamically load mathjax for compatibility with self-contained --&gt;
&lt;script&gt;
  (function () {
    var script = document.createElement(&#34;script&#34;);
    script.type = &#34;text/javascript&#34;;
    var src = &#34;true&#34;;
    if (src === &#34;&#34; || src === &#34;true&#34;) src = &#34;https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML&#34;;
    if (location.protocol !== &#34;file:&#34;)
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, &#39;&#39;);
    script.src = src;
    document.getElementsByTagName(&#34;head&#34;)[0].appendChild(script);
  })();
&lt;/script&gt;
&lt;/body&gt;

&lt;/html&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>http://localhost:1313/msc-thesis/references/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/msc-thesis/references/</guid>
      <description>&lt;!DOCTYPE html&gt;
&lt;html lang=&#34;&#34; xml:lang=&#34;&#34;&gt;
&lt;head&gt;

  &lt;meta charset=&#34;utf-8&#34; /&gt;
  &lt;meta http-equiv=&#34;X-UA-Compatible&#34; content=&#34;IE=edge&#34; /&gt;
  &lt;title&gt;References | Interpreting Neural Language Models for Linguistic Complexity Assessment&lt;/title&gt;
  &lt;meta name=&#34;description&#34; content=&#34;MSc Thesis at the University of Trieste and SISSA under the supervision of Prof. Davide Crepaldi and Dott. Felice dell&#39;Orletta&#34; /&gt;
  &lt;meta name=&#34;generator&#34; content=&#34;bookdown 0.20.6 and GitBook 2.6.7&#34; /&gt;

  &lt;meta property=&#34;og:title&#34; content=&#34;References | Interpreting Neural Language Models for Linguistic Complexity Assessment&#34; /&gt;
  &lt;meta property=&#34;og:type&#34; content=&#34;book&#34; /&gt;
  &lt;meta property=&#34;og:url&#34; content=&#34;https://gsarti.com/master-thesis&#34; /&gt;
  &lt;meta property=&#34;og:image&#34; content=&#34;https://gsarti.com/master-thesisfigures/cover.png&#34; /&gt;
  &lt;meta property=&#34;og:description&#34; content=&#34;MSc Thesis at the University of Trieste and SISSA under the supervision of Prof. Davide Crepaldi and Dott. Felice dell&#39;Orletta&#34; /&gt;
  &lt;meta name=&#34;github-repo&#34; content=&#34;gsarti/interpreting-complexity&#34; /&gt;

  &lt;meta name=&#34;twitter:card&#34; content=&#34;summary&#34; /&gt;
  &lt;meta name=&#34;twitter:title&#34; content=&#34;References | Interpreting Neural Language Models for Linguistic Complexity Assessment&#34; /&gt;
  
  &lt;meta name=&#34;twitter:description&#34; content=&#34;MSc Thesis at the University of Trieste and SISSA under the supervision of Prof. Davide Crepaldi and Dott. Felice dell&#39;Orletta&#34; /&gt;
  &lt;meta name=&#34;twitter:image&#34; content=&#34;https://gsarti.com/master-thesisfigures/cover.png&#34; /&gt;

&lt;meta name=&#34;author&#34; content=&#34;Gabriele Sarti&#34; /&gt;



  &lt;meta name=&#34;viewport&#34; content=&#34;width=device-width, initial-scale=1&#34; /&gt;
  &lt;meta name=&#34;apple-mobile-web-app-capable&#34; content=&#34;yes&#34; /&gt;
  &lt;meta name=&#34;apple-mobile-web-app-status-bar-style&#34; content=&#34;black&#34; /&gt;
  &lt;link rel=&#34;apple-touch-icon-precomposed&#34; sizes=&#34;152x152&#34; href=&#34;figures/icons/apple-icon.png&#34; /&gt;
  &lt;link rel=&#34;shortcut icon&#34; href=&#34;figures/icons/favicon.ico&#34; type=&#34;image/x-icon&#34; /&gt;
&lt;link rel=&#34;prev&#34; href=&#34;app-params.html&#34;/&gt;

&lt;style type=&#34;text/css&#34;&gt;
p.abstract{
  text-align: center;
  font-weight: bold;
}
div.abstract{
  margin: auto;
  width: 90%;
}
&lt;/style&gt;
&lt;script src=&#34;libs/jquery-2.2.3/jquery.min.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;libs/gitbook-2.6.7/css/style.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;link href=&#34;libs/gitbook-2.6.7/css/plugin-table.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;link href=&#34;libs/gitbook-2.6.7/css/plugin-bookdown.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;link href=&#34;libs/gitbook-2.6.7/css/plugin-highlight.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;link href=&#34;libs/gitbook-2.6.7/css/plugin-search.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;link href=&#34;libs/gitbook-2.6.7/css/plugin-fontsettings.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;link href=&#34;libs/gitbook-2.6.7/css/plugin-clipboard.css&#34; rel=&#34;stylesheet&#34; /&gt;









&lt;script src=&#34;libs/kePrint-0.0.1/kePrint.js&#34;&gt;&lt;/script&gt;



&lt;link rel=&#34;stylesheet&#34; href=&#34;templates/style.css&#34; type=&#34;text/css&#34; /&gt;
&lt;/head&gt;

&lt;body&gt;



  &lt;div class=&#34;book without-animation with-summary font-size-2 font-family-1&#34; data-basepath=&#34;.&#34;&gt;

    &lt;div class=&#34;book-summary&#34;&gt;
      &lt;nav role=&#34;navigation&#34;&gt;

&lt;ul class=&#34;summary&#34;&gt;
&lt;li&gt;&lt;a href=&#34;introduction.html#introduction&#34;&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1&lt;/b&gt; &lt;strong&gt;Linguistic Complexity&lt;/strong&gt;&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.1&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subchap:categorizing&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.1&lt;/b&gt; Categorizing Linguistic Complexity Measures&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.2&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subchap:intrinsic&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.2&lt;/b&gt; Intrinsic Perspective&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.2.1&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subsubchap:structural&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.2.1&lt;/b&gt; Structural Linguistic Complexity&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.2.2&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subsubchap:lm-surprisal&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.2.2&lt;/b&gt; Language Modeling Surprisal&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.3&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subchap:extrinsic&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.3&lt;/b&gt; Extrinsic Perspective&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.3.1&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subsubchap:readability&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.3.1&lt;/b&gt; Automatic Readability Assessment&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.3.2&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subsubchap:pc&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.3.2&lt;/b&gt; Perceived Complexity Prediction&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.3.3&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subsubchap:eye-tracking&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.3.3&lt;/b&gt; Gaze Metrics Prediction&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;1.4&#34; data-path=&#34;chap-ling-comp.html&#34;&gt;&lt;a href=&#34;chap-ling-comp.html#subchap:garden-path&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;1.4&lt;/b&gt; Garden-path Sentences&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2&lt;/b&gt; &lt;strong&gt;Models of Linguistic Complexity&lt;/strong&gt;&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2.1&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html#subchap:desiderata&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2.1&lt;/b&gt; Desiderata for Models of Linguistic Complexity&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2.2&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html#subchap:nlm&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2.2&lt;/b&gt; Neural Language Models: Unsupervised Multitask Learners&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2.2.1&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html#subsubchap:syntax-nlm&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2.2.1&lt;/b&gt; Emergent Linguistic Structures in Neural Language Models&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2.3&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html#subchap:analyzing-nlm&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2.3&lt;/b&gt; Analyzing Neural Models of Complexity&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2.3.1&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html#subsubchap:probe&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2.3.1&lt;/b&gt; Probing classifiers&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2.3.2&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html#subsubchap:rsa&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2.3.2&lt;/b&gt; Representational Similarity Analysis&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;2.3.3&#34; data-path=&#34;chap-models.html&#34;&gt;&lt;a href=&#34;chap-models.html#subsubchap:pwcca&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;2.3.3&lt;/b&gt; Projection-Weighted Canonical Correlation Analysis&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3&lt;/b&gt; &lt;strong&gt;Complexity Phenomena in Linguistic Annotations and Language Models&lt;/strong&gt;&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3.1&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html#subchap:ex1-data&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3.1&lt;/b&gt; Data and Preprocessing&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3.2&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html#subchap:ex1-analysis&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3.2&lt;/b&gt; Analysis of Linguistic Phenomena&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3.2.1&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html#subsubchap:ex1-analysis-bins&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3.2.1&lt;/b&gt; Linguistic Phenomena in Length-controlled Bins&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3.3&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html#subchap:ex1-modeling&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3.3&lt;/b&gt; Modeling Online and Offline Linguistic Complexity&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3.3.1&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html#subsubchap:ex1-modeling-bins&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3.3.1&lt;/b&gt; Modeling Complexity in Length-controlled Bins&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3.4&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html#subchap:ex1-probing&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3.4&lt;/b&gt; Probing Linguistic Phenomena in ALBERT Representations&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;3.5&#34; data-path=&#34;chap-ex1.html&#34;&gt;&lt;a href=&#34;chap-ex1.html#subchap:ex1-summary&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;3.5&lt;/b&gt; Summary&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;4&#34; data-path=&#34;chap-ex2.html&#34;&gt;&lt;a href=&#34;chap-ex2.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;4&lt;/b&gt; &lt;strong&gt;Representational Similarity in Models of Complexity&lt;/strong&gt;&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;4.1&#34; data-path=&#34;chap-ex2.html&#34;&gt;&lt;a href=&#34;chap-ex2.html#knowledge-driven-requirements-for-learning-models&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;4.1&lt;/b&gt; Knowledge-driven Requirements for Learning Models&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;4.2&#34; data-path=&#34;chap-ex2.html&#34;&gt;&lt;a href=&#34;chap-ex2.html#subchap:ex2-experiments&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;4.2&lt;/b&gt; Experimentsl Evaluation&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;4.2.1&#34; data-path=&#34;chap-ex2.html&#34;&gt;&lt;a href=&#34;chap-ex2.html#subsubchap:ex2-data&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;4.2.1&lt;/b&gt; Data&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;4.2.2&#34; data-path=&#34;chap-ex2.html&#34;&gt;&lt;a href=&#34;chap-ex2.html#subsubchap:ex2-inter&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;4.2.2&lt;/b&gt; Inter-model Representational Similarity&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;4.2.3&#34; data-path=&#34;chap-ex2.html&#34;&gt;&lt;a href=&#34;chap-ex2.html#subsubchap:ex2-intra&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;4.2.3&lt;/b&gt; Intra-model Representational Similarity&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;4.3&#34; data-path=&#34;chap-ex2.html&#34;&gt;&lt;a href=&#34;chap-ex2.html#subchap:ex2-summary&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;4.3&lt;/b&gt; Summary&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;5&#34; data-path=&#34;chap-ex3.html&#34;&gt;&lt;a href=&#34;chap-ex3.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;5&lt;/b&gt; &lt;strong&gt;Gaze-informed Models for Cognitive Processing Prediction&lt;/strong&gt;&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;5.1&#34; data-path=&#34;chap-ex3.html&#34;&gt;&lt;a href=&#34;chap-ex3.html#subchap:ex3-setup&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;5.1&lt;/b&gt; Experimental Setup&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;5.2&#34; data-path=&#34;chap-ex3.html&#34;&gt;&lt;a href=&#34;chap-ex3.html#subchap:ex3-experiments&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;5.2&lt;/b&gt; Experimental Evaluation&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;5.2.1&#34; data-path=&#34;chap-ex3.html&#34;&gt;&lt;a href=&#34;chap-ex3.html#subsubchap:ex3-magnitudes&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;5.2.1&lt;/b&gt; Estimating Magnitudes of Garden-path Delays&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;5.2.2&#34; data-path=&#34;chap-ex3.html&#34;&gt;&lt;a href=&#34;chap-ex3.html#subsubchap:ex3-predicting&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;5.2.2&lt;/b&gt; Predicting Delays with Surprisal and Gaze Metrics&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;5.3&#34; data-path=&#34;chap-ex3.html&#34;&gt;&lt;a href=&#34;chap-ex3.html#subchap:ex3-summary&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;5.3&lt;/b&gt; Summary&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;conclusion.html#conclusion&#34;&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;&#34; data-path=&#34;conclusion.html&#34;&gt;&lt;a href=&#34;conclusion.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;Broader Impact and Ethical Perspectives&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;&#34; data-path=&#34;conclusion.html&#34;&gt;&lt;a href=&#34;conclusion.html#future-directions&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;Future Directions&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;appendix&#34;&gt;&lt;span&gt;&lt;b&gt;Appendix&lt;/b&gt;&lt;/span&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;A&#34; data-path=&#34;app-ling-feats.html&#34;&gt;&lt;a href=&#34;app-ling-feats.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;A&lt;/b&gt; Linguistic Features&lt;/a&gt;&lt;ul&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;A.1&#34; data-path=&#34;app-ling-feats.html&#34;&gt;&lt;a href=&#34;app-ling-feats.html#raw-text-properties-and-lexical-variety&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;A.1&lt;/b&gt; Raw Text Properties and Lexical Variety&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;A.2&#34; data-path=&#34;app-ling-feats.html&#34;&gt;&lt;a href=&#34;app-ling-feats.html#morpho-syntacting-information&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;A.2&lt;/b&gt; Morpho-syntacting Information&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;A.3&#34; data-path=&#34;app-ling-feats.html&#34;&gt;&lt;a href=&#34;app-ling-feats.html#verbal-predicate-structure&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;A.3&lt;/b&gt; Verbal Predicate Structure&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;A.4&#34; data-path=&#34;app-ling-feats.html&#34;&gt;&lt;a href=&#34;app-ling-feats.html#global-and-local-parsed-tree-structures&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;A.4&lt;/b&gt; Global and Local Parsed Tree Structures&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;A.5&#34; data-path=&#34;app-ling-feats.html&#34;&gt;&lt;a href=&#34;app-ling-feats.html#syntactic-relations&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;A.5&lt;/b&gt; Syntactic Relations&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;A.6&#34; data-path=&#34;app-ling-feats.html&#34;&gt;&lt;a href=&#34;app-ling-feats.html#subordination-phenomena&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;A.6&lt;/b&gt; Subordination Phenomena&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;B&#34; data-path=&#34;app-et-metrics.html&#34;&gt;&lt;a href=&#34;app-et-metrics.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;B&lt;/b&gt; Precisions on Eye-tracking Metrics and Preprocessing&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;C&#34; data-path=&#34;app-et-modeling.html&#34;&gt;&lt;a href=&#34;app-et-modeling.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;C&lt;/b&gt; Multi-task Token-level Regression for Gaze Metrics Prediction&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;D&#34; data-path=&#34;app-intra-sim.html&#34;&gt;&lt;a href=&#34;app-intra-sim.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;D&lt;/b&gt; Intra-model Similarity for All Models&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;E&#34; data-path=&#34;app-garden-paths-et.html&#34;&gt;&lt;a href=&#34;app-garden-paths-et.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;E&lt;/b&gt; Gaze Metrics Predictions for Garden Path Sentences&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;F&#34; data-path=&#34;app-params.html&#34;&gt;&lt;a href=&#34;app-params.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;&lt;b&gt;F&lt;/b&gt; Reproducibility and Environmental Impact&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;chapter&#34; data-level=&#34;&#34; data-path=&#34;references.html&#34;&gt;&lt;a href=&#34;references.html&#34;&gt;&lt;i class=&#34;fa fa-check&#34;&gt;&lt;/i&gt;References&lt;/a&gt;&lt;/li&gt;
&lt;li class=&#34;divider&#34;&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://gsarti.com&#34;&gt;Back to my website&lt;/a&gt;&lt;/li&gt;

&lt;/ul&gt;

      &lt;/nav&gt;
    &lt;/div&gt;

    &lt;div class=&#34;book-body&#34;&gt;
      &lt;div class=&#34;body-inner&#34;&gt;
        &lt;div class=&#34;book-header&#34; role=&#34;navigation&#34;&gt;
          &lt;h1&gt;
            &lt;i class=&#34;fa fa-circle-o-notch fa-spin&#34;&gt;&lt;/i&gt;&lt;a href=&#34;./&#34;&gt;Interpreting Neural Language Models&lt;br /&gt;
for Linguistic Complexity Assessment&lt;/a&gt;
          &lt;/h1&gt;
        &lt;/div&gt;

        &lt;div class=&#34;page-wrapper&#34; tabindex=&#34;-1&#34; role=&#34;main&#34;&gt;
          &lt;div class=&#34;page-inner&#34;&gt;

            &lt;section class=&#34;normal&#34; id=&#34;section-&#34;&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;!-- If you&#39;re outputting to LaTeX, the heading and references will be generated by the OxThesis LaTeX template. This .Rmd file serves only to add the References headline to gitbook output before  the references are added by pandoc --&gt;

&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div&gt;
&lt;p&gt;Abdou, Mostafa, Artur Kulmizev, Felix Hill, Daniel M. Low, and Anders Søgaard. 2019. “Higher-Order Comparisons of Sentence Encoder Representations.” In &lt;em&gt;Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (Emnlp-Ijcnlp)&lt;/em&gt;, 5838–45. Hong Kong, China: Association for Computational Linguistics. &lt;a href=&#34;https://doi.org/10.18653/v1/D19-1593&#34;&gt;https://doi.org/10.18653/v1/D19-1593&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Abnar, Samira. 2020. “Visualizing Model Comparison.” &lt;em&gt;Blog Post&lt;/em&gt;. &lt;a href=&#34;https://samiraabnar.github.io/articles/2020-05/vizualization&#34;&gt;https://samiraabnar.github.io/articles/2020-05/vizualization&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Abnar, Samira, Lisa Beinborn, Rochelle Choenni, and Willem Zuidema. 2019. “Blackbox Meets Blackbox: Representational Similarity &amp;amp; Stability Analysis of Neural Language Models and Brains.” In &lt;em&gt;Proceedings of the 2019 Acl Workshop Blackboxnlp: Analyzing and Interpreting Neural Networks for Nlp&lt;/em&gt;, 191–203. Florence, Italy: Association for Computational Linguistics. &lt;a href=&#34;https://doi.org/10.18653/v1/W19-4820&#34;&gt;https://doi.org/10.18653/v1/W19-4820&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Abnar, Samira, Mostafa Dehghani, and Willem Zuidema. 2020. “Transferring Inductive Biases Through Knowledge Distillation.” &lt;em&gt;ArXiv Pre-Print&lt;/em&gt; 2006.00555. &lt;a href=&#34;https://arxiv.org/abs/2006.00555&#34;&gt;https://arxiv.org/abs/2006.00555&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Alain, Guillaume, and Yoshua Bengio. 2016. “Understanding Intermediate Layers Using Linear Classifier Probes.” &lt;em&gt;ArXiv Pre-Print&lt;/em&gt; 1610.01644. &lt;a href=&#34;https://arxiv.org/abs/1610.01644&#34;&gt;https://arxiv.org/abs/1610.01644&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Alammar, Jay. 2018a. “The Illustrated Bert, Elmo, and Co. (How NLP Cracked Transfer Learning).” &lt;em&gt;Blog Post&lt;/em&gt;. &lt;a href=&#34;https://jalammar.github.io/illustrated-bert/&#34;&gt;https://jalammar.github.io/illustrated-bert/&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;———. 2018b. “The Illustrated Gpt-2.” &lt;em&gt;Blog Post&lt;/em&gt;. &lt;a href=&#34;https://http://jalammar.github.io/illustrated-gpt2/&#34;&gt;https://http://jalammar.github.io/illustrated-gpt2/&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Ambati, Bharat Ram, Siva Reddy, and Mark Steedman. 2016. “Assessing Relative Sentence Complexity Using an Incremental CCG Parser.” In &lt;em&gt;Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies&lt;/em&gt;, 1051–7. San Diego, California: Association for Computational Linguistics. &lt;a href=&#34;https://doi.org/10.18653/v1/N16-1120&#34;&gt;https://doi.org/10.18653/v1/N16-1120&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Andreas, Jacob, and Dan Klein. 2014. “How Much Do Word Embeddings Encode About Syntax?” In &lt;em&gt;Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)&lt;/em&gt;, 822–27. Baltimore, Maryland: Association for Computational Linguistics. &lt;a href=&#34;https://doi.org/10.3115/v1/P14-2133&#34;&gt;https://doi.org/10.3115/v1/P14-2133&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Ba, Jimmy, J. Kiros, and Geoffrey E. Hinton. 2016. “Layer Normalization.” &lt;em&gt;ArXiv Pre-Print&lt;/em&gt; 1607.06450. &lt;a href=&#34;https://arxiv.org/abs/1607.06450&#34;&gt;https://arxiv.org/abs/1607.06450&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Bahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio. 2015. “Neural Machine Translation by Jointly Learning to Align and Translate.” In &lt;em&gt;Proceeding of the 3rd International Conference on Learning Representations (ICLR’15)&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Barrett, Maria, Joachim Bingel, Nora Hollenstein, Marek Rei, and Anders Søgaard. 2018. “Sequence Classification with Human Attention.” In &lt;em&gt;Proceedings of the 22nd Conference on Computational Natural Language Learning&lt;/em&gt;, 302–12. Brussels, Belgium: Association for Computational Linguistics. &lt;a href=&#34;https://doi.org/10.18653/v1/K18-1030&#34;&gt;https://doi.org/10.18653/v1/K18-1030&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Barrett, Maria, Joachim Bingel, Frank Keller, and Anders Søgaard. 2016. “Weakly Supervised Part-of-Speech Tagging Using Eye-Tracking Data.” In &lt;em&gt;Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)&lt;/em&gt;, 579–84. Berlin, Germany: Association for Computational Linguistics. &lt;a href=&#34;https://doi.org/10.18653/v1/P16-2094&#34;&gt;https://doi.org/10.18653/v1/P16-2094&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Belinkov, Yonatan, Sebastian Gehrmann, and Ellie Pavlick. 2020. “Interpretability and Analysis in Neural NLP.” In &lt;em&gt;Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts&lt;/em&gt;, 1–5. Online: Association for Computational Linguistics. &lt;a href=&#34;https://doi.org/10.18653/v1/2020.acl-tutorials.1&#34;&gt;https://doi.org/10.18653/v1/2020.acl-tutorials.1&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Belinkov, Yonatan, and James Glass. 2019. “Analysis Methods in Neural Language Processing: A Survey.” &lt;em&gt;Transactions of the Association for Computational Linguistics (TACL)&lt;/em&gt; 7: 49–72. &lt;a href=&#34;https://doi.org/10.1162/tacl\_a\_00254&#34;&gt;https://doi.org/10.1162/tacl\_a\_00254&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Bender, Emily M., and Alexander Koller. 2020. “Climbing Towards NLU: On Meaning, Form, and Understanding in the Age of Data.” In &lt;em&gt;Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics&lt;/em&gt;, 5185–98. Online: Association for Computational Linguistics. &lt;a href=&#34;https://doi.org/10.18653/v1/2020.acl-main.463&#34;&gt;https://doi.org/10.18653/v1/2020.acl-main.463&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Berruto, Gaetano, and Massimo Simone Cerruti. 2011. &lt;em&gt;La Linguistica. Un Corso Introduttivo&lt;/em&gt;. De Agostini.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Berzak, Yevgeni, Boris Katz, and Roger Levy. 2018. “Assessing Language Proficiency from Eye Movements in Reading.” In &lt;em&gt;Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)&lt;/em&gt;, 1986–96. New Orleans, Louisiana: Association for Computational Linguistics. &lt;a href=&#34;https://doi.org/10.18653/v1/N18-1180&#34;&gt;https://doi.org/10.18653/v1/N18-1180&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Bever, Thomas G. 1970. “The Cognitive Basis for Linguistic Structures.” &lt;em&gt;Cognition and the Development of Language&lt;/em&gt;. Wiley.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Box, George EP. 1976. “Science and Statistics.” &lt;em&gt;Journal of the American Statistical Association&lt;/em&gt; 71 (356). Taylor &amp;amp; Francis: 791–99.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Brunato, Dominique, Andrea Cimino, Felice Dell’Orletta, Giulia Venturi, and Simonetta Montemagni. 2020. “Profiling-UD: A Tool for Linguistic Profiling of Texts.” In &lt;em&gt;Proceedings of the 12th Language Resources and Evaluation Conference&lt;/em&gt;, 7145–51. Marseille, France: European Language Resources Association. &lt;a href=&#34;https://www.aclweb.org/anthology/2020.lrec-1.883&#34;&gt;https://www.aclweb.org/anthology/2020.lrec-1.883&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Brunato, Dominique, Lorenzo De Mattei, Felice Dell’Orletta, Benedetta Iavarone, and Giulia Venturi. 2018. “Is This Sentence Difficult? Do You Agree?” In &lt;em&gt;Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing&lt;/em&gt;, 2690–9. Brussels, Belgium: Association for Computational Linguistics. &lt;a href=&#34;https://doi.org/10.18653/v1/D18-1289&#34;&gt;https://doi.org/10.18653/v1/D18-1289&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Cangelosi, Angelo, and Huck Turner. 2002. “L’emergere Del Linguaggio.” &lt;em&gt;Scienze Della Mente&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Carr, Jon W, Valentina N Pescuma, Michele Furlan, Maria Ktori, and Davide Crepaldi. 2020. “Algorithms for the Automated Correction of Vertical Drift in Eye Tracking Data.” &lt;em&gt;OSF Preprints&lt;/em&gt;, June. &lt;a href=&#34;osf.io/jg3nc&#34;&gt;osf.io/jg3nc&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Caruana, Rich. 1997. “Multitask Learning.” &lt;em&gt;Machine Learning&lt;/em&gt; 28: 41–75. &lt;a href=&#34;https://www.cs.utexas.edu/~kuipers/readings/Caruana-mlj-97.pdf&#34;&gt;https://www.cs.utexas.edu/~kuipers/readings/Caruana-mlj-97.pdf&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Christie, Agatha. 2003. &lt;em&gt;The Mysterious Affair at Styles: A Detective Story&lt;/em&gt;. Modern Library.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Collins-Thompson, Kevyn. 2014. “Computational Assessment of Text Readability: A Survey of Current and Future Research.” &lt;em&gt;ITL-International Journal of Applied Linguistics&lt;/em&gt; 165 (2). John Benjamins: 97–135. &lt;a href=&#34;http://www-personal.umich.edu/~kevynct/pubs/ITL-readability-invited-article-v10-camera.pdf&#34;&gt;http://www-personal.umich.edu/~kevynct/pubs/ITL-readability-invited-article-v10-camera.pdf&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Conneau, Alexis, German Kruszewski, Guillaume Lample, Loı̈c Barrault, and Marco Baroni. 2018. “What You Can Cram into a Single $&amp;amp;!#* Vector: Probing Sentence Embeddings for Linguistic Properties.” In &lt;em&gt;Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)&lt;/em&gt;, 2126–36. Melbourne, Australia: Association for Computational Linguistics. &lt;a href=&#34;https://doi.org/10.18653/v1/P18-1198&#34;&gt;https://doi.org/10.18653/v1/P18-1198&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Cop, Uschi, Nicolas Dirix, Denis Drieghe, and Wouter Duyck. 2017. “Presenting Geco: An Eyetracking Corpus of Monolingual and Bilingual Sentence Reading.” &lt;em&gt;Behavior Research Methods&lt;/em&gt; 49 (2). Springer: 602–15.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Culotta, Aron, Andrew McCallum, and Jonathan Betz. 2006. “Integrating Probabilistic Extraction Models and Data Mining to Discover Relations and Patterns in Text.” In &lt;em&gt;Proceedings of the Human Language Technology Conference of the NAACL, Main Conference&lt;/em&gt;, 296–303. New York City, USA: Association for Computational Linguistics. &lt;a href=&#34;https://www.aclweb.org/anthology/N06-1038&#34;&gt;https://www.aclweb.org/anthology/N06-1038&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Day, Matthew. 2004. “Religion, Off-Line Cognition and the Extended Mind.” &lt;em&gt;Journal of Cognition and Culture&lt;/em&gt; 4 (1). Brill: 101–21.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Demberg, Vera, and Frank Keller. 2008. “Data from Eye-Tracking Corpora as Evidence for Theories of Syntactic Processing Complexity.” &lt;em&gt;Cognition&lt;/em&gt; 109 (2). Elsevier: 193–210.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Devlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. “BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding.” In &lt;em&gt;Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)&lt;/em&gt;, 4171–86. Minneapolis, Minnesota: Association for Computational Linguistics. &lt;a href=&#34;https://doi.org/10.18653/v1/N19-1423&#34;&gt;https://doi.org/10.18653/v1/N19-1423&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Edmonds, Bruce M. 1999. “Syntactic Measures of Complexity.” PhD thesis, University of Manchester Manchester, UK.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Eisape, Tiwalayo, Noga Zaslavsky, and Roger Levy. 2020. “Cloze Distillation Improves Psychometric Predictive Power.” In &lt;em&gt;Proceedings of the 24th Conference on Computational Natural Language Learning&lt;/em&gt;, 609–19. Online: Association for Computational Linguistics. &lt;a href=&#34;https://www.aclweb.org/anthology/2020.conll-1.49&#34;&gt;https://www.aclweb.org/anthology/2020.conll-1.49&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Eisenstein, Jacob. 2019. &lt;em&gt;Introduction to Natural Language Processing&lt;/em&gt;. MIT press.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Elman, Jeffrey L. 1991. “Distributed Representations, Simple Recurrent Networks, and Grammatical Structure.” &lt;em&gt;Machine Learning&lt;/em&gt; 7 (2-3). Springer: 195–225.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Ettinger, Allyson. 2020. “What BERT Is Not: Lessons from a New Suite of Psycholinguistic Diagnostics for Language Models.” &lt;em&gt;Transactions of the Association for Computational Linguistics&lt;/em&gt; 8: 34–48. &lt;a href=&#34;https://doi.org/10.1162/tacl_a_00298&#34;&gt;https://doi.org/10.1162/tacl_a_00298&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Fine, Alex B, T Florian Jaeger, Thomas A Farmer, and Ting Qian. 2013. “Rapid Expectation Adaptation During Syntactic Comprehension.” &lt;em&gt;PloS One&lt;/em&gt; 8 (10). Public Library of Science: e77661.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Frankle, Jonathan, and Michael Carbin. 2018. “The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks.” In &lt;em&gt;Proceedings of the 8th International Conference on Learning Representations (Iclr’18)&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Frazier, Lyn. 1978. “On Comprehending Sentences: Syntactic Parsing Strategies.” PhD thesis, University of Connecticut.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Frazier, Lyn, and Janet Dean Fodor. 1978. “The Sausage Machine: A New Two-Stage Parsing Model.” &lt;em&gt;Cognition&lt;/em&gt; 6 (4). Elsevier: 291–325.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Futrell, Richard, Edward Gibson, and Roger P Levy. 2020. “Lossy-Context Surprisal: An Information-Theoretic Model of Memory Effects in Sentence Processing.” &lt;em&gt;Cognitive Science&lt;/em&gt; 44 (3). Wiley Online Library: e12814.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Futrell, Richard, Ethan Wilcox, Takashi Morita, Peng Qian, Miguel Ballesteros, and Roger Levy. 2019. “Neural Language Models as Psycholinguistic Subjects: Representations of Syntactic State.” In &lt;em&gt;Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)&lt;/em&gt;, 32–42. Minneapolis, Minnesota: Association for Computational Linguistics. &lt;a href=&#34;https://doi.org/10.18653/v1/N19-1004&#34;&gt;https://doi.org/10.18653/v1/N19-1004&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Gauthier, Jon, Jennifer Hu, Ethan Wilcox, Peng Qian, and Roger Levy. 2020. “SyntaxGym: An Online Platform for Targeted Evaluation of Language Models.” In &lt;em&gt;Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations&lt;/em&gt;, 70–76. Online: Association for Computational Linguistics. &lt;a href=&#34;https://doi.org/10.18653/v1/2020.acl-demos.10&#34;&gt;https://doi.org/10.18653/v1/2020.acl-demos.10&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Gauthier, Jon, and Roger Levy. 2019. “Linking Artificial and Human Neural Representations of Language.” In &lt;em&gt;Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (Emnlp-Ijcnlp)&lt;/em&gt;, 529–39. Hong Kong, China: Association for Computational Linguistics. &lt;a href=&#34;https://doi.org/10.18653/v1/D19-1050&#34;&gt;https://doi.org/10.18653/v1/D19-1050&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Gibson, Edward. 1991. “A Computational Theory of Human Linguistic Processing: Memory Limitations and Processing Breakdown.” PhD thesis, Pittsburgh, PA: Carnegie Mellon University.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;———. 1998. “Linguistic Complexity: Locality of Syntactic Dependencies.” &lt;em&gt;Cognition&lt;/em&gt; 68 (1). Elsevier: 1–76.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;———. 2000. “The Dependency Locality Theory: A Distance-Based Theory of Linguistic Complexity.” &lt;em&gt;Image, Language, Brain&lt;/em&gt; 2000: 95–126.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Glavas, Goran, and Ivan Vulic. 2020. “Is Supervised Syntactic Parsing Beneficial for Language Understanding? An Empirical Investigation.” &lt;em&gt;ArXiv Pre-Print&lt;/em&gt; 2008.06788. &lt;a href=&#34;https://arxiv.org/abs/2008.06788&#34;&gt;https://arxiv.org/abs/2008.06788&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;González-Garduño, Ana Valeria, and Anders Søgaard. 2018. “Learning to Predict Readability Using Eye-Movement Data from Natives and Learners.” AAAI Conference on Artificial Intelligence.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Goodfellow, Ian, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. 2016. &lt;em&gt;Deep Learning&lt;/em&gt;. MIT Press Cambridge.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Goodman, Joshua. 2001. “A Bit of Progress in Language Modeling.” &lt;em&gt;arXiv Preprint Cs/0108005&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Grodner, Daniel, Edward Gibson, Vered Argaman, and Maria Babyonyshev. 2003. “Against Repair-Based Reanalysis in Sentence Comprehension.” &lt;em&gt;Journal of Psycholinguistic Research&lt;/em&gt; 32 (2). Springer: 141–66.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Gulordava, Kristina, Piotr Bojanowski, Edouard Grave, Tal Linzen, and Marco Baroni. 2018. “Colorless Green Recurrent Networks Dream Hierarchically.” In &lt;em&gt;Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)&lt;/em&gt;, 1195–1205. New Orleans, Louisiana: Association for Computational Linguistics. &lt;a href=&#34;https://doi.org/10.18653/v1/N18-1108&#34;&gt;https://doi.org/10.18653/v1/N18-1108&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Hale, John. 2001. “A Probabilistic Earley Parser as a Psycholinguistic Model.” In &lt;em&gt;Second Meeting of the North American Chapter of the Association for Computational Linguistics&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;———. 2016. “Information-Theoretical Complexity Metrics.” &lt;em&gt;Language and Linguistics Compass&lt;/em&gt; 10 (9). Wiley Online Library: 397–412.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Hauser, Marc D, Noam Chomsky, and W Tecumseh Fitch. 2002. “The Faculty of Language: What Is It, Who Has It, and How Did It Evolve?” &lt;em&gt;Science&lt;/em&gt; 298 (5598). American Association for the Advancement of Science: 1569–79.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Hendrycks, Dan, and Kevin Gimpel. 2016. “Gaussian Error Linear Units (Gelus).” &lt;em&gt;ArXiv Pre-Print&lt;/em&gt; 1606.08415. &lt;a href=&#34;https://arxiv.org/abs/1606.08415&#34;&gt;https://arxiv.org/abs/1606.08415&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Hewitt, John, and Percy Liang. 2019. “Designing and Interpreting Probes with Control Tasks.” In &lt;em&gt;Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (Emnlp-Ijcnlp)&lt;/em&gt;, 2733–43. Hong Kong, China: Association for Computational Linguistics. &lt;a href=&#34;https://doi.org/10.18653/v1/D19-1275&#34;&gt;https://doi.org/10.18653/v1/D19-1275&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Hewitt, John, and Christopher D. Manning. 2019. “A Structural Probe for Finding Syntax in Word Representations.” In &lt;em&gt;Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)&lt;/em&gt;, 4129–38. Minneapolis, Minnesota: Association for Computational Linguistics. &lt;a href=&#34;https://doi.org/10.18653/v1/N19-1419&#34;&gt;https://doi.org/10.18653/v1/N19-1419&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Hinton, Geoffrey, Oriol Vinyals, and Jeff Dean. 2015. “Distilling the Knowledge in a Neural Network.” &lt;em&gt;ArXiv Pre-Print&lt;/em&gt; 1503.02531. &lt;a href=&#34;https://arxiv.org/abs/1503.02531&#34;&gt;https://arxiv.org/abs/1503.02531&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Hochreiter, Sepp, and Jürgen Schmidhuber. 1997. “Long Short-Term Memory.” &lt;em&gt;Neural Computation&lt;/em&gt; 9 (8). MIT Press: 1735–80.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Hollenstein, Nora, Maria Barrett, and Lisa Beinborn. 2020. “Towards Best Practices for Leveraging Human Language Processing Signals for Natural Language Processing.” In &lt;em&gt;Proceedings of the Second Workshop on Linguistic and Neurocognitive Resources&lt;/em&gt;, 15–27. Marseille, France: European Language Resources Association. &lt;a href=&#34;https://www.aclweb.org/anthology/2020.lincr-1.3&#34;&gt;https://www.aclweb.org/anthology/2020.lincr-1.3&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Hollenstein, Nora, Jonathan Rotsztejn, Marius Troendle, Andreas Pedroni, Ce Zhang, and Nicolas Langer. 2018. “ZuCo, a Simultaneous Eeg and Eye-Tracking Resource for Natural Sentence Reading.” &lt;em&gt;Scientific Data&lt;/em&gt; 5 (1). Nature Publishing Group: 1–13.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Hollenstein, Nora, Antonio de la Torre, Nicolas Langer, and Ce Zhang. 2019. “CogniVal: A Framework for Cognitive Word Embedding Evaluation.” In &lt;em&gt;Proceedings of the 23rd Conference on Computational Natural Language Learning (Conll)&lt;/em&gt;, 538–49. Hong Kong, China: Association for Computational Linguistics. &lt;a href=&#34;https://doi.org/10.18653/v1/K19-1050&#34;&gt;https://doi.org/10.18653/v1/K19-1050&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Hollenstein, Nora, Marius Troendle, Ce Zhang, and Nicolas Langer. 2020. “ZuCo 2.0: A Dataset of Physiological Recordings During Natural Reading and Annotation.” In &lt;em&gt;Proceedings of the 12th Language Resources and Evaluation Conference&lt;/em&gt;, 138–46. Marseille, France: European Language Resources Association. &lt;a href=&#34;https://www.aclweb.org/anthology/2020.lrec-1.18&#34;&gt;https://www.aclweb.org/anthology/2020.lrec-1.18&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Hollenstein, Nora, and Ce Zhang. 2019. “Entity Recognition at First Sight: Improving NER with Eye Movement Information.” In &lt;em&gt;Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)&lt;/em&gt;, 1–10. Minneapolis, Minnesota: Association for Computational Linguistics. &lt;a href=&#34;https://doi.org/10.18653/v1/N19-1001&#34;&gt;https://doi.org/10.18653/v1/N19-1001&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Howard, Jeremy, and Sebastian Ruder. 2018. “Universal Language Model Fine-Tuning for Text Classification.” In &lt;em&gt;Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)&lt;/em&gt;, 328–39. Melbourne, Australia: Association for Computational Linguistics. &lt;a href=&#34;https://doi.org/10.18653/v1/P18-1031&#34;&gt;https://doi.org/10.18653/v1/P18-1031&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Hu, Jennifer, Jon Gauthier, Peng Qian, Ethan Wilcox, and Roger Levy. 2020. “A Systematic Assessment of Syntactic Generalization in Neural Language Models.” In &lt;em&gt;Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics&lt;/em&gt;, 1725–44. Online: Association for Computational Linguistics. &lt;a href=&#34;https://doi.org/10.18653/v1/2020.acl-main.158&#34;&gt;https://doi.org/10.18653/v1/2020.acl-main.158&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Iverson, Jana M, and Esther Thelen. 1999. “Hand, Mouth and Brain. The Dynamic Emergence of Speech and Gesture.” &lt;em&gt;Journal of Consciousness Studies&lt;/em&gt; 6 (11-12). Imprint Academic: 19–40.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Jawahar, Ganesh, Benoit Sagot, and Djamé Seddah. 2019. “What Does BERT Learn About the Structure of Language?” In &lt;em&gt;Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics&lt;/em&gt;, 3651–7. Florence, Italy: Association for Computational Linguistics. &lt;a href=&#34;https://doi.org/10.18653/v1/P19-1356&#34;&gt;https://doi.org/10.18653/v1/P19-1356&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Jurafsky, Daniel. 1996. “A Probabilistic Model of Lexical and Syntactic Access and Disambiguation.” &lt;em&gt;Cognitive Science&lt;/em&gt; 20 (2). Wiley Online Library: 137–94.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Kennedy, Alan, Robin Hill, and Joël Pynte. 2003. “The Dundee Corpus.” In &lt;em&gt;Proceedings of the 12th European Conference on Eye Movement&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Kriegeskorte, N., M. Mur, and P. Bandettini. 2008. “Representational Similarity Analysis – Connecting the Branches of Systems Neuroscience.” &lt;em&gt;Frontiers in Systems Neuroscience&lt;/em&gt; 2. &lt;a href=&#34;https://doi.org/10.3389/neuro.06.004.2008&#34;&gt;https://doi.org/10.3389/neuro.06.004.2008&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Kudo, Taku, and John Richardson. 2018. “SentencePiece: A Simple and Language Independent Subword Tokenizer and Detokenizer for Neural Text Processing.” In &lt;em&gt;Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations&lt;/em&gt;, 66–71. Brussels, Belgium: Association for Computational Linguistics. &lt;a href=&#34;https://doi.org/10.18653/v1/D18-2012&#34;&gt;https://doi.org/10.18653/v1/D18-2012&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Kusters, Wouter. 2003. “Linguistic Complexity.” PhD thesis, Netherlands Graduate School of Linguistics.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;———. 2008. “Complexity in Linguistic Theory, Language Learning and Language Change.” In &lt;em&gt;Language Complexity: Typology, Contact, Change&lt;/em&gt;, 3–22. John Benjamins Amsterdam, The Netherlands.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Laakso, Aarre, and Garrison Cottrell. 2000. “Content and Cluster Analysis: Assessing Representational Similarity in Neural Systems.” &lt;em&gt;Philosophical Psychology&lt;/em&gt; 13 (1). Taylor &amp;amp; Francis: 47–76.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Lacoste, Alexandre, Alexandra Luccioni, Victor Schmidt, and Thomas Dandres. 2019. “Quantifying the Carbon Emissions of Machine Learning.” &lt;em&gt;ArXiv Pre-Print&lt;/em&gt; 1910.09700.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Lan, Zhenzhong, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. 2020. “ALBERT: A Lite BERT for Self-Supervised Learning of Language Representations.” In &lt;em&gt;International Conference on Learning Representations&lt;/em&gt;. &lt;a href=&#34;https://openreview.net/forum?id=H1eA7AEtvS&#34;&gt;https://openreview.net/forum?id=H1eA7AEtvS&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Levy, Roger. 2008. “Expectation-Based Syntactic Comprehension.” &lt;em&gt;Cognition&lt;/em&gt; 106 (3). Elsevier: 1126–77.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Lin, Yongjie, Yi Chern Tan, and Robert Frank. 2019. “Open Sesame: Getting Inside BERT’s Linguistic Knowledge.” In &lt;em&gt;Proceedings of the 2019 Acl Workshop Blackboxnlp: Analyzing and Interpreting Neural Networks for Nlp&lt;/em&gt;, 241–53. Florence, Italy: Association for Computational Linguistics. &lt;a href=&#34;https://doi.org/10.18653/v1/W19-4825&#34;&gt;https://doi.org/10.18653/v1/W19-4825&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Linzen, Tal, and Marco Baroni. 2021. “Syntactic Structure from Deep Learning.” &lt;em&gt;Annual Review of Linguistics&lt;/em&gt; 7 (1): null. &lt;a href=&#34;https://doi.org/10.1146/annurev-linguistics-032020-051035&#34;&gt;https://doi.org/10.1146/annurev-linguistics-032020-051035&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Linzen, Tal, Emmanuel Dupoux, and Yoav Goldberg. 2016. “Assessing the Ability of Lstms to Learn Syntax-Sensitive Dependencies.” &lt;em&gt;Transactions of the Association for Computational Linguistics&lt;/em&gt; 4. MIT Press: 521–35.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Liu, Nelson F., Matt Gardner, Yonatan Belinkov, Matthew E. Peters, and Noah A. Smith. 2019. “Linguistic Knowledge and Transferability of Contextual Representations.” In &lt;em&gt;Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)&lt;/em&gt;, 1073–94. Minneapolis, Minnesota: Association for Computational Linguistics. &lt;a href=&#34;https://doi.org/10.18653/v1/N19-1112&#34;&gt;https://doi.org/10.18653/v1/N19-1112&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Loshchilov, I., and F. Hutter. 2019. “Decoupled Weight Decay Regularization.” In &lt;em&gt;Proceeding of the 7th International Conference on Learning Representations (Iclr’19)&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Martinc, Matej, S. Pollak, and M. Robnik-Sikonja. 2019. “Supervised and Unsupervised Neural Approaches to Text Readability.” &lt;em&gt;ArXiv Pre-Print&lt;/em&gt; 1907.11779. &lt;a href=&#34;https://arxiv.org/abs/1907.11779&#34;&gt;https://arxiv.org/abs/1907.11779&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;McDonald, Ryan, Joakim Nivre, Yvonne Quirmbach-Brundage, Yoav Goldberg, Dipanjan Das, Kuzman Ganchev, Keith Hall, et al. 2013. “Universal Dependency Annotation for Multilingual Parsing.” In &lt;em&gt;Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)&lt;/em&gt;, 92–97. Sofia, Bulgaria: Association for Computational Linguistics. &lt;a href=&#34;https://www.aclweb.org/anthology/P13-2017&#34;&gt;https://www.aclweb.org/anthology/P13-2017&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;McWhorter, John H. 2001. “The Worlds Simplest Grammars Are Creole Grammars.” &lt;em&gt;Linguistic Typology&lt;/em&gt; 5 (2-3). De Gruyter Mouton: 125–66.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Meyer, Bonnie JF, and G Elizabeth Rice. 1992. “12 Prose Processing in Adulthood: The Text, the Reader, and the Task.” &lt;em&gt;Everyday Cognition in Adulthood and Late Life&lt;/em&gt;. Cambridge Univ Pr, 157.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Miaschi, Alessio, Dominique Brunato, Felice Dell’Orletta, and Giulia Venturi. 2020. “Linguistic Profiling of a Neural Language Model.” In &lt;em&gt;Proceedings of the 28th Conference on Computational Linguistics (Coling)&lt;/em&gt;. Online: Association for Computational Linguistics. &lt;a href=&#34;https://arxiv.org/abs/2010.01869&#34;&gt;https://arxiv.org/abs/2010.01869&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Miaschi, Alessio, and Felice Dell’Orletta. 2020. “Contextual and Non-Contextual Word Embeddings: An in-Depth Linguistic Investigation.” In &lt;em&gt;Proceedings of the 5th Workshop on Representation Learning for Nlp&lt;/em&gt;, 110–19. Online: Association for Computational Linguistics. &lt;a href=&#34;https://www.aclweb.org/anthology/2020.repl4nlp-1.15&#34;&gt;https://www.aclweb.org/anthology/2020.repl4nlp-1.15&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Miestamo, Matti. 2004. “On the Feasibility of Complexity Metrics.” In &lt;em&gt;FinEst Linguistics, Proceedings of the Annual Finnish and Estonian Conference of Linguistics&lt;/em&gt;, 11–26. Tallin, Finland.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;———. 2008. “Grammatical Complexity in a Cross-Linguistic Perspective.” In &lt;em&gt;Language Complexity: Typology, Contact, Change&lt;/em&gt;, 41. John Benjamins Amsterdam, The Netherlands.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Mikolov, Tomas, Kai Chen, G. S. Corrado, and J. Dean. 2013. “Efficient Estimation of Word Representations in Vector Space.” &lt;em&gt;CoRR&lt;/em&gt; abs/1301.3781.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Mikolov, Tomas, M. Karafiát, L. Burget, J. Cernocký, and S. Khudanpur. 2010. “Recurrent Neural Network Based Language Model.” In &lt;em&gt;INTERSPEECH&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Mishra, Abhijit, Kuntal Dey, and Pushpak Bhattacharyya. 2017. “Learning Cognitive Features from Gaze Data for Sentiment and Sarcasm Classification Using Convolutional Neural Network.” In &lt;em&gt;Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)&lt;/em&gt;, 377–87. Vancouver, Canada: Association for Computational Linguistics. &lt;a href=&#34;https://doi.org/10.18653/v1/P17-1035&#34;&gt;https://doi.org/10.18653/v1/P17-1035&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Mitchell, Don C. 1984. “An Evaluation of Subject-Paced Reading Tasks and Other Methods for Investigating Immediate Processes in Reading.” &lt;em&gt;New Methods in Reading Comprehension Research&lt;/em&gt;, 69–89.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Morcos, Ari, Maithra Raghu, and Samy Bengio. 2018. “Insights on Representational Similarity in Neural Networks with Canonical Correlation.” In &lt;em&gt;Advances in Neural Information Processing Systems 31&lt;/em&gt;, edited by S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, 5727–36. Curran Associates, Inc. &lt;a href=&#34;http://papers.nips.cc/paper/7815-insights-on-representational-similarity-in-neural-networks-with-canonical-correlation.pdf&#34;&gt;http://papers.nips.cc/paper/7815-insights-on-representational-similarity-in-neural-networks-with-canonical-correlation.pdf&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Moro, Alberto, and Laura Lonza. 2018. “Electricity Carbon Intensity in European Member States: Impacts on Ghg Emissions of Electric Vehicles.” &lt;em&gt;Transportation Research Part D: Transport and Environment&lt;/em&gt; 64. Elsevier: 5–14.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Munro, Robert, Steven Bethard, Victor Kuperman, Vicky Tzuyin Lai, Robin Melnick, Christopher Potts, Tyler Schnoebelen, and Harry Tily. 2010. “Crowdsourcing and Language Studies: The New Generation of Linguistic Data.” In &lt;em&gt;Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk&lt;/em&gt;, 122–30. Los Angeles: Association for Computational Linguistics. &lt;a href=&#34;https://www.aclweb.org/anthology/W10-0719&#34;&gt;https://www.aclweb.org/anthology/W10-0719&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Nivre, Joakim, Marie-Catherine de Marneffe, Filip Ginter, Yoav Goldberg, Jan Hajič, Christopher D. Manning, Ryan McDonald, et al. 2016. “Universal Dependencies V1: A Multilingual Treebank Collection.” In &lt;em&gt;Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC’16)&lt;/em&gt;, 1659–66. Portorož, Slovenia: European Language Resources Association (ELRA). &lt;a href=&#34;https://www.aclweb.org/anthology/L16-1262&#34;&gt;https://www.aclweb.org/anthology/L16-1262&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Pascanu, R., Tomas Mikolov, and Yoshua Bengio. 2013. “On the Difficulty of Training Recurrent Neural Networks.” In &lt;em&gt;Proceedings of the 30th International Conference on Machine Learning (Icml’13)&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Pennington, Jeffrey, Richard Socher, and Christopher Manning. 2014. “GloVe: Global Vectors for Word Representation.” In &lt;em&gt;Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)&lt;/em&gt;, 1532–43. Doha, Qatar: Association for Computational Linguistics. &lt;a href=&#34;https://doi.org/10.3115/v1/D14-1162&#34;&gt;https://doi.org/10.3115/v1/D14-1162&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Peters, Matthew, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. “Deep Contextualized Word Representations.” In &lt;em&gt;Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)&lt;/em&gt;, 2227–37. New Orleans, Louisiana: Association for Computational Linguistics. &lt;a href=&#34;https://doi.org/10.18653/v1/N18-1202&#34;&gt;https://doi.org/10.18653/v1/N18-1202&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Pimentel, Tiago, Josef Valvoda, Rowan Hall Maudslay, Ran Zmigrod, Adina Williams, and Ryan Cotterell. 2020. “Information-Theoretic Probing for Linguistic Structure.” In &lt;em&gt;Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics&lt;/em&gt;, 4609–22. Online: Association for Computational Linguistics. &lt;a href=&#34;https://doi.org/10.18653/v1/2020.acl-main.420&#34;&gt;https://doi.org/10.18653/v1/2020.acl-main.420&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Prasad, Grusha, and Tal Linzen. 2019a. “Do Self-Paced Reading Studies Provide Evidence for Rapid Syntactic Adaptation?” &lt;em&gt;PsyArXiv Pre-Print&lt;/em&gt;. &lt;a href=&#34;https://tallinzen.net/media/papers/prasad_linzen_2019_adaptation.pdf&#34;&gt;https://tallinzen.net/media/papers/prasad_linzen_2019_adaptation.pdf&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;———. 2019b. “How Much Harder Are Hard Garden-Path Sentences Than Easy Ones?” &lt;em&gt;OSF Preprint&lt;/em&gt; syh3j. &lt;a href=&#34;https://osf.io/syh3j/&#34;&gt;https://osf.io/syh3j/&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Radford, A., Jeffrey Wu, R. Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. “Language Models Are Unsupervised Multitask Learners.” &lt;em&gt;OpenAI Blog&lt;/em&gt;. OpenAI.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Raghu, Maithra, Justin Gilmer, Jason Yosinski, and Jascha Sohl-Dickstein. 2017. “SVCCA: Singular Vector Canonical Correlation Analysis for Deep Learning Dynamics and Interpretability.” In &lt;em&gt;Advances in Neural Information Processing Systems 30&lt;/em&gt;, edited by I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, 6076–85. Curran Associates, Inc. &lt;a href=&#34;http://papers.nips.cc/paper/7188-svcca-singular-vector-canonical-correlation-analysis-for-deep-learning-dynamics-and-interpretability.pdf&#34;&gt;http://papers.nips.cc/paper/7188-svcca-singular-vector-canonical-correlation-analysis-for-deep-learning-dynamics-and-interpretability.pdf&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Rajpurkar, Pranav, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. “SQuAD: 100,000+ Questions for Machine Comprehension of Text.” In &lt;em&gt;Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing&lt;/em&gt;, 2383–92. Austin, Texas: Association for Computational Linguistics. &lt;a href=&#34;https://doi.org/10.18653/v1/D16-1264&#34;&gt;https://doi.org/10.18653/v1/D16-1264&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Rayner, Keith. 1998. “Eye Movements in Reading and Information Processing: 20 Years of Research.” &lt;em&gt;Psychological Bulletin&lt;/em&gt; 124 (3). American Psychological Association: 372.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Rello, Luz, Susana Bautista, Ricardo Baeza-Yates, Pablo Gervás, Raquel Hervás, and Horacio Saggion. 2013. “One Half or 50%? An Eye-Tracking Study of Number Representation Readability.” In &lt;em&gt;Human-Computer Interaction – Interact 2013&lt;/em&gt;, edited by Paula Kotzé, Gary Marsden, Gitte Lindgaard, Janet Wesson, and Marco Winckler, 229–45. Berlin, Heidelberg: Springer Berlin Heidelberg.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Rogers, Anna, O. Kovaleva, and Anna Rumshisky. 2020. “A Primer in Bertology: What We Know About How Bert Works.” &lt;em&gt;ArXiv Pre-Print&lt;/em&gt; 2002.12327. &lt;a href=&#34;https://arxiv.org/abs/2002.12327&#34;&gt;https://arxiv.org/abs/2002.12327&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Ruder, Sebastian. 2017. “An Overview of Multi-Task Learning in Deep Neural Networks.” &lt;em&gt;ArXiv Pre-Print&lt;/em&gt; 1706.05098. &lt;a href=&#34;https://arxiv.org/abs/1706.05098&#34;&gt;https://arxiv.org/abs/1706.05098&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;———. 2020. “Why You Should Do NLP Beyond English.” &lt;em&gt;Blog Post&lt;/em&gt;. &lt;a href=&#34;http://ruder.io/nlp-beyond-english&#34;&gt;http://ruder.io/nlp-beyond-english&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Samek, W., Grégoire Montavon, A. Vedaldi, L. Hansen, and K. Müller. 2019. “Explainable Ai: Interpreting, Explaining and Visualizing Deep Learning.” &lt;em&gt;Explainable AI: Interpreting, Explaining and Visualizing Deep Learning&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Sanguinetti, Manuela, and Cristina Bosco. 2015. “PartTUT: The Turin University Parallel Treebank.” In &lt;em&gt;Harmonization and Development of Resources and Tools for Italian Natural Language Processing Within the Parli Project&lt;/em&gt;, edited by Roberto Basili, Cristina Bosco, Rodolfo Delmonte, Alessandro Moschitti, and Maria Simi, 51–69. Cham: Springer International Publishing. &lt;a href=&#34;https://doi.org/10.1007/978-3-319-14206-7\_3&#34;&gt;https://doi.org/10.1007/978-3-319-14206-7\_3&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Saphra, Naomi, and Adam Lopez. 2019. “Understanding Learning Dynamics of Language Models with SVCCA.” In &lt;em&gt;Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)&lt;/em&gt;, 3257–67. Minneapolis, Minnesota: Association for Computational Linguistics. &lt;a href=&#34;https://doi.org/10.18653/v1/N19-1329&#34;&gt;https://doi.org/10.18653/v1/N19-1329&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Sapir, Edward. 1921. “Language.” London: Harcourt, Brace &amp;amp; World, Inc.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Sarti, Gabriele. 2020. “UmBERTo-MTSA @ AcCompl-It: Improving Complexity and Acceptability Prediction with Multi-Task Learning on Self-Supervised Annotations.” &lt;em&gt;ArXiv Pre-Print&lt;/em&gt; 2011.05197. &lt;a href=&#34;https://arxiv.org/abs/2011.05197&#34;&gt;https://arxiv.org/abs/2011.05197&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Schijndel, Marten van, and Tal Linzen. 2020. “Single-Stage Prediction Models Do Not Explain the Magnitude of Syntactic Disambiguation Difficulty.” &lt;em&gt;PsyArXiv Pre-Print&lt;/em&gt; sgbqy. &lt;a href=&#34;https://psyarxiv.com/sgbqy/&#34;&gt;https://psyarxiv.com/sgbqy/&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Schotter, Elizabeth R. 2018. “Reading Ahead by Hedging Our Bets on Seeing the Future: Eye Tracking and Electrophysiology Evidence for Parafoveal Lexical Processing and Saccadic Control by Partial Word Recognition.” In &lt;em&gt;Psychology of Learning and Motivation&lt;/em&gt;, 68:263–98. Elsevier.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;———. 2020. &lt;em&gt;Eye Tracking for Cognitive Science&lt;/em&gt;. SISSA Course.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Schotter, Elizabeth R, Bernhard Angele, and Keith Rayner. 2012. “Parafoveal Processing in Reading.” &lt;em&gt;Attention, Perception, &amp;amp; Psychophysics&lt;/em&gt; 74 (1). Springer: 5–35.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Sennrich, Rico, Barry Haddow, and Alexandra Birch. 2016. “Neural Machine Translation of Rare Words with Subword Units.” In &lt;em&gt;Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)&lt;/em&gt;, 1715–25. Berlin, Germany: Association for Computational Linguistics. &lt;a href=&#34;https://doi.org/10.18653/v1/P16-1162&#34;&gt;https://doi.org/10.18653/v1/P16-1162&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Shwartz-Ziv, Ravid, and Naftali Tishby. 2017. “Opening the Black Box of Deep Neural Networks via Information.” &lt;em&gt;ArXiv Pre-Print&lt;/em&gt; 1703.00810. &lt;a href=&#34;https://arxiv.org/abs/1703.00810&#34;&gt;https://arxiv.org/abs/1703.00810&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Silveira, Natalia, Timothy Dozat, Marie-Catherine de Marneffe, Samuel Bowman, Miriam Connor, John Bauer, and Chris Manning. 2014. “A Gold Standard Dependency Corpus for English.” In &lt;em&gt;Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC’14)&lt;/em&gt;, 2897–2904. Reykjavik, Iceland: European Language Resources Association (ELRA). &lt;a href=&#34;http://www.lrec-conf.org/proceedings/lrec2014/pdf/1089_Paper.pdf&#34;&gt;http://www.lrec-conf.org/proceedings/lrec2014/pdf/1089_Paper.pdf&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Simi, Maria, Cristina Bosco, and Simonetta Montemagni. 2014. “Less Is More? Towards a Reduced Inventory of Categories for Training a Parser for the Italian Stanford Dependencies.” In &lt;em&gt;Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC’14)&lt;/em&gt;, 83–90. Reykjavik, Iceland: European Language Resources Association (ELRA). &lt;a href=&#34;http://www.lrec-conf.org/proceedings/lrec2014/pdf/818_Paper.pdf&#34;&gt;http://www.lrec-conf.org/proceedings/lrec2014/pdf/818_Paper.pdf&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Singh, Abhinav Deep, Poojan Mehta, Samar Husain, and Rajkumar Rajakrishnan. 2016. “Quantifying Sentence Complexity Based on Eye-Tracking Measures.” In &lt;em&gt;Proceedings of the Workshop on Computational Linguistics for Linguistic Complexity (CL4LC)&lt;/em&gt;, 202–12. Osaka, Japan: The COLING 2016 Organizing Committee. &lt;a href=&#34;https://www.aclweb.org/anthology/W16-4123&#34;&gt;https://www.aclweb.org/anthology/W16-4123&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Sinnemäki, Kaius. 2011. “Language Universals and Linguistic Complexity: Three Case Studies in Core Argument Marking.” PhD thesis, University of Helsinki.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Smith, Nathaniel J, and Roger Levy. 2013. “The Effect of Word Predictability on Reading Time Is Logarithmic.” &lt;em&gt;Cognition&lt;/em&gt; 128 (3). Elsevier: 302–19.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Socher, Richard, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. 2013. “Recursive Deep Models for Semantic Compositionality over a Sentiment Treebank.” In &lt;em&gt;Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing&lt;/em&gt;, 1631–42. Seattle, Washington, USA: Association for Computational Linguistics. &lt;a href=&#34;https://www.aclweb.org/anthology/D13-1170&#34;&gt;https://www.aclweb.org/anthology/D13-1170&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Stiennon, Nisan, Long Ouyang, Jeff Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul Christiano. 2020. “Learning to Summarize from Human Feedback.” &lt;em&gt;ArXiv Pre-Print&lt;/em&gt; 2009.01325. &lt;a href=&#34;https://arxiv.org/abs/2009.01325&#34;&gt;https://arxiv.org/abs/2009.01325&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Straka, Milan, Jan Hajič, and Jana Straková. 2016. “UDPipe: Trainable Pipeline for Processing CoNLL-U Files Performing Tokenization, Morphological Analysis, POS Tagging and Parsing.” In &lt;em&gt;Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC’16)&lt;/em&gt;, 4290–7. Portorož, Slovenia: European Language Resources Association (ELRA). &lt;a href=&#34;https://www.aclweb.org/anthology/L16-1680&#34;&gt;https://www.aclweb.org/anthology/L16-1680&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Strzyz, Michalina, David Vilares, and Carlos Gómez-Rodríguez. 2019. “Towards Making a Dependency Parser See.” In &lt;em&gt;Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (Emnlp-Ijcnlp)&lt;/em&gt;, 1500–1506. Hong Kong, China: Association for Computational Linguistics. &lt;a href=&#34;https://doi.org/10.18653/v1/D19-1160&#34;&gt;https://doi.org/10.18653/v1/D19-1160&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Sturt, Patrick, Martin J Pickering, and Matthew W Crocker. 1999. “Structural Change and Reanalysis Difficulty in Language Comprehension.” &lt;em&gt;Journal of Memory and Language&lt;/em&gt; 40 (1). Elsevier: 136–50.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Sussillo, David, Mark M Churchland, Matthew T Kaufman, and Krishna V Shenoy. 2015. “A Neural Network That Finds a Naturalistic Solution for the Production of Muscle Activity.” &lt;em&gt;Nature Neuroscience&lt;/em&gt; 18 (7). Nature Publishing Group: 1025–33.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Tay, Yi, Dara Bahri, Donald Metzler, D. Juan, Zhe Zhao, and Che Zheng. 2020. “Synthesizer: Rethinking Self-Attention in Transformer Models.” &lt;em&gt;ArXiv Pre-Print&lt;/em&gt; 2005.00743. &lt;a href=&#34;https://arxiv.org/abs/2005.00743&#34;&gt;https://arxiv.org/abs/2005.00743&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Taylor, Wilson L. 1953. “‘Cloze Procedure’: A New Tool for Measuring Readability.” &lt;em&gt;Journalism Quarterly&lt;/em&gt; 30 (4). SAGE Publications Sage CA: Los Angeles, CA: 415–33.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Tenney, Ian, Dipanjan Das, and Ellie Pavlick. 2019. “BERT Rediscovers the Classical NLP Pipeline.” In &lt;em&gt;Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics&lt;/em&gt;, 4593–4601. Florence, Italy: Association for Computational Linguistics. &lt;a href=&#34;https://doi.org/10.18653/v1/P19-1452&#34;&gt;https://doi.org/10.18653/v1/P19-1452&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Thompson, Bruce. 1984. &lt;em&gt;Canonical Correlation Analysis: Uses and Interpretation&lt;/em&gt;. 47. Sage.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Turian, Joseph, Lev-Arie Ratinov, and Yoshua Bengio. 2010. “Word Representations: A Simple and General Method for Semi-Supervised Learning.” In &lt;em&gt;Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics&lt;/em&gt;, 384–94. Uppsala, Sweden: Association for Computational Linguistics. &lt;a href=&#34;https://www.aclweb.org/anthology/P10-1040&#34;&gt;https://www.aclweb.org/anthology/P10-1040&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Vajjala, Sowmya, and Ivana Lucic. 2019. “On Understanding the Relation Between Expert Annotations of Text Readability and Target Reader Comprehension.” In &lt;em&gt;Proceedings of the Fourteenth Workshop on Innovative Use of Nlp for Building Educational Applications&lt;/em&gt;, 349–59. Florence, Italy: Association for Computational Linguistics. &lt;a href=&#34;https://doi.org/10.18653/v1/W19-4437&#34;&gt;https://doi.org/10.18653/v1/W19-4437&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Vajjala, Sowmya, and Ivana Lučić. 2018. “OneStopEnglish Corpus: A New Corpus for Automatic Readability Assessment and Text Simplification.” In &lt;em&gt;Proceedings of the Thirteenth Workshop on Innovative Use of NLP for Building Educational Applications&lt;/em&gt;, 297–304. New Orleans, Louisiana: Association for Computational Linguistics. &lt;a href=&#34;https://doi.org/10.18653/v1/W18-0535&#34;&gt;https://doi.org/10.18653/v1/W18-0535&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Van Schijndel, Marten, and Tal Linzen. 2018. “Modeling Garden Path Effects Without Explicit Hierarchical Syntax.” In &lt;em&gt;Proceedings of the 40th Annual Conference of the Cognitive Science Society&lt;/em&gt;, 2600–2605.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Vasishth, Shravan, Titus von der Malsburg, and Felix Engelmann. 2013. “What Eye Movements Can Tell Us About Sentence Comprehension.” &lt;em&gt;Cognitive Science&lt;/em&gt; 4 2. Wiley interdisciplinary reviews: 125–34. &lt;a href=&#34;https://onlinelibrary.wiley.com/doi/full/10.1002/wcs.1209&#34;&gt;https://onlinelibrary.wiley.com/doi/full/10.1002/wcs.1209&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. “Attention Is All You Need.” In &lt;em&gt;Advances in Neural Information Processing Systems 30&lt;/em&gt;, edited by I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, 5998–6008. Curran Associates, Inc. &lt;a href=&#34;http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf&#34;&gt;http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Voghera, Miriam. 2001. “Riflessioni Su Semplificazione, Complessità E Modalità Di Trasmissione: Sintassi E Semantica.” &lt;em&gt;Scritto E Parlato. Metodi, Testi E Contesti&lt;/em&gt;. Aracne, 65–78.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Voita, Elena, Rico Sennrich, and Ivan Titov. 2019. “The Bottom-up Evolution of Representations in the Transformer: A Study with Machine Translation and Language Modeling Objectives.” In &lt;em&gt;Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (Emnlp-Ijcnlp)&lt;/em&gt;, 4396–4406. Hong Kong, China: Association for Computational Linguistics. &lt;a href=&#34;https://doi.org/10.18653/v1/D19-1448&#34;&gt;https://doi.org/10.18653/v1/D19-1448&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Wallace, Eric, Yizhong Wang, Sujian Li, Sameer Singh, and Matt Gardner. 2019. “Do NLP Models Know Numbers? Probing Numeracy in Embeddings.” In &lt;em&gt;Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (Emnlp-Ijcnlp)&lt;/em&gt;, 5307–15. Hong Kong, China: Association for Computational Linguistics. &lt;a href=&#34;https://doi.org/10.18653/v1/D19-1534&#34;&gt;https://doi.org/10.18653/v1/D19-1534&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Wang, Alex, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2018. “GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding.” In &lt;em&gt;Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP&lt;/em&gt;, 353–55. Brussels, Belgium: Association for Computational Linguistics. &lt;a href=&#34;https://doi.org/10.18653/v1/W18-5446&#34;&gt;https://doi.org/10.18653/v1/W18-5446&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Wolf, Thomas, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, et al. 2020. “Transformers: State-of-the-Art Natural Language Processing.” In &lt;em&gt;Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations&lt;/em&gt;, 38–45. Online: Association for Computational Linguistics. &lt;a href=&#34;https://www.aclweb.org/anthology/2020.emnlp-demos.6&#34;&gt;https://www.aclweb.org/anthology/2020.emnlp-demos.6&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Wu, Y., Mike Schuster, Z. Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey, M. Krikun, et al. 2016. “Google’s Neural Machine Translation System: Bridging the Gap Between Human and Machine Translation.” &lt;em&gt;ArXiv Pre-Print&lt;/em&gt; 1609.08144. &lt;a href=&#34;https://arxiv.org/abs/1609.08144&#34;&gt;https://arxiv.org/abs/1609.08144&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Xu, Wei, Chris Callison-Burch, and Courtney Napoles. 2015. “Problems in Current Text Simplification Research: New Data Can Help.” &lt;em&gt;Transactions of the Association for Computational Linguistics&lt;/em&gt; 3: 283–97. &lt;a href=&#34;https://doi.org/10.1162/tacl_a_00139&#34;&gt;https://doi.org/10.1162/tacl_a_00139&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Zanzotto, Fabio Massimo, Andrea Santilli, Leonardo Ranaldi, Dario Onorati, Pierfrancesco Tommasino, and Francesca Fallucchi. 2020. “KERMIT: Complementing Transformer Architectures with Encoders of Explicit Syntactic Interpretations.” In &lt;em&gt;Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (Emnlp)&lt;/em&gt;, 256–67. Online: Association for Computational Linguistics. &lt;a href=&#34;https://www.aclweb.org/anthology/2020.emnlp-main.18&#34;&gt;https://www.aclweb.org/anthology/2020.emnlp-main.18&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Zeldes, Amir. 2017. “The GUM Corpus: Creating Multilayer Resources in the Classroom.” &lt;em&gt;Language Resources and Evaluation&lt;/em&gt; 51: 581–612.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;p&gt;Zhang, Kelly, and Samuel Bowman. 2018. “Language Modeling Teaches You More Than Translation Does: Lessons Learned Through Auxiliary Syntactic Task Analysis.” In &lt;em&gt;Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP&lt;/em&gt;, 359–61. Brussels, Belgium: Association for Computational Linguistics. &lt;a href=&#34;https://doi.org/10.18653/v1/W18-5448&#34;&gt;https://doi.org/10.18653/v1/W18-5448&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

































            &lt;/section&gt;

          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
&lt;a href=&#34;app-params.html&#34; class=&#34;navigation navigation-prev navigation-unique&#34; aria-label=&#34;Previous page&#34;&gt;&lt;i class=&#34;fa fa-angle-left&#34;&gt;&lt;/i&gt;&lt;/a&gt;

    &lt;/div&gt;
  &lt;/div&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/app.min.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/lunr.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/clipboard.min.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/plugin-search.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/plugin-sharing.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/plugin-fontsettings.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/plugin-bookdown.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/jquery.highlight.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;libs/gitbook-2.6.7/js/plugin-clipboard.js&#34;&gt;&lt;/script&gt;
&lt;script&gt;
gitbook.require([&#34;gitbook&#34;], function(gitbook) {
gitbook.start({
&#34;sharing&#34;: {
&#34;github&#34;: true,
&#34;facebook&#34;: true,
&#34;twitter&#34;: true,
&#34;linkedin&#34;: true,
&#34;weibo&#34;: false,
&#34;instapaper&#34;: false,
&#34;vk&#34;: false,
&#34;all&#34;: false
},
&#34;fontsettings&#34;: {
&#34;theme&#34;: &#34;white&#34;,
&#34;family&#34;: &#34;sans&#34;,
&#34;size&#34;: 2
},
&#34;edit&#34;: {
&#34;link&#34;: &#34;https://github.com/gsarti/master-thesis/tree/master/extra/References.Rmd&#34;,
&#34;text&#34;: &#34;Edit&#34;
},
&#34;history&#34;: {
&#34;link&#34;: null,
&#34;text&#34;: null
},
&#34;view&#34;: {
&#34;link&#34;: null,
&#34;text&#34;: null
},
&#34;download&#34;: [[&#34;Sarti_2020_Interpreting_NLMs_for_LCA.pdf&#34;, &#34;PDF&#34;]],
&#34;toc&#34;: {
&#34;collapse&#34;: &#34;subsection&#34;,
&#34;scroll_highlight&#34;: true
},
&#34;info&#34;: false
});
});
&lt;/script&gt;

&lt;!-- dynamically load mathjax for compatibility with self-contained --&gt;
&lt;script&gt;
  (function () {
    var script = document.createElement(&#34;script&#34;);
    script.type = &#34;text/javascript&#34;;
    var src = &#34;true&#34;;
    if (src === &#34;&#34; || src === &#34;true&#34;) src = &#34;https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML&#34;;
    if (location.protocol !== &#34;file:&#34;)
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, &#39;&#39;);
    script.src = src;
    document.getElementsByTagName(&#34;head&#34;)[0].appendChild(script);
  })();
&lt;/script&gt;
&lt;/body&gt;

&lt;/html&gt;
</description>
    </item>
    
  </channel>
</rss>
