<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Interpretability | Gabriele Sarti</title>
    <link>https://gsarti.com/tags/interpretability/</link>
      <atom:link href="https://gsarti.com/tags/interpretability/index.xml" rel="self" type="application/rss+xml" />
    <description>Interpretability</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Â© Gabriele Sarti 2023</copyright><lastBuildDate>Mon, 27 Feb 2023 09:47:38 +0200</lastBuildDate>
    <image>
      <url>https://gsarti.com/img/avatar.jpg</url>
      <title>Interpretability</title>
      <link>https://gsarti.com/tags/interpretability/</link>
    </image>
    
    <item>
      <title>Inseq: An Interpretability Toolkit for Sequence Generation Models</title>
      <link>https://gsarti.com/publication/inseq/</link>
      <pubDate>Mon, 27 Feb 2023 09:47:38 +0200</pubDate>
      <guid>https://gsarti.com/publication/inseq/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Inseq: An Interpretability Toolkit for Sequence Generation Models</title>
      <link>https://gsarti.com/project/inseq/</link>
      <pubDate>Tue, 13 Dec 2022 00:00:00 +0000</pubDate>
      <guid>https://gsarti.com/project/inseq/</guid>
      <description>&lt;p&gt;Inseq is a Pytorch-based hackable toolkit to democratize the study of interpretability for sequence generation models. Inseq supports a wide set of models from the ðŸ¤— Transformers library and an ever-growing set of feature attribution methods, leveraging in part the widely-used Captum library. For a quick introduction to common use cases, see the &lt;a href=&#34;https://inseq.readthedocs.io/examples/quickstart.html&#34;&gt;Getting started with Inseq&lt;/a&gt; page.&lt;/p&gt;
&lt;p&gt;Using Inseq, feature attribution maps that can be saved, reloaded, aggregated and visualized either as HTMLs (with Jupyter notebook support) or directly in the console using rich. Besides simple attribution, Inseq also supports features like step score extraction, attribution aggregation and attributed functions customization for more advanced use cases.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Introducing Inseq: An Interpretability Toolkit for Sequence Generation Models</title>
      <link>https://gsarti.com/talk/gronlp-rg-inseq/</link>
      <pubDate>Thu, 20 Oct 2022 00:00:00 +0000</pubDate>
      <guid>https://gsarti.com/talk/gronlp-rg-inseq/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Towards User-centric Interpretability of Machine Translation Models</title>
      <link>https://gsarti.com/talk/linguistics-lunch/</link>
      <pubDate>Thu, 20 Oct 2022 00:00:00 +0000</pubDate>
      <guid>https://gsarti.com/talk/linguistics-lunch/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Towards User-centric Interpretability of NLP Models</title>
      <link>https://gsarti.com/talk/tech-talk-translated-2022/</link>
      <pubDate>Wed, 18 May 2022 00:00:00 +0000</pubDate>
      <guid>https://gsarti.com/talk/tech-talk-translated-2022/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Empowering Human Translators via Interpretable Interactive Neural Machine Translation</title>
      <link>https://gsarti.com/talk/xai4debugging21/</link>
      <pubDate>Tue, 14 Dec 2021 00:00:00 +0000</pubDate>
      <guid>https://gsarti.com/talk/xai4debugging21/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Characterizing Linguistic Complexity in Humans and Language Models</title>
      <link>https://gsarti.com/talk/aperitivo-bocconi/</link>
      <pubDate>Fri, 05 Nov 2021 00:00:00 +0000</pubDate>
      <guid>https://gsarti.com/talk/aperitivo-bocconi/</guid>
      <description></description>
    </item>
    
    <item>
      <title>That Looks Hard: Characterizing Linguistic Complexity in Humans and Language Models</title>
      <link>https://gsarti.com/publication/that-looks-hard/</link>
      <pubDate>Sun, 06 Jun 2021 09:47:38 +0200</pubDate>
      <guid>https://gsarti.com/publication/that-looks-hard/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Interpreting Neural Language Models for Linguistic Complexity Assessment</title>
      <link>https://gsarti.com/publication/interpreting-nlms-for-lca/</link>
      <pubDate>Sat, 19 Dec 2020 09:47:38 +0200</pubDate>
      <guid>https://gsarti.com/publication/interpreting-nlms-for-lca/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Probing Linguistic Knowledge in Italian Neural Language Models across Language Varieties</title>
      <link>https://gsarti.com/publication/italian-transformers/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://gsarti.com/publication/italian-transformers/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
